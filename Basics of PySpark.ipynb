{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtAIg545rNgo"
      },
      "source": [
        "# PySpark\n",
        "\n",
        "**PySpark** is an interface for Apache Spark in Python. It is not only allows you to write Spark applications using Python APIs, but also provides the PySpar shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark's features such as Spark SQL, DataFrame, Streaming, MLib (Machine Leaning) and Spark Core.\n",
        "\n",
        "[**YouTube Playlist Link**](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApoAAAF1CAYAAABI9ko1AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAO+RSURBVHhe7P13lF3JdeaJ7vQWQMJ7712hCkB570lW0RRFUlLLu5ZaErtbr99aMz29Vo/mzet/pme6JfVreUqiKImi6FkslmEBZYEqoKrgvffeI5FIn2//dty49+TNm94j9wfsPOeeEyfciR3xxQ5z8grn/XKLOBwOh8PhcDgcfYz81LGPAGftjfQWufzsjvQWufzsjvQWufzsjvQWufzsjjgcDsdwRa46bSDF4RiayM/LyxOk94gFPbvwd1VAPPYEST96IiAee4KkHz0REI89QdKPngiIx54g6UdPBMSjw+FwDCck67DBEBCPDsfQQl7R/F8JpbOlQQpa6iVfGu1n7sKrpDR11ha4621Bx/eekl4Pf+SG39tw20NX49Jf4Q82PP1dw52a/q6j6zmQdNkdXe+reqF34Xf8BH53PSf6FjFmHcWwu3HrTv44hj765/237yt38KMgQzQP7/87GVOgl5VnFheKNHMxAZxHcTgcDocjie42Y0MRMQ3ZafH2z+HIjVx6j57AIZv1Zoue5BUv/FVzd2Tv38o4JZoFDSJFRe0oWi4fHQ6Hw+HolIXRgMRGpAe0rb32p8vedC38pKsIc91e+MMJMcl3Qloc3Ud/vP9sNcJvvcYhEs0+XgzkcDgcDodjyMEIAc2/iluNRh4G6v1nE09FIJqEq4d4P54nrzkcDofD4RiuSJEME4U37iMMg/f+8wkzLxVwklhCeNPnqaPD4XA4HA6Hw9FV5Ady2ZImk3YhASeZDofD0U2EirW1OPoHCSONox2ky19WYYw/2xPHnYH0u8x6sfFne9IZuuJGkZqjmSGaDseIRVLBOlKIbHe5pCfoCz8cgw+b/5RD/J12guw8SyDrZ07keGzYIFv3+1oMOS8mEDMwkZHZj3Qkg41ccXIJYsh5MYEevP8uwohmXmozI3u2o4eT93LEyeEYtshV7k0hckhXkOu5XBKRPI/Ida03SIbbnvQUufwabtJT5PSnnQoy2+2dJF1CZ46z8iuiq/536q6z8AOiq665bgfZnnQkQwbt5H9XkCtdAymOPkAv3n8HyG9J9b7jezKiyaUcL67D4Ps+bg5H/yFWTlFywPYByyFBY/oIHYRv6CSOfY5keN2ROwG50tUVcXQRvcy47MdzSYfomuNsV1HCnyxkO0rKCEPLIP9zDAHkeA1cSg2dN7fWi3aUxBrY9hSoM8XifkfSF8jlb0+lq8j1bF9Id5HLj95IV5Hr2b6Q7iKXHx1JJ4j6YmU+ISAeQUvyR3+jo3hnpy+XKJJpyZZcpLo7ksvP4SS50tQdae1fmPneWrLduLSWjvPK0d/Iqiy6gfh+st/ZQAmIR0dP0fP3b+jgReQVL/zllsKWOjm675+lqkCkOHUjG8kXauy0vbfawzg6HEMNnRXxSDI7nG7SDbQXXlfRledzuYnXehv+cFf9kZ7+wUZH+Rfv9SSP++y9dFZAehlQb8vfYGOw499n79nRLaTzPVkA9CI/2bC9WXvheSVGNGvl6N5vSVWhSFFw1gY8hIeNepK9Mj3pf/Lc4RjOaK8sU/wRrFgc6XilhgbS10CWmnQKwmsvzOzrudx19HxHiM/15NkkYr4MR4z09PcUHeUZjUxX0ZW8MzcpR9GtHVPhJDt8yfuJy91GMgn4w+/ktfbOu4tsfx2O4YCoX+k2MBZivchphmgu+JWWQqmRY3v/xYim/s8JHsLDW/VNcvNWjTTq0wUFBemhdIY74rHnCtPzJw3p1Z4OR9eRKTGpwqzIVYqaW0IpL8jPt2NLU6MdCwsKpbSkWEqLiqRAL3ANQfE4dhXtld44LNukfy5dupJuwIuKizPutZUN57lDTLvLQub53s10CmnWv+3OrekMGrLpby9gYfcwfKKuh56nvws1X4fx64P0DxTaSUdLr+aRhPzrIHcU4S5/87W8W+Omx2LVu+LiItXDTKzMTeoYr3UXhBkl6uDl6ze17WvScw0/n9Yy+G660yqg7oUa0pc5DjxiSh13LjrShu6//6KiQmmor5e8piYZP6ZKSrXxUxUM1ZgGw6EV0SxqqZGj+zonmo36wPXqW3Kj5rY0tDQb2cwgUdWmAhl4DE6ojuGNXKUmVzluUa1BTQPR1DuqYHzItahQG7rCQpk2cXyrxs2UTqWrIKzsuMRraBr6d/HyZT3SrOVJfWMDTlLoqJnO7Xcr6KM9p5kBgWimfvQEfUI0e4g0Ue8p0rVf++gsfsOKaAZ0RC67Szw7y79YvvlLVtGpQ/8KCwqkaswYqawoTXf0oh52t7OXBLFB7+Kxuq5eLqj+NWm6rLOXX5COE64y+sO17oUan+xNEU570hNYuL3xwDH0oS+5vVfcg/dPu9dYX6cNU6NMnzRJqspKrM3Lx5tUUDmJ5mhlmQyd5yrrPFTf2CLXqqvl2m0lmjytDW5u1z0FfvXUv+5lkmNkIZaO7paSpHsaTkpnQb5RqgTRLJSS4iKZPK6qTQMXpSsgrOz4xWs0dHXKNGnoIJr8xq4CiE1s8OKVXKFGv9pHx3c7R1dT2h4GO/y+iH9HaejM/96mf6DQ23xqC3zsmGhmyng4U6HDpTpYWJgvY6uUaJaXmKEEV/1BNG83qf5dUqKpv4iB/lQE33HTmmj2DB3lQMeIYfY8bNh7e+Gbrz2P3PCAJrJX6e/H59tDl/xNw1x3gO6FXlCg2qU6mNcUiGZFUbBodkA0bynR/LaMUi1lMVCu6PBQg7ar12/XyJWaGqlXLWtSH6zTqhIauxQ6S0976FV3zuEIaK/82rEXRcwsNCooWL4eW2hplGhizYRoTps0rtdEM4n4OzZ2t+obA9HUH416JYwopEiv/g3uY2itQ832u3/QWUoHJhY9A3HvLP6dgfQN5TT2LfKSkyKz0NG99tDxE5FoBpJp+qVZ3dLcJEUFeUY0RynRtIYuJbiOx56ANxl1j2NNQ9A/OnjNWpE0ascvxCn11vU0/u55qF1BrjLWlXBzPZdBx3f7N0VDAb1Nf38/3x4G670wwldUUCAF2g5NnTRBRpXkt080Sxf8Usqi+R2pTFk0Uc5s8FBDU4sSzdtyGaKpDW2jKl5o5vQJVbyWxPBTPHYPPXvKcechKk93lCg2bt15JqIr5ba1RTMQzTzVIuaqlBQp0ZyYsWgiyfPuIjs+oaFrlnOXLgkD5o3oHx09PU9TzbyguRbLHIF2PjLbk5hGxGfb8yM7RUMNxLs36Qekbaimb+ijs5wL5TwQTc5o0JgnjUVz/NiqtEUTLYi6F489AfFpRTQbG+TCpSs2V5oNASPRDPEOodnvHpDsriOEljlmEPU/N9p/zuGIJbY7JZc1C8VKNPO1HZw8fpyMLmufaHZUMtsA/WlpadaHsWQGZcdHCnggmS4uw1Mox/HYkQTlCZpk5V6P4XcG8Vfrq90DoUXE0Fn4wBnhWVyZumJCXMITdl3/E3ZSQNfiE8LovoBc15MCcl0fChKR615XJCLXPZfOJJRbPbYjdk9dDiaw4ND+ZRDinsec7fwCjWIU/d1vEv1PhhekYxDXeHRxaS1dafvaCMODqSHCzkYwjHz2BC1KU21CtgUaAiIstj7queS5uLSSUK66JhGU6Z5IZ8gOJ4n2rncPbWMUfeVIfqRvpU4dDkdfISpXtrRGcpFT0PuopQobNuhvyYV4PdttUhyOwUF+7KF1tRiaNSUlafXK8XBUPxeXnsjIRdtGIeYHbRrnpn/hUsLlyM41x50HSnQUCnrOEp64mHbbK0T9C9Kuf2nFUxdm1UHSFwcJgx2+w5EbqaHz7hXQQDQ5iwrmcIw89I0Vs31kaxdW/zRU/xhCy7chs3ijf+PjcAwUrCTTxqQkV8mOt/sbhI20HjYH8Q4jHflhCF3rhMESh2MoIVkilWimrCN6tatKG4cOUC6Q36oFdDhGFqI+9DdCWxJGE5JaHOZspfqMgw7yIspwRDL+3RFHf4Bi3pXWJbrritveIOq6zR21Mw3T2z/HCEQs9XSwcilesla0OZpY/rP7arngvSaHY7BhGpuSoYbsuA3FOHaEZPx7Io7BQGyXBqp1Sq1/8LfuGNGwNRScqBKgg1Efon4kkY/LniqLK5nD0ddAS3NoahpBnZMucp1HX6KAeMyNju92H7F2CPF1OLoKK4laZNorkRmrYqZkxVIWf/cchJqU1siEp2etnCRjMBjicAwgEkUOFUjrZEqykY8DW+CTrVNsgJRAvA2LjZ5ykbPs7V0cjjsdbbY06hMdQMuSkht2V4MLm1LoiSpvq3/WArcWu6b32hcQj30F8iQpDkfHsBKoRaWjkkhJSuqfFXGO4WcvQKgZif+ykQkrnOVpXBB2YmEe52CJwzGQiJoC2PorIupH5goWTUX2xa4gmkp7ihhmZ+JwDGUM+PysxPYpUQOTzWLrX5l/8XrH4nAMD2S0QEsuQ3jME0v97m9YuKouIawYC4dj+COW5s6E5iL5u43hJXWMCNqZaGO609zYvn49gEWCR7sg2RF2OEYu0IakOBx3LrKbg4i+GT3oPdJamIycwzFMEctzV6VZyz1CpysaXKIqxGNE2qLZFWQbb7r6XBL2jPoTI9uZRLcOh8PhGFmgyclutLLR3+1DdvjptsnhGKGIOhAluVA8l84Gi2Y3EL+XnDoYm3U4HAMBVzaHI8IaOVWJ2NhF6UtEjePo2udwBKQX/+SYG5zUmYhuEc30gylt7muldjgcncGbO4cjIrZBHPurPXKNczgyQB+yJSKph0l97LZF0+FwDDVE9U6qtsPhcDgcfYdILNmAPQrggG0zHrPtnPlxWXrKvcPhGCaImpv553A4HA7H0EIbi6aZQb3FcjiGLGxjo9T2YpwHpXU4HA6Ho/8Q2pzMinMEcIBMxmM2sUz/dm7pcAwXQDJR6aDlmR01QawKXKMdDofD0bdItjDZLU0kmyAeQTbxdDgcwwYZshmQrfrJKsDhcDgcjv5FskWKcKLpcAwr5FJjh2N4w0p1qo/kJdzhGMbIYd9woulwDFukWuY0aKKjAG+yHcMHyZLrcDiGPmILlJRcSuxE0+EYTkh/IcFUOoXkeWyuvcl2DH+kGy+HwzFk0ZmOOtF0OBwOx5CGk02HY/jCiabDMawQmty8hMWS8+Rvh+NOQpJkcp78HUu92/AdjqELJ5oOxzCDNara2qYJpg2ne1PruLOQTSrtnOKeOrcSryfJ0o84HI6hBSeaDsewQqbpjY2qN7KOOx1JkpkNL/cOx9CGE02Hw+FwDDq8s+Rw3JlwoulwOBwOh8Ph6D1yDDs40XQ4HA6Hw+Fw9BmSoxNONB0Oh8PhcDgc/QInmg6Hw+FwOByOfoETTYfD4XAMb+Raju5wOPofXVjB50TT4XA4HMMWvlrd4Rg80MdjK+eO+npONB0Oh8MxLGEEM9XCOeF0OIYSMtTTiabD4XA4hjWcYDocgwuzbIbTNnCi6XA4HI5hh+yGzS2aDsfQhBNNh8PhcAwvJBmls0uHY0jDiabD0ceI7Z63fw5HP0AVy6yZHFPnSbj+ORyDiJTiJfXSiabD0QfIz8sz/UqKw+Hoe8Qh8yjZQPe8YXM4Bh659BH0qT523rhmVxHZ4nAMTwxY6XUG63C0QdS/SDI59qWqJP3K9jut+30ZoMMxbNB569fnHb+ohEkJiJHhmEtAPDocQx+tSm+moCdP+wWtwnU47hC0V6btmipVe+Wd63l5een71u7oDzvala6B55MSkfajWa+2tOh/FT1vaWnWSy1BWj3hcIwkZGtMW/QZ0YxKnS2t0VFkXFEdwwyJAt65qjkcju7CdEr1LFu3crcvAe1d7wra0+Okn73x3+G4M5HRGu2C2d80VGF8KovD4XA4hg3ytQ0zi6WKneu1KA6HY3DQWg9Zs6D/9AfiRNPh6BT0zpCIoEoBQbXilfDL4XD0B7L1zCSpmoqsnw6HYwAR9K+1FjrRdDiyEBuwgKTCcB7mgrVkmrm0JH+hWPE8F5jnxdyu1uqYjRASbvGHo4WTl6/n4XdSHA5HCu0pnsPh6Ffkaon6iWiGBrK1OBzDFdllOFBI6F+8w2KEJIKL9tE9jeiea4djJMO1xeEYHATdS/4N6AeiGb3nmC0Ox52BWJq9dDscAwfXN4djaCPoZmsN7WOiGT1vHUhXRzGiFSgpeGVHh2OIoXUpz/z28upwOByOkYLWbWHbKWF9SDSj19lBBHTW+EZSmS3eaDuGC3yapMMxeKCtiOJwOAYWkbZFxN/oYz9aNIO0nsnWOZKVhVcYjuGC7DmaA4auqZXD4XA4HP2OXE3SAM7RBPHocDh6i/j1E4fD4XA4hir6gWiCXIQyXIsNY7RYRgHx2F9IhjWywbvIJQ6Hw+FwOBx9h34imh3DNtjNkuxNd/sakWQ62YwZnZnWECRcczgcDoejt/A21xExKEQTDGwhDGw2ryVI/N1W7ly0TmHmLPMOcpPN+FyQ9v9lu8wtAfaE9iyyn2tNfDPuhzJaNNNiTPu/HDscDsdQR9s6PVNLOkYiBo1oDhwo6IFIZRf+1pIhW92VrqN7T7anntm+ZEs2zI/Uzehf2r1eiNbkmP7oKvlccBiPbQXiGJ5oT0Agl83qTzNnPKf/Wr2HVGcgBBufG5pIx04jG8+Jt8PhcIxMZOr61gLisX8Q2oyei6P/0E9EM/PaMsWs9auM1zP3+wLBtyRxIYGBwGQKVKvztERy0z0B8dgxkq47fjLmSfZnBXFdkB9eWfQlW0A8ppEidJC7QPC4SJ4k8skIHs8m8kwFd+FfPGsHqRvJuLQW/IvIfH4xbRFMhx9kuOwVZGkIp4ZMGh0Oh2MkINaCsSZM1vUR8VrSbbb0HDE8jj2RiM5ikoxtUhwdox+IZnxtrV9leBnhnhGdLAnkp2uIjxk50UNzY6Pk5+VJoZIw5Ma1a3L00CHZt3u3nD99RqqvXRdpapICddNYX2+JztcIGbFJSFukIpaOcPsId6O7XKJ/8/JtG5wg+Rrn1oI7UhSDbGxqlJqaW3KrulpqVOpr6ywNpDU3EmGlz0LCoHXNmsgWDeZ27W3Njka5rX431NVKXU2NnDtzRg7s3StnT56SC2fPSv3t21JUUCAtmm/SrPS0BdHnkWb1E29VGhsapLiw0PK0Sc+vXbkiZ0+flr27dmv+75ETR4/KlQsX9Waz+lkrpcXFUlZaFvzRCJo3emzQd1hzu0bOnj0jV69ekbr6OqLdDmLqssXhcDgcA4FQ64Y2ONVypX/nRtJNtiT96r70BaylTHlo51lI3s+WXO4dGeQVzP25loKCOjm6/3syTtlCKVet9dc/+eRiBly+dKNazt64IY1GjgoSuQ/aZrd5FU4N0Xm2S/NBGQf/OgMumpUAtSgByodcFhRKYWGB7Ni+TU6cOCEnjh2T6ps31U2zuR0/frxUVVXJ1GnTZPGSxTJmzBgpLimRRn0eyxosM1PcIzjX2MSI2i1+ZMc8IKMq7QNyGRFD4hieyVj5IISgsbZWdmzbLocOHJASJWjPPP2MTJoyWQmopp0niHsa6lP8mWDN+Ih/llZ9n6UlxZLX2GR+Hz90WI4dOSoXLpyX69dvyI0bN6W4qFjKKyqkclSlTNb8WrZiucyaM0fqGxuUnIb8jIDwFinJrFVSekbJ5YF9++WsEtZzZ89Jo7rnHY3VfB81apT5uXrNWpk9b54og5UmPNL48I4gzxDprR9/IseVmE6aNElWrFgp02fNSicpIJlryZiA6LL1Ez1B9Dn4xN/oZ+tw6TA0NzeZmlSVl8nUqkop0fOMi/Zh5U5hnQ47y4VUDKyM5ku9Hk+cOy+3m1o0//L1Lk8GnYm+JGPrcAx3WFnWop0p01qfab1C3UO9IdphLi0qlAljx8qoUm0H1IW2Silt6B6SuhM0K4Nmletaz53TjjRtn8ZA62GtD7UNxEgwEvUv5FGuFLeXA529lexcHzgQY95xOni9kB0TS1V70cvh/k4GbRJtF21fgfKLSePHyujiAtM9tKNAc4OxUcsz/VOQP3bFH+bnN8m//+rPSpk+hKKmgSJnoaauXqrr6pS8qGqliVM3sziH83Ap02B2BFwE5dajJgKL39vr1sv6n74pO7dulXOnTsvVS5fllhLiSxcuysVz5+TI4cNyXAlobW2dEpmJUqzEraBAswWyozlBhkQBMRaWBekoJV3kRnTeVsivkDoEcAy+EX7qn17kmtHe5hbZvHGjfPj++3L9ylVZuXyFjNUKtUmv5ynBzhDN1EPR5xhACvhJQrAalijRvHL5kuzeuVPeWf+WbPrgAzl88KBcvnhR6mpq5cL5C3JNwzqlhP3wwUNy/vx5mahEHUtkUWFRukiYBVP9u119S7Z89LG88+Z62fbxx3L04GG5rH7UYoG9VSPnlXgiJ48dl0t6vfpmtRFJPCrQBoK8J1dq1O07b78t27dtM8vonNlzZMLECSEwQ0yUJSYlSXR2v+to/8nW/vI+g8JpB62oSBu6YinU8/afzyD95vThrrgnkCY9XNf8btSHWwg0FZ/4z+G4I5FVtDONnN5oaZbCgnwpLyuTksJ8q5eQ3mpD0KwM0Nc6re+qlWzS9lFr0VlMj1L1OsThh+w8Cog1WzxGRJcd5VNuHwcDuWLRWcxHGqzca8IhmxXlqn+qh0H3oJjkSCZXhi3RRJobtelVZd+4YYNsUDly+JASqRJZtHCR3HPPPXL//ffL0iVL7FpDY4OcPHVKzp07K3VaWVSOGiUlpaVSVFJknkHygschBlECS1OYlTCjPCGTydJs5LoWgPuUr4ZwlvHX/sbgVAr0wt5du+Tk0aMyRuO7aNEimTBhojQ2N0lhoZI0hrANrf2lAmxupn9GHEMScFmoZChfS8UeJZmQcvwuLS6RBQsWyLJly+WRRx6x8wnjJxiJPHv2rBLAW3Lx0kWZPGmyjB8/LoRCBav/qm/eUKL6obzx2mty8sRJs27OmDFd1qxeY3l/992rZPbs2UbuGxoa5cTx42Zxvll9U+bOmyv5DLkrYSZfbt2qlj0aHyyj46rGyrLly80SHVMYENMZJRsd3es62n+6td/2/q2xcaLpcPQbsoq2E82hgTCFq3UNHaCtqeYNIG8CsnM0B4JFpG3Gp8Dokb17bTM6ROoZ2joLn/eTjkcGOa+lBHR0P1tGIsgfsujOJpr6B/JyWxX/hz/6kZxRAplfUCBf+sqX5dnnn5P5CxfK1OnTZLYSmtnz5si8BfPlshKmy5cvyvXr12TKlMkyR++ZVVP9KlDSU1FeIY3qJ8O4pUXFmoGacWSe5mSwoMZhm3x7jkJPZQNZomxT+egPM8Hn6XPoDcMruClSIsK9tAKqREXlGn7oidTV1tqLa9Z4tGjFdmj/fjl57JhWpOVy7733StW4cdLEnEme12dIM740pZQwXGdI1wYCVCk11uqA6wy537xxQzZ/8IFs/2SLlJeUyhNPPCFPPfOMrFm7RmbMnCHTZszQ40y5e9XdRjaPKsm9eu2aksXbsnLlykBwNU0Ml+/YvkPeeP0Nuab5OU5J4b1KLh946EFZtWaNLF62TKbPnmXD79NmzrLh932alhs3b8pNlQLNw7l6rYh81jgybL93zx45f+6cEc0lixebnyFfA4LiJ6V/0L7PrcMlPlbx6SUnmg5HPyGraAedc6I52CDdtAesJSjUdoj1ESwwZUobuWHvQTNO35S5jbnKKCRtHW0p7R2ji2QwbUJoH9Ud//W0Rds12jLWCPC+7d1z0/zIiLWFiLphHYYtmjW34b49wz19h7ij/cZfjDWc85xdLwzXHZ3Dyr1mK3ylM6LJ9WELFP3CxYtyWwsrctequ2S1khzIWEUl8wxH6bFSpk2fLgsXLpCnnn5SKivKbej41MkT0lCfWlyj/ty6fl1uXL0qLQ0NUqmVVr0Sq9u3btlCGYbga6pvSkNtnS2QISNtkYwWfs4hZCgEQ7616r5a/bqlZArCWptacBMWLFlZt2co/fWqEJAuLKyFejNPC3iDpoNFM1WjR4ewcGpAm1CGPFMGhsDp4REeZNteqypqk8arsLDIrLhGolPgfnFRoaXp+pUrFvdpU6bIIs2XWUoIK8eMlvJRlTJm3FiZPH2qkcSHHn1Exk0YL7f0mWjdJA1FGv6Vy5dlx7Ztcv3aVSktLZWVd62UR594XJatWKl+jJNGdVeg18dMnCgLli6R1ffdJ1/UTgBzP5nqsGPbVjlz4qTka5rJf/LP0qgS8yeIw+FwOIYaIGTMxWdeP2hpUoKmdXlsF+MCUq7ZAlJzFA7ch9yVaRvBglJrv1RoX3BkazD0eaZraUNvxDF2LoK/gcya8wSiG9ooOiC4o52sb6i39jY/v8CO1dphv63tLPcbNHyMN7SdGK+KtJ0k3jwX/QxE2dFTDOuhc3onWOh27tihx+tSrqz6nrvvNosYpK8E61vKMkjh19Ijl5SYUmiY6zhr1iwZowQLgscq6ffffkcJ5w0lijdk184d8sHGjbLpw42yd89uW5AzWknrKCVKDBFboVQlIx7Neryt5On44aOyY8tWsxhu2rBBPnjvfbMcXjijJO3mDSOP5WWlqRS2yM6dO+XDDzbKqRPH9VqLvP3Weg3vA9mze5dcunBBJowfL0eOHJYD+/dLpT678q67ZOKkSap3pEX03lF555135OOPPpKLly7ZohusgLVKVnl1sfeGRTME2SI3r1+Tw/v2yRX1f6z6uWjRQpk4eZJZYOmxN6CY6jRPFXVU1Rg5duKE1CohnzZ9mhF5KgYUlYU/77z1tlmAlyiRfOSRR2Wm5icWSpQWhW5Sv+jtUrfkq3/T1Y8zmhdnzpyW20qQIf0L5s2TUs0TVtfv1XSfP3dWxuu7WbJkiZLcCSnLXUBQ9qT0D9r3uXW4xCdYV9yi6XD0G7KKdtC5QCio192iOTiI7+DatavWrmKMYbRQGZpUlJfb/UxOUlfyVCCT3MJAc+H8ebl65Yr5wYLXIhVGEVncy+4j7ELCDilM24IQJpHxPxidsK5y7dq1y3Lx4gV9/rJdKynRNlffU722Y5cvX9brV2XcuHFmkCkuLrJrl1SwylqZUj95ztrOGGlFDK09GWmwcq8J530O8NB5W/fZMBf6h2NS+BuPnSG6gHBROLdt25baBuimzJw2XcaOGWNbAVWWlVvCMc8XKvGBJC1ctEhWr10jS5ctM0scGVRbc1vefest+WTzRzYkDNHbu3evzee8dPGSDeeeOnlSrmkBHaPkjMUxDIVTEKmCTh4/bgti1r/5phzYs9fc3bh+PW09PK3PsuBmNERw3Fjb3ofeIPHesOE9Obh/nykphPmgEtobSp4btAc2RUklCnzs2FGp0jQt0ziPV/JFT4w5jd/653+2Z65oeJC1hQsXmaWTHlmwbDIMoPTE3pUqouaBarcNxZ9SaVTFY2HOaCWUkMImVSoUtqBAS4G+20JVwrlKBFevXi3LV7AQqcoUrzo1zL1v914j3w88+KAsWbZUKwhVdC11tlCJF6Th4hckM1ZKrEL/aPMmDbtB8y9P7lG/Gf6ngtqveW5D55pHGaJpHpmE8hbffq5ykqkUeoNcPgeEeESQJieaDkc/I6toO9EcGoAoXrt2TV555cfy9vp1snHD+7J961Yjb1OnTLHpaOQiRgfaBGsvU/UlufXuu+/ID3/wfdm+fasZS5iTj6EEsgm++93vylvr18snH38sK6y9LrHrAXgS8zwcIYdM8dq86QP5yU9+bG1URUWFTJ8+XYpKSmTDxo3y6muvanjbZeaMmUZeif+bb/5U2+512t6fs3hTlvCSdimEESyqSXA1KSMRVu418V0hmq1zr1eInsYA2pc82EOWhCFi7ncdFNxRY0YLWxZhYmfI+Vvf/Kb8zz/5H/Lxpk1y+MBBOXfmrM3ZsP0etQCP08I1fcYMIzHllRUaZL6Z6euVPF/THtSRg4fk+LHjVrDuv+9++fwXPi9r1641Qnv40CFZ98abclTdFOlzuLl5s1oOKDl8S4kqPbOJEyfKqrtWyXPPPScPPfSQzJk92/aZvHn9qqz/6RtGPim4EEB6alcvX5HrV68p2TxgWwLMnT1HKrU3yOKfKVgaeWmaLRzpKUpTs5w7dUr+6s/+XE5jbbxdIwvmz5MXX3jBVtPbkIJlIxmaEq6pgrdoePi9YO5cI8y1mmYson/2p38qr7z8Yzl6+LCtDLcpA0wdUALOtlDM2Zyg6WrQ5yH3t5TUHzt6VAtPi1SNGm3KWVlRaQQ3IDQECI0CFbLFRBMyQeM4btwEKVFlvlldI6dOnbYoZqYIZBCq8wjO4q/M1QyiBxZSShwOh8PRH4CI0d5cunjBjC3sX81Uqr27d5vxRit+6wzY4h2tjtmGkPagQInn5UuXZP++fbb487S2ZxcvnFOSWKNNXKPW7gxn18sldXPu/Hnb+QTDTLKNsNP0BY5xqL7Z2q6L+szZM6fVL71H8Or2ytXL2t6ctLaLIX/WNTA6Sbt95vQp20u6pKjIht1tD2kNs1nbPCys4St2jp6iD4kmiGSgc8n+F653H8xDfPDBh2xYd5SSJ4Zyjxw5It/73vfkT5VA/dVf/7X86Ec/kg8++EBOnDiuheqqFXazvOmzECe4btzu6Eb1DZk9Z5a89MWX5IXPvSiPPvaofO5znzXCybD50SOHbcj45vXrRqyqb92U3apYWORmz5ktL7z4onzqxc/I408+IS989kX54pd/RuO20iyNJ08elxPHQiFnGCCY6kXKtafGkPoLL3xGfvVXfll+4zd+XR56+CGbX2pQhWWro3yV86oQP/zOd+TE4SO2qfqqlSvkZ37mJakaO0azUN2oUjBcTfpCj5t8hbC1GOHGqjtvwQJ58plnpFzJ7LWbN8xau+G9d+Wv/+zP5K///E/ln7/x9/LuujflxJFDckUrEeaYMocSpSSPiDs9RyZ/o5BYTgO5BRnlNyWPoveJC71V5n0SN6YeQHbted6Humz1DLBH054rkufZSD3jGKJIvtueiMPhGCoo1PbPrH7aLmHMqNd2ouZWtRzT9pepbBhTzMighK2xUdskrf8hnrdv1xh5ZJ4/U9xY74AhqLGx3toRG4XDP22vGO6ur621tgeka/8U8TMCGBoOa1dtNE/DshFObZvzC7T1UE9ps1izMV7bnslTJtk0O9xxj7Ahp7e0LSRcjDKsqdBHLb7EjzRAYjtCOm6ONuhjojmwgPQwLD5q9Cj50pe/bKunWcgyYfIkKSkvl7NKko4osVv39lvy6muvyd/8zd/I63o8uH+/Da1DONnIPE97WHwxh5JVUl4iDzz8oCxYvEDKKsr0mkjV+CpZdfddMnHiBCNYLCRCoegRsVCmuLTYho7X3HevLFi6SMZPnij5pUVSNrpSJkydIhVjRpn/hUXak9NeFeQWhShQUoiU6vNT9Jk1a+6RKdOmyLSZM2Tq9Om26MfmiTAfVEnltYsX5Uff/o6cPXZcVAPl8Ycekp/TdE9Q5WG4AtUL6hcQm2fIG0qABZYh9fGqaPc+8pA89szTMnfRQpmqYTLsVK+KeUPzbNumD+Tt116Vv/uLP5d/UdJ55vgRadTeZlQ8CKKRWJUSDRd/jbDzz7QthhzIJu40s9NxLC0rsa8TsQk8M0Lz1GPeJZVMqIz0CPHU8/SRm45hDMpDPPZEQDw6HI7Bhn0QxeprbTeLwuJU/SOXtQ3ZtWNHWJBK+6V1fzRQ2FaDDWx1d8L2bi7WNi4KVkzq/qjznLOAlTaEc/VKw6NtyLQThB0ltBGMoBGv4A/tp8VLce/atfI7v/078h/+w/9bpkyZEtpWdYuhgwVNGJswnjCyaIYdjYZ9FEDjH1fSd4auuBmJGLZEkxdKIabgjh49WsleiXzqM5+W3/3q78unP/uirL53ray8524lbTOloKhIbioxPHvunLz33nvywx/+SPZDNpWs4RHkqLa+wb5Ww6rreQvnS3FZqTQ0Ndj8QvyGdDJPkfAouFhHq6urbcX2b/zar8nv/e7vyqKFC63AM7n4VGrPToT5mnVKbGNPjaH6SKwAVkiG9Ms1DHqJ3GAhDnMmIZsoKivL3163zj4TiSLM0XS98OlP27xP1SwlkJ33tsivelV29q+s0Ocee+pJ+eVf/RX50le+Ik8+/ZSmb7nMmDFNKivLtZdXJ9eZSnD4oPzNX/+1vPfuOzbUj7W0ro59MbUnqvFi0ncZC5xCDZEGv5JXqCzqtMfLSv9IHLHykgdG2LUH2j6SBCNUQm3FMfSQ/X568576wg+Hw9FXsO2BtC2zdkHbg3LWPJSVWTuzZ/duW4XOHFbaCVqABm1jmY6FJfPihfM21E57CZHDAJKt2oyicR9DC5bP1vfDD/5GiaCtwZDDiKERXUgj/mg4xNm2YtI2MKyabwztp/rPvtwsBGbHmJsaN4jw5QsX5aZeq7vNzjFNGFLTpDa9kl6R3d45WmPYEk0AuYL01NTUWG+EldOjxlTJw489Jl/48pfkX/+b35Ff+83fkJ/7hV+QBx95RCZqL4aysW/vPtn4/gY5d+acKkqwAfJRRwjY6LHjpLSi0shprRY+NkfH8sY9Jg9TgGu10F26dNmUa1xVlRQVFEqjEsnTx4/Lqz96Wf7lH/5R/vFrfyv/5T//ofz//p//Lu+vf0salShaL0kLaOxhUfARNlLHCtusJbVRiSjkFouhfTYSh0ri6pQUnz5zxtyzWAiLIMcyVWyeR2naL+5BybjDfNE4ZABpHjN2rCxbuVJJ51Py1f/wB/Kvf//35IUvviSrH7xfxk+dLDdu3ZJTZ07L+rfekj1KcgGf74Rgkg4supDgJjbPTyCSSUDcbD6Pvi/O2bTdlF5/V1WNtcoouk2nIa3D8YRjZ+IYOojvoy/fT9LP3iNV0tKSjc7uZyPbfXdlsDDY4TuGJ0IdH4gc9fnUKVNtkU1leYUZYpgLCQllr8r4mU7+1dyslmOHj8h4bWt5xrZH0uarQOlIQV6BkjkslOpe1TxaFSEqiJXVdtSfNjNWDbgLYWawY/t2+au//Etbk8AcUYBVk3QU89U7bZtYp/CD735X/p//+n/Lf/pf/6P8H//5f5d//qd/sq38aO9tBLK4xMhpR3FxtMawJpqYvNmH8owSMCyIwOYoKgEqZPV1aYlMmTFd7rlvrTzzqefl0y9+1r6fje39yJFjcvr0GSNIDJ1TSCGTJWWlej/PyFt6DooSPSyQpUrqbLN0LWEQXFa0UWBZpf71r/+d/M3XviZvvP66bYXEtg2QsQksPpo2LWzdQE9KhV5WUxPDz83qX1hgU1lZEcgWyqtpCPe0J6fh8btJNZF9LqvUv7LKcrl0+Yp8/MkncuHCBY1eIJLtQ/3VSJeWlGov8rqc1LRfPHtWqvUcpWcovHLMGA0jT8pGjZJHHn9cvvRzPyeff+klmTN/HjolNUomd+3aZYrJynH2KWXv0kuXL8nVa1elrJx8IyziEQVovmr+sZ2RXdP00KNlyAI/Ro/GIpvajFc9CE+aCvNw6m+2n46hiez31Pn7SrruGpJ+tyedI5SrcEyeR7S+3z1/eyIOx3CDEU2t22kjWeBDQ3HfvffZKBxrJfbt3WuLgmjz2KWFXV6Y6kabye4iLJydMWOGtaM8X5BfaCQTjaA9Q2gXaC/xoz0djNqJ+6hPWEFDi5LRr2rlCoRLvFirwXXbp1OfwwBy4dx5+elrr8uHGz+w3WfY6QVyuW3LFvnR938gH7y/QW5cvSYN9fVpPw1dqxpGNIY10YRk/eAHP5A/+u//XXsqf2W/y5XcsY8khKhBC1sh8y6UYI2fPMW+WLPy7ntsm4obSrCuKdHCnE+PC0LHwhQEMz7Ek615OMeCyZAz2wixZQNmd1ZjM2zAXBO+TLR1xw4pU4WZO3++rFp9jzz/mU/LS1/6GflXv/iL8vCjj0iFksSCQvYHy1fFpGyzIS1DA5oQPVJaOS/QP6pbJvxmqBlFLlLivHLN3fL5r/yMzFTyV6/PvPLaq7L/wH4jvVgqM1QtAK/pQQbkyXVN7/vvvCN/8xd/IX/xx/9DLp06LaM0b8q0h4YyswdmnvYo8wuKNO3FsnbtffLZFz9n1uJb1bdUSc+bbyyK4lOTTCdgSsKhw4fs60BB44JAJC9duqjv5Hx6kjbpOaDxZdI4biZNnGAEm3sMo1v3MEY3gZCOrotjMBAr9VAGk5V8tkSk2hQTO+8yMuUsKZkw2w87Sntoex+/I5LnDocjAs2gzVy0mC32wjZEjBwyJM12QbRjtAlsJ8S+0OxhyZf7ZsyaqdeDXsUhaZAxnGhbwiVrKNuH3U1ZGbFjZp5PQX9DJhlFw3oJ8aF9p23iGnMzIcEQ0IULFshLX/iCfOUrX5HPfOYzMmv6DLl68ZKsf+Ontt82i5MgvsR3JCO+q66gz4gmrzWX9CfYoByyd/36deslHTt6TC4q2aSgQ5yihdCsknpkOBwyOGHiJGHpCr2ZYiVZFLTS4lItOPlKguqNWBJ5iBfltUjdsBfYiZOn5PqNG7ZanW2V6pWknjx5Ui5o2MwTXbHyLluUhLz42c+G/SWXLrV8YKVdY2p4maFrrJUEwqo4i6s6MuUw0dhxX4+NDfV6SRWhpEjW3n+f3L12jSy7a4VMmjaFMQV5W4nj+fPnTA9ZdGPDDZpWigDCeVRgLMCEwTyUS0oa9+3cLTU3qo1oknZIt/VMVbM5EhU+fVlWViHlmm8QbOZoQuLnzJ0jk6ZMlkYlvAcOHpRDhw5ZepLCBvTrfvpT2b1zp203cU2VmBX7rCCcMG6cfQedfLP80PcI4lzTNFlQryL/DNIxiQDx6BgohHcSj10WikkCceEXw23ZYJpGnHKCm2yJlX6bMDqRiFz3kIBQnsPvcJ4tlMtc11tLewj3u+ZH30rrMB2OHkKVgxEuVnYvXLzYSOeJ48fkwrlztkc05QwDwwVtqzA2YKzgC35IbBdNl+0snAMrlZyqxBIaOqWh5LZGuGLOOWUoTsmlOdf4FGv7z+KiUuUHYRheSazetwVHGh6EeOnSJfIFJZlPPvmk3H///fL0U0/KQ9qOYxS5cvGibPnoIzl/9qy15yE8FQsMcbSHPrdoxixPSn+BBSlsxjpu7DibK7h+3TrrRbEnZvwsI9cxdTcqmYGI7tq9W25owR9TNdY2P489E0z7uD914pQcPHBQGuoapKSoRJ9tlAZVhGtKzg4cPCSNWngZZp6/cLG6ZuHPVfucFT00FgbNmzdPKpScMR+R7YTYhPzI4cMaDv4zhHzbelYAggnB4x6kl+1OKbPMS+FrQ3yWEgsne4uxYludSn5hvjz4yMP2DfcGbXiPnDgmr7/xhly+csXiH+azZCStjuoxWwvxNST7epDmz8GDB7R3uc8IIEMNsbGmoSfP2N/z/fc3SG1tnRHOmSysUgKKhZMKYpGSaL4gdE4rjw3vv2+flazV/GauC5+VPHr4kHy44X35p7//uvzwu9+VV15+WY4dOar5U6H5NF9Wr14jNbeUgEMuU+SW+JJWVqZDtINohaBubJsJFSOmKnEydiSpsXICnEVx9CeSBIy6vcX0kndiBFDLOIvu6Azy8QE6hDU3bkqjdujUkZV1yrzN80q9T8qhbaeluhkWAYQwKB/Wacrxjzdtc7JQEvvdFmEucPvSOQIxyyUh/R0JiMckkvda+0keUEewVy7br9gnbWng0nrRWpLPdlX0wZSAeBx+aGPB6gck65ckCJtRsZGKdGnS7MEI8+BDD5thgjp6944dqt8t1h4g165cth1bGBG89777pJZ6QJ9ljib5aFPTcKvlXjVA/eRvKJkZiSU3HNNX7f2EK62hfqh+0K6xILXO9uuMQ+voGYE3SUVFmdxzz90yV9tWqgMWA7OjzZrV98iiBQs0jk1y5tRJaztN5yysKCBX2HcWeDdR16I+hDq5Y/Qp0RzobKawLl++XBYvWiRlJaX29R3IzOs/eVX2794jp44eVzkmR5Q4frJps93btXOnPYuJn6/igGhR5Ms5kMPNH26WLZ9ssa8CMW/k+LET8s4778nRY8ekonKUTJ0+wxbqNBrBCUoCgTxz8rRtEM/G8XVKoC6cPqvhfiTHDx+ThlrmdeQpCVYCqaSSzWtp3OzLORpuYWFxuJ4amuc+8QqLnPS+EmEIL/UZJO/Bhx+WWXPmWG+QuZq7d+22F24KiqIWMEQfX294MywymjJtmizTPBuleYclkj1GX1bZ9slW2+D+5LHjJkcPH5GfvPIT2fLxJ0Y6S0vLZOWKlRYv4jRr1mybIrBAe68NWqHs2rFT1r3xU9n43nu2Mv6gypwZM2SChlOrDeSubdtl08YPzWoKqeeTlRMmTJRifW9YlLlmadY0XL96VXbv3CV7d7WWPYh2FJgTy96lN6tvpNMWjlEc/YvsvKbC1r9a7jhSbrFaM9Vi04cfyvr16+VHP/yR/PD735ef/OhleWf9W/KxXt+9fYd98rWUxQCqS+xdR4ktK2HlaoF1GCGnfGY2bj8SESu8KPqH/+E8BSrCpPQGPN0VaR+ZeLVFyLdsoaGmo3lL9Wfjhg2yQTttbIwdVsg2Wp2VlhzPI50h4y7Ej7+5xOHoDKE9K7DpY5MmTzb9ZUEQHwrBaMI+lcf1N/UD8zNLtE2JU9HseZWov2EU0k4D2dRzk3BJkbqZhupQgs1k74ICwoiIklnVHYgi5JM2kut0crHGzl+4wMLhS3ms12B9BNsa8jlo2ldWyjMNgHQC3Aa4lnSEPvwEZVu3/YUYGoWSb5byGcS6mhozafPJRwjTmZMn5YQSw4P79sseJUGsGjt39pyZz+++e5U89OBDNhGZCcpsWwABPa7uUYIbN67L5ct8Z/WcHDp4SD7ZskW2bNlqCjFPleiZZ581qyCNX031Lbly6bISydAoXjh73oYLTijJ3fju+7Jz63azLLLoCJJYUl4md69ZY6vcDx86LPs1fuWqcIsWLZbp06Zre0tPiUaGxUDNZqE9fOSwlJVXyLJlyzXcCVJUXKQ9rTFyW9/D4SNH5TZhX7okc+bNs03rGZaHuPE8GWWvUf8YAdDrfK4SRbyu6eQ5voR0TvOOrxPt2bNHdm7bIds13lh2+dwln4N88qkn7ROYti2T+sN2TZVKusvKym2OKFbRq5oPxw4fDouNlHDXat4gt1WYVI2ikt+Q5ilTp2qayqRUhSH966rArAqEqDOB/KrGa7+mfa92GPbs2avkkqOKEsxder57z+7wmcpxY7Wi0ArDKhaUnSNn4dgTtP+kZWQ4VUBeKIPk78j5BGWMeTzGWIjNb4YMYnU+eeKEdWA2KEH6WDsrJ0+ekDMnTtnnWI9quaLTcOrESevEVFSUS2VFhZSVhikvyHnVvQ82fmBf3apU/Z40eZKNShBqKzKZPmbOIjojl5GgJR7pFfAm6ZX9ToURJPWeUo46IohEnYYaHFdy+WPtJO/WPJs9c5Ys0c5dyicTEM71X7yQhUyOBSSfi0BnrFwFz0zSxSxzGHhkBRx0Dgu23lBCYJ+g1HqlpIh1y8F60tu48nzSD/Kv3U9Qplz3tjMz3EB6mde4XdvHKxcv2SeKsVKyLoIdUq5pW4wFfr62mWOrxlo7s27dT+0TzGvvvVfmLphv6yr4khCdybvvuVtmzJolBQxla33OKBlfGmJ63HPPPx8+TalZTN5n53e4Et794YMHtN0+YORx9dq1MlnbGt4f07sOa3vO6NgjjzxiWxlSlfDZTOqbadOnyeOPPy4V2oa2aB3EiCHtcXNDoy0Uoi3Cn3u0/R6nRJnRvKhXISYxFiMDLPoiA9lfu0I5xQB9gnLgwRAdJGeRVrzPPvecPPzwIzJWiSerqE+eOmWNFF8JgqgxCRkL5j333CP3P/CA7RkJIaPxgpAxvIdlcdrUaTJrxixbGb3pgw/lzZ++KTt37LJKZakSvec+/WmZOm2aKkOx7UW54q67tOCtNisjc1Q+3rpFXnvjp/LyK6/IKVW2u+6+Wz7/xS/KhClT9ZkSOXX6rBZeimeqSswr0N6TkghVCohoPivv9Brn3GvRODVzVCksLrWh+1tK/gj/4Uce07QvNavgRSV527ZtNysr6eCIF9Z4pF44i6MKSopMwRl+Z4P52XPnEqgc0nzar0p46PAROXHqjJy7cFFKKyvk/ocekuc/8xlZe9/9UqThNKji1Tc0KUEsl2Kt3JeuWCEvvPBZef5Tn5YZ2ggSb/Yr3bd/v5FzCDtdzVmzZpsVlpX7zHP9oTacr77+hmxXgl+n75G4kS3FpeVWkZ+9cEnOnr+QkvMmfI4sfJLsnAkdAiMf2tjwfgKyj46+R+68pRNH43/65Cl57+13ZOsnn9j+c7NmzrTv5X/qU5+SZ55+2joIY7STckErd+bwvvna63Jcy18T01TUPZ9A5XNwjExs/fhja8yMlKnf6eqL961ipJOj6nAYfg8dtVyNvj2fEuJpc0KRxL04fG9JTJ12GalnLK4dPd/RvRSIB3PN7fN9CnbXYKNr9pulnmHkAkRvOvHOkCToGX3pOnryjOPORlLLbPsfLZeUE0bNKLs3bty075ij0xe1zmZPynFjx8p0JXW0UdZeaftAm4VvSf8Y6YpFLlxvv/zFOxwJl6NZG1Uy50HH07rO9Sh6n04DRhC2M2SRK6OWcAPiyLZGGJowLkF8eR7wrKNzDGuLZqxsKQxlFeVKmmbL8pUrZcbsWTJu0kSZPW+ebWe0Shu5VUoGH370UVmlxI+5lNok2X6QWpSNcELSzp49a+ZzSCvuZs+ZI3PVj3tW32O/IagMCWBhsQJsEcmTsePHy8Ili2WSEtAZs2fLgqVLZPndq+SuNffI0rtWysJlS23j+HmLFqq7JUbuaA8ZPpg7d55tBD9TSRpWS4YMjHQp67JFR6PH2D6X8xYs1LCnSLmFnW9pZhgfy+Bi9XPBokXm1/jxE2w4gneDwieVib/2SlWYED1P3c+eO0fzQ0XjNIPf8xfI4pUrZMXd98iDjz5mK/WnzJghxcw71QeZCYbUa6OHn2wNNVorDoYcJmjekOfTZk5XYj1Z5ixcKIuXL5O1999vhHT1mrUyUwln9a0auXz1ityorlbyWiLM9awYM8a+5nTXPattZ4C79J2tWHW3xuNu/a2ySvOTPEU4V1mqz1EGUPo4lBFBlRWSyt/uof0nUpmXglVWmsfk6cixaLZFjAlxpEF56811snHDRq1c8uS+++6TZ1Sf6OCxmnOR6sBi7RguXrxIG56Ltjjg4sVLMl47arYNmJaHMpVdu3bbNmEQwbu0MzdxwsRgTddgIIlU9pBKftvn6SxDW6wxYISCxotWCnehIQv5Y2QrRSbpdWMNJ+/ilBMWu4Uh61CujJDqo7Gc4Qa9svfGDYW50/sWht5j+odtr5J6JgrP0qG1oUL80XqHa/ELYTxv/ut13JdoR47ONKMQc1Q/6VCz0IIFjQzpUT+oQ+2AhkV61AmxTqQDZmnT+4DOmP5N54tmnoXFb/bipTFlXi15mnGjDYTlHduxsUAr9fxAIyQhDeJNutyiObggvXQkzaJ56ZJtn4exp0jfRZW2Cdu1Ta1mPraW4YXzFsjGjRttVfdd2ibeq22CvkA5d+6cHNi7T2pu3ZZVd91txop8Le/4vW3rFrM0oq/Pax3C1DLeA21O1C3Ae2A7QMC8Zobr9+3bb9dXr71XJppFM08OsmhVpUnbrocfeURGjx9rOoNFk07vKG1bVyxfIZWjR9lWh3wwhRGXsuJi7fB+YhZZ1is89uQT1uajgxaflCTPRgJsap7mUVcsmsOaaALrvZBYLRjMqaCQMJl3nhIfKuVFSvrmLphnhHHsuLFmUaMQUfAo6PyjQt6xY4ecOn1aJitZekAJ5QwlhhOUCLIABv8gnWOqqoycUrjYq5PKmUJfOXq0FTy2apg5Z7YNYUPcWLAzZvw4WzzEHpj4MUOJFr03lITV3IQ3VRUB6yjzNYkLVkviyCp5/GY4YdYc9UvDt0ZK853402hBPFm9PWPGTBk3jk9RlgQF1P8oWmtkKsMSrRQg5+M13lOmTZUp06erP3M1rxbavMv5SgogtsSrUP3USKeUm2NBaPRT12wfNe3pTZw8SYm95gHx1fRDfhcpCWa6wTT1f/SYKrs3ecoUI9sTtDOARXii5gFpgjQjU6dNl+lKbqfPnGHueRd2znH6DPuNf+ndBGh002UxCdIa0tsdtP9Ea//Iy9DoOdFE0A2mQPzkxz+Wq5cvW5l84YUXZbHq4ER91xCmMu1MsA8rHSJy4fiJE1Jzu8bK9eLFS2SMlncW8jGisG3bNitnRjRVFxnygujw+VhVFCOYLKJjBWh8D+RnJI22bZY2EjR0NBiQJYiTxdfcUgVAGJqNXEK02OaErcK4bpZyLOYaLsDKGIip6o/6y5H5yeaRhmeL1VLPcM+6eeZPIJj85BvL+EM8STPkkiFyOru4Q2zRk8bjxs3wQQa2bOOTebbnoJ5Dgo1IpoghozrkrRFu9Tt+USUunsJ9KtUWLvHAfz40Qf7xwQvmRxMfi7emN+xgQSmlsJE28j5F7tJInvcjsoIJ79qJ5mCD9KaHzi9dtqkvDz30kJRrG8yoItOpzpw6ZWR09KjRps/sm7xWO5+ztM3S0ivnz1+w6W3oNLu2UN/TllM+f/r6G1aPsIjo6aeesradck7eW6dI3z1lgGznyCJSdOmwdlD37dtLBGWNEs1JKaJ56NBBmw6GrrPl4GjlA5FoQmjZ9ogpOqwdYD1Dgf5mRXrtrRpbg8BWhvAHRub4cp8qVsgH+wtCORgpGFFEE8SGhW9mU1kWl2iDrwWSDdf5zQo4LYmafJIUjsETPdcjFoTt27fbHE0WGK1Zu1bGao+MuYxYFZgbQuHHvZE39cuUIUV08AM/KZyQXT5ZyVzEfK28Kcg0RhRaHsVCoY8YiDcS/TYFIh1ct/kfeWa+t96VHq3nRqNnbhXqD1sw2T0arxThooEL2yclEQMNkSW/rKLUxoj8Ib4s+OGIVamosFidql/WcOG/hqvuY2OIf/GcXyEOxFEJgOYXjWN5ZYW9B4K25xW4ZY4ppBxyDpkn3mwtYUOkep9waRyNzGpaM6Lp0+t6YnlPI03+WTnUa23BtVzXO0b7T7T2z8qSpp+gnWhqZaLv5Mb1G/L2W28ZuYNQMmeLPWRtmoo2DqQDksSXrZjmwvSSm9XVRlLXrFlt+9W++9571ihdu3rVCB0k7cyp00YCx2qn7fTJE7L5ww/NisICAz4ksHnzJjl+/Jg1cJC58vJSK0Pnzp6R3bt2yq7dO2Wn6viuHdvl2LGjcvr0KfuYQqnqakNjvepVo5I61VlNhxFLLa+nlAQf2LdPPvpos3zy8ceyTRukvXv32nzm2zW3tG4otukChVqe0euzGtaWTz6Wi9p4MjLCNI9tGuamTR/Kzp27bGcI5mozH5WGkek927WDu+WTTzRuO2x3CvarnaAdRuot8grdZMu2bVu3yRFtKJkbzdAjacP9ti1bbW50lXY4jx85aovoyJtPNn9s82GZ71yqeQgJjYTT3peWoZpbt+xDFx9+uEk2frDRFv6xmTV5Wq7vgx0zsDax0fVEvopmcdKHDfEkfaH/kBVE0DknmoMN0ltbVytbt26Ry5cv2Xz5Nffea+sQmNs4Sssui1TpNF5KCQaIhx95WCqUNDIPklHEfbv3WLmnQ4kxIey5nCebPvhAzp87ax2/5557Tt9vsb0I62DRGdK2KfOuITxKNvXZw4cOydEjR8wYRHwmT5lqrg4qAYVoUjdh0Ryj9Q/t1o4U0aSNwio7Z94ca5O5x641WDM3b9psu8ssW7FCVqy6y9o2bXws5MxbD+VipCCU90A0R2l7X6wn4X3coUTTYBcgHoF8GOnRgoiVECqUa/sJruCWhm+HVvRMYGaPTYbKK7TiLtSCDVGCMJofFnCwFMS004Da5xfJDypy7qt7yFMgV6H3ZaSSa1ox2t6RqigsnICEcp9CzTChESeVkCD9rY03vTjchDkrIR1hBZ2kenH15jfPAsKIlXFrhCqRtKTjr+HzHPO/YsVpaVExEkmY6o576RzkXuqUI8/gFj8gG4i5VwlxSrlXd1hzG7RRZ9iPxtimL2ja+ZKEPmB5y+Ig6y2BdKAZELcggWS2TSeI13Ld6xjtP8GdzN2YRwTvRFOhecG7ZOEW82fppLA4DYs9+YSekV4ID+CLINNnTJMVK1fKatW5MVVj5Jw2LD9986dy8sRxK5s3rl+3rZH4pF2pdiDnaUPFnOs3Xn/NJuizyGjzRx/JUSWZh5R4nb9wXubMmW3E47gSyg+1sYJEsUk0W3nx7CklYOx/e+nSBdNBRjoos8y/YkoJ80NZLf+TH79sw3dHlcBdUZIIoQ2L5vbZt5oh0ugyHTTqgBPHj8sbr70mZ0+fMZL83ob3jUju2bvH/Dh18pSFXamVMo3vunVvWjiQO+LH/cuXLloljQWXIXLykx0gfvi979nixsl6fdGChVYfbFaCyNdKqjV/rmr8mFO+XQn6fiXHp5VA0nE+e/q05n2zdpzH2dBgBERz7549sn7devlkyydmWSb+kNmTx09ow3tGPtF8xT8adnbpoBNKpzqD9JvvX2QFEXRO60Z0wInmoAE95otvdILYtoxO4YMPPyTFTMVgez7VKcoPHUm2FGRkbP7ChXLfAw9oHcb8xxbt8J2WfXv22kdW7tJ6gNE56jqqiA83brTFqhg6IIDnteO2V90yBM7w+FHVZYbCD2qZZccZwh+tbQrln20MaUexaE6cONnaofTQub7HRx59RKrGj7d2d7t21iCahHPx4gVzO2b0GKmprbHO19YtW6wTi1Hn0ccfl7nz59mK+USLmEJ2qbmzQT5RtzBFqlI7FyOEaBKfcGpDU/qPjLBKQMuDHRUck//YvogGkDkdbF+EhW35ipW2qpz0RUsMw9kWqgUSnqWQElYgRcF/KkEaByOGED4yXV+EXrZ7RrS0R4U/EDNgFZQKbuI8LQtNr+ErQ3IxBCrXcITUBjJLIxmuhzjwNz6bLWRGOKauaR4Fkqt5pf7YEX+JAFBH6lUGek7suEZFC/jLNSyVsUzEStfSYu74HSpnLLTx3YR7uAsVF8/ZM+kIBET/OkYIIwjoyjNt0f5TSb/1LBVXouZEM4B08W537thpesOKaeZh2Reu9F6lNkJmWdP4ozdY7MaPH2ekjXLMXMOL2rjUq/tabcRKNF8XLlpo01cWLVpke9+aJXD7dqm7XWN+1zXU2+dmIUEs5rtn1SoNr8E+B7tp0yazSlSOqpTly1fYFBpgH1q4cNHuQXCnTp1iabhx/ZqR00Bkz9kK2YULF9j8ZxoY3DTW18pFJblXr1y2/WDpMAEI6vvvvic1N8I2Ljdtv94qm185YeIEI5ls+syiCMgl6ZjGVBENGysn89f4dC11x1x9pkoJMPpy6cIFs3ryudj5ep3t3OgAYr2kUWW3C6ybfPkL/+bMnmMfs2ABEY0zn4llXnlMe4PW4ZDQn/70DbO4km8LlAAsX7bMpi5A7o8fPWYWT0jmhAnjbVoDVhwnmk40I8h/dprYtm2rdZKwwj/x9NNKKIstb/jq20XVMaak1SiRnDh5ii1CnTt/gdTyIRKtB86eOSuHDx+Sai3brDWYrzpG/YHfmzd9aKvRMT7wmek9e3bbAt8D7P98QDuOJlgpVfTaRC2n6A1xoYOHMWfNmntl2rRpFBrZrR2r06dO2vlDDz8so8dW2XnYkeasEU3a0yOquzt37rRdYXZs32adOEb4PvXpz9gaAfbWpJ3UR62QZN56pia+U0uCtVeUf9W70NZjHMqTURXlUqQ/g+7dQUQzN2I82hOQ+Q3po6xAam5po0ajx2q5sDCnTOq1YgmkkeHqzLNUdGYh1f+hcrGbqhzRRWsxcGKPWIjmRy5HRrKCE8t+e7F2JyCe5xRtBeyof1pdb0daQR+i8szcJRIgdUzeUrF0JH4TcigPdqbHiOgIxPOMhOdCoxHyMoPs3x0jus343VO0/2Rrf4mfE80YC42T5gWNAtMm0Ceskze1AcF6R0PANiZs0cP3hqtTw+VM0+ADB3SwIJnlWmHx1S38Oq+NEAuDVq9dIw8++KBtZcbUlNNnz1hD0lhbZ1uqsNjsyWeeMcsoi47Gjh4jWz762Cx8EFauP/nUU3L/g/ebfi9UUgXxO3zkkJy/iFWzyBaWYemAmG35+GOb+M/Q+uc+/3l5+tlnZaHN9Z5vi5nqa2vSDRYWFOYNk27mqX304YfSov5w/fEnn1R5XNbcu1Ybu6lm5cTNDT0yTeTTn/m0DeHdt2atxYk9fK9evmJTBVhMwX6E5AND5x9v3mxzxZYsWSyLFi7S4pCvDfRhs/RSmLAOv/DiC5pPD9gCrLvuWiUlxSWWnstKYJkCxIJGwKdhsTRhzWSqEIsbHn/iCVmp+bRC82eM5v8lzRdGGWh8J06cJCtWLM8imvGtpy8oYqmOSN7rBbK8CTrnRHPQoemnI3JICRk7iUyeMtnmPipd038s0mNOdpls+miz6m2ZzNHy/Mijj9luJdRnyOlTp60zypQXdoJh7qZNe1PS+tbbb1mnixFFTDYskoXgcT+OnCWFNRAscIXYnjx5WstxlXbKFlsnlZFDtlJCr7F8oiOsb6AzeOjAQdPN+UqA2Xap+la1peeiduAw5jAv836tf/jSHx1draxsWN7efeqVp0pA6t+dD3SQDgGaESyadyDR7DFShRuJVSJHSBOLYuhRsSKdOYYUcKvQNBMRngmVSkhpe9Iewn2qp3ZAvMJJ6sglwuzI12yk3HbnkQgLJwpI5lAC7fodFCz8bQ9J/7MRr0c3Me1JAdnXooB47B3a9yUZlp5p/EKj50QTgagxXAbpwaI2ZswYW6jD0DfEik/QQToZvjp69KhZI7BUUPHjnsaFhoRGCXf7lJgyXLtSSRMWN0gme7cd1mexaBLmPatXy9PPP2eL0MazsfK48XJZG4iPNm2SE8eO2hD0F176ghLMZdqwlEtpOStix9g5cyhPnDwhtzVeU6dPtfgylHfk8BEjew8+9JCs1oZn3OTJUlDCdmaV1ug0NdTbV69ua4M0SzulkDq+IMacUeZHFinxwer41HPP2B5+47DYjh2naboiZ9R/9uVji6enn3vWhgr5shnz0Mg/ph0QNosS2EEDMsOcz482bbbV9Ms1HfOV7NKQ79q9y6YLUOHfrSSSuW9Tpk6z6QrkJVZS5pReuXrFrK4r77rLiP2Z02fkrfXrbSP41WvW2G4cU6ZOsbweUzVa30WFLRBiw30IAPm64q6VtngQopmpEeKbB9n1Rbwej71AlhdONIcINP1MPWGR2t3awWMfTEYOKCdhJwXm/5fbgtAntKNHR43PFjOixWgf08LYcpAOkO3yorrALiQQSogc8yGXLFsmTzz9pHXWHnn8MVuIw4IjdDNbZurzY7SDNmXKVFmqevLQww8Z+Qxf1cvXTtUY7TCtkIfVrVk5C/KMbC5Qgsm+2uyPOXfefFm+apWW97tkoerwGiWkT2lHc7HGYzQLgNQfdI/6mDKBZN57dqm5cxF1kDLAZvydWTQjUxwxIIOCYCYP8xzjXDJ691g5bTKyKkp6nqAiZltPpXNoke2aw/bR9cB6hg787pBEdxkxAT0VR/8ikogMkrnO3GRWQEMGsSI89vjj8vO/8AvyC7/4i3L/Qw/aTgKMGlBBQWRYKLDhvffl1Vd/YvMFIT5M1Ldv8qtOMiyHjuIfVgzbP1IbJxYMNWqjT0MGOZw4abIRK+ZYcp2vdzCMzdfCpkyaaCSEeYxsIM1nHG9ev2afwFw4b571xPm845kzp23qBztGfPlnvyK/99Wvyv0P3G9zMFkFzxw0CBpkkqFq4kjaiTNEmjTZHGs9Mtw+Rxs4hvF53hb7KSC9tjpc6xc2sR5VOcrSSZqZG8o1GmjIEyvCAfVUrKuYchC+KJbKdQ0PSw2Eh+F5dnXAcmn5pWESHsOXdJyZA0eehwV+zWYlLVO3M/WdsAADkkl9V1NzyxZC3q8NLIshcct7J687V7Fk+WhbVhx3DmhDAWWRnVNY3Emniv2VucMi1tvaYcrTcsiuKRO048ROISx2pbOClZJnKZtYCafNnCFlSvpYBMvQuxZ2m0M5efq0sG0eO5ew28iMtjKd4/QZ6s9Em45GHYE+TJ48xcq0rT9oajYLJzthoGfBKsq0uSZr+61DqM8Th/Ea19nz59kWe2y9N3HKFJsvzT7PrJS3zlYUywXQqXKMaAxLoskr7anYEC3nWkIQ5j9SgVuFqkpAhc8wGBLnQUYB1qC0I4MPin2m6PcXYt4lpW+QfFNtJQxVtC+O/kR8yZmXzVtJAqLFjgtY5GxrrjGjZeHiRbJqzT3y2S98Xn7tt35Tfl3lcy+9JEuXLZNJSoTYCogvQL3y41fs2/roXVKXaCBszpYKW/swvxkyRkOHNYXhdLOk6nOsSlfWZfvkoct8qpGtU/76L/5C/vav/lr+6P/6r/Jf/8t/kf/r//wv8nd//heycf070lBdI/W3auXq5as2R7m0stxWo45WAssw9oZ33pUff/d78vqPXpZv/NXX5E//6E/knTfXy5WLl8O8bYijxqu5MN8sXVpRmHWyrLjE4kh8iR8NHr8hitRBrBy3Ddn1nOk53Iu6RFpIR8yF9so3eRIbfEgldRdDmcxPpb7is7h8kYVvxmNpJh4QVuZu1ih5prGloQ9DYFikmyxO5N3kSZOkVNNgmqdBBOuhOXM4DBSHKJQhOiPoEGqhlYH+13ZUO3/swZxfGMpaLPNMlQGUq6jLtnUg5VzvMX8TslqizzZrKYwf9EDHTPQabmxdgUqY9696o9dtv01tztGZ4iItz6EUm0Au+ZKexdHc0qEL/hMexBj/qL8YQWHuty0C1rAIg3ThvjUvCGnJHB3ZaFt73WHIFLGMoBkUQhQjCgU+l6Q1aaAQWxskHXhn0jewvGkHlncalEn8nSV9g1w+J8UxeGhb1pJvBcscC0jYxufY0WNSy1d+am/bPC32wJs8dYptb/LoE4/J51/6gjz//PM27AYBunHtus2JYq9L9rxkvqZ1ALURgoixihVAhmhQaKxonIyE6jlxgGhB8rAwsiUPWxBBvPhMLBY85hzevlVj8YRMsU8ke+cxn5KwILW4Ye7jd7/zHfmnf/xH+e63vy3vv/uuvPvW27Y4iD0BWX3KfrUx5ZndIvgdRksQfsa4Rb1Jta96DpFMCFmr8UEsLkoIjXzq8wVs26Z1VMj/jMQ6irAYotSQ7V+2uwj8xbJZq+8k7M7BYkgaW60DNd8YJg/DwaEBjvlvcbBGPBttw3CMDFAmEMpHLCOUAsqPLS6lfYWY5QDuI3CPWClS3UAgfAb1M4qG0Fq4nnUNd5FsJiV21JDsewSY8SMVB45ZYk45pPxJhosE/1qj7ZWRi2FJNLMLQS7pCLGSRTk6Q9Jte+6zi1lUmK4iE+dkCroqIB57jhjdmJZcQmHJoapp6QzJGOdC5/5EH9oTx2DA3ptmP5sus+r67772N/Kdf/mWra5mCLiiPOynektJTmFJsX2uder06fLQIw/bMBZ7qDJky3A3RC5YCiBXWCnYQL1I3ZSny0cUiCZD0bgFPMdvOo4Mo/MMQ8OPP/6YPP3M0/LE00/JU88/J08+96w89vTT8thTT8qzn3pennn2WVl9zz1SX1Mj185flPWvvS6bNmyUS+cvSNWYMTJ79hy598EH5FMvviC/8a9/yz7JWmrxUQKmGsGG0uwlVxDYov3jSBPEdY40TaEJzLiJzZRWL8FQkkIkeEmEJGb8RSCjPMfOGHGHi/Q/jUt0Z8/pb4bUCwqxAmmczcLKFjNsrdZkJJNhy0geIOgdI8Yv5X/6t2MkIUk0aScZGcwuuxHRLdNFWEzbnrsI0+uEE1TA1KAdWCnkEcpwlPRTbYV/8RjO9Zd6El20BwsnJZ0h+jUQMpQxTIhm8tVmJBSj3BLva2lu8xxCGc51vSOJ/tq5Pm+SvpZ52SiSSQ/ePo9YYQ9eht9RUtdbe6sX7LrGKSGZdIe/rSWZksy1bJ+7i/B09LE17EoqEfzLuIsS0Tp20ToUxTGYyJSPViVFXxXvjIbm5o3rcu7sadsuhC18+FIHFkTIE8NmzAVkxTnXIILMsWTPWqyJWC4B1jsaGbMU6jkkCH9i2WeInAYNP62BU7bFdSyhNF4M0dmwm4bFnKyHHnpYnnv2eXn0scftm/xPPPmULYDhM3hsKL9s6TKZMW26jFJCfPLYcZNSJar36/0vf/nL8u/+/b+XX/qlX7KV63zRiuE35okCywd2pVBiht5BGiNCQ9lsRJT8MeFSyDBDwrldD7oREc7xs42/QNPIdUgmFuDYwWWojyejxDPcMTxZUVlhBJ75tExzqKtjoVadVFZUWtqYt1ZXVx+e1LgjWJ3aIte1bMRYdEUcA4W+yvlofAkEkiKZKi9aNqNOx3NrEylH4ZE2SNbvyXjZeeJeRLge/DR/DdFzjlliDXJrCQQTSVxNeZX+bb8SR01zFBQ65kEqRiZpfe8jCf4OXwwDoqmFSN8jPfT4EhEyn4aGOV5U8nU1NXL7ZrXUsyqQuUgqeVTE6o5z24tS/UFCg0ADhVhZaeV3R5KrANBo0jhiYbHGT69GdJtsqpc8Yi9G48x+eSxcaEIYUkT0HGE/PIRVsMxHi2KfoFOP8IO0RYUgtsH3eGybmjiUYcMZaeXtKjJ5EqQ1UtHQRi5zni3pZ6lYVFrHzjF4sJdjx0zlHEXLCu9HT2fNmWXD3+fPn5Vdu/jazSFbMFOsBAaCiC7yhQ8snezXyPY7DHVjbbPV0qpHBeoR5f727Vqb42jD0lo44nAuGobQcBF6JF6UEFaXsjCBldJnz52X06fPyI1rN7gjo0aN1iMLavhKV568994G+e9/9Cfy9a9/XbZu2Sq1NbW2cXN9bb19BGHFipWyYuUqJcRlMkpJGLoHITt77mzQD/XNdCVVB0E4LUdiRUOW4EjLsVk2LZp2QZ9r3XGye6k6Lvjcttwn9ZH8sAY2dW77+ul5WtS/SDiTIH/Z9ohPAUIybfP64ycsD2tv19jXf/D3x6+8Yl9yAaQRImtRTEnX0CpGnQiIx5GBZH72p2TDcjnLUU9yPj4OwlxF9Ud1IFj5g46yRJtt9zIuFZRd/RnffBS0wTSC+6lr6FHSTSvhTzoWyTDCEXWiXJt+qNg1jolrSOQC0QcTfZb6JQbG7+g+N3AYj0G/0w/3iYBYM+SW9tEqZW0k/utPDHmiGTIw+TcD9pDbuX2bbP34I9m7c4fs273Tjnt3bJM927bK4f375MDe3XJa3bEVCZYFJsazmIChIawi7Rec1mj9alQhUj01zvmMHY1GnZJcvpQQ3VhBzUK8FyUX4j1WufLVAj6BtXvHTjm0d58c3LNXDuzeI/t37U7L3p27ZM/OnbLt44/l7OlTRqo7DiPcCc10SvS/KX9KUHDNHJOOFaxriE/bMYdWxOokxEpF31X6XCU0yuHYngwuepc/wwOksbXYW9NThCFZNlWH6DF8zdY63/vud2XbJ1vMSnj1wiW5fumKXFYCePbUKXn91dfsSx81Nbdt5enUKVOts8bWYlg/S1SvWLRy6tRp+1TdtavXjCjpyzbSme4QaRwAw8EIW/mwHx9bl7CFkX3168QpabhdJ811DXL+zDnZt2uPfSqz+voNawjHjhmrfkNksaAqqdLjhYuX7HvjpDN0apvss4wffPBBsMpqPLEKohs0tGkimNIbskj/WyMVI4lby6/UJbvMNXVpDS0PpJB6xGBaoPdimg2EkToyNSHpZ7KZM+gJaQDs6XnXqlU2N3On1pdsycT7uXXjpm2/xOf2dmmdYhu2k78xvH7FQIQx9GDlox8lG5bLeoNjUuK13iASFv4VMMqgBTbs7ZwicrG8pkB5tjKMaHvD6EXUDZCOmyLMvWyLqP9BzEMu50bidvS3tevMr+g0Cn+tY5fQh+xj9DWtn32K3vqZnaIg/Avn/YvhsY9mwut4yvGH3/++/PSNN2Tj++/Jbq0wP9m0SXZs+Vh2bN2ijdvHsmnD+7Jp4wbZv2+P9txP2DN8m5jevzUOekRAprB0DdZgpI58JouvE5w8ccJWjPJ1Dcs6Iz9BwfhpvSK7kkL6JCDes8tKYl979VX58csv22fm9iqh3PrRJyofqXxsG1JvVWLJ5tLxyHYxrMJdsGCBNYI09tlh5IIpt0U4NFCRspllhusqXcme7IokG/RwcZFLQDhqdWFhhWrD3guiP5JVSltRJ+ZB9K3naN8H7mTuWhnQuJFNZVquKkfoPpoRkB0+bsBG55BHVnIz53LHlm1yaN9+ObL/oHWMPtn0kerlB0pmdtq30fmax3PPPy9z5s0Nw95adtl/c+euXXLl6lX7hB3Huvp627aEL3fs2bNHysrLZdHixTJ95oxQCvij2RDnZ7IgiAVG9pUbPb957bp9D/yTzZu1bvjAvuzD6u+HH35Y7lp5l60056tER9V9tZKsi5cu2vHq1Sty+OAh+WjzR7J+/TrrTDJNAP3ie+3sAQjxPc/G6qqHbEg9f+ECmad6GKCRUvdsJL1P6wksteznx7YstmBC77HtC5vbb3h/o9VJ3FvGPpr69OVLl+UTrc+w7i5dtjRs5K7X99snNY9Kiz7/6OOPmUXYioEKh1ol6Xw56OyZM1Yn3XvvfbZnqa02VyHv2Tyfz3jyST9I5lbtFHz4wYf2CVHIAeWbfUDZyJqpCEYO0kgF1mdox7+sS8QJvSF+1LHDdR/N7DAGDDkCjZe6E59st/yOQoalf8d8yVyw/ATNesY5wrC7udUfcRQjAkNCdv7GX+Fy+NWqHuSUm3a0KwHZvw3EiVjEm60dEbbVL4rIGRiFsTJoSN1UxLPs+PYeoZ5Pxq5r0v5zwNrb+KOLCDpIPoyAfTSpLJkQT898vFb4kydPkIls2jxhnDV2Y8eOkYqKMqs0t2/fJv/wjb+Xt99+yxqOUEmF7UZsuLsHhYJ96ADfUP3BD34gP3nlJ2Zltd6bvrw4d4yXgiSRHVq0ygXuHtwzZMe8M6ywNIh8Co5tTCZOmmR7lyFTOE4JMnXaVJuDFTsAhGHklrjE3ynJBfIAixAWX0gmMSYe1kPL8VDSv1x+Zt8PQhWdS3LDruufzi2WrfN3oGDxSwQ9OLEYXMQ3yHY/bLa+ZNlyefGzn7XN1BmuhthglWSvzO3btsuhI4flghJA9txbuGSpPP7UU8J+dexVB4GmIh83XnVZyzbXTpw8Ke+9/75s2bYtrEhVN2Gifya/KaOMMLAqnVWvfG7x7jVr1P8ltg3KDiW43/3u9+Tb3/6ObN78sVxQQjlz1hxZc9/9sva++2RUVZXU1jcoQVwkK1atkjHjxsq5ixfk7ffela///d/Lt7+rz2knr6Sk1EjXpMlTLA6XlQDzFaSwrYvGgXhpeCYxnhbXMJklSrPqKNfjVi32nJ2Hjh7p4rp95jadv3qubrAcExa/4+dc7etlORDyJ2heQJ5UVY21ealPPf20rFm71kj5qRMnZRejJkqoIaVPPPFkaqGVhpVjZCbjX/8i6tfAhDbEQLZHSSGSHN4JgvU/J1LPJd88SOdnlkS32QiEQsucSkT0MwrtHWLtaMq/Vu1Nym8kTndBYpmN/tuIROoYHcV7RujT560lRJ+/lozUWUBse5FsxHggAckrbSUdXso/6ho6xe35HRfU5brPe0Twz0ZpOkGMBaOVqvyWT8l87VBw14EMBIalRTMKVkSG3pQT2ebKfCh/8dIlsmDxYlmwZLFMnTFd5i5YYPv5XbtxQ85duGBbr9BYsK2KpsCG6ALJ04qbITMKhYoNi+vR5p7okcuA4XYKCIWIYTM2c96xfbts+vBD27Nu2bKl9hWTem20LJKKWEARI5+ca6StsClJptEgfGuO9J4Grv43ymFNH9YY3N93//32JRH7FN6iRXZcqGEtWr5MFulxiR5na+OKJYVNsUOEKcQh/qSVoQksRgwN2lwsdUG4xMPmvmk4VAQaw1AA8UN/49AKpfkYJF9/ox4Iv23/UfVfU6K/gr/kkeWh+m3D+ZrPtjJRK5gwbcFc2tFOrRKyM30+TE1gs138JO5Wkem9ZH62ktS/3qJ9Hyz0cKogzJC3mv7iYNEMq4w7R8il4EdX3KvDIWrRDOFaQ6DvlnfGghI2Kl+opI2OERuAT1LSOJ5O0tQpVk4XaPm9X/X1Pr7eMX+eDXMz0kA6GbbGOgfZnDJ1qlkNx+s5cyY5LyktMwLLwhy+3sFG5TRaoXwEvVJPbCHQnLnzZeyEiVKu/o3XDtqkadNk2uzZMkOfve/hh2XtAw/aPaxVhElHbaySTL4zPnY8HTs6cNPsm+az5swRPiv56OOPW1oqRo1SsjrLCDHWUEpCmZI2rKxsID1h4iS7FkTk1u1aDWOMDevPmj1HqsaOk4LCIlqdQBo079Cj6dOny4yZM2XKzBnSpHrAvphlSuD5RB51C5tLo8NsYG9fNtF8nqFuR2ueWPrtb54RSBoxPkTBKn+sqBBiW6Wu4dE5Z2N5VtVPmjTZvtKyWOP+0IMPy0zNOz4PyNeM5msdyleKirTepNyFEg/6usyF8pyNdGipE6sr9YfVlZo/d6RFkwDVfz4O8MHGjfYlLcoAemVlXdNMPoQOeJ5c1Q7PJ598Ivv377f9U9kHlbrX6lktX7F+j/VN+lyFc8vLrOsI9TZliLCweG/csFHbDtVPLXdhr2l1ZzEQravrrV3h2/xbt2yRy5cvyxSNh7WX1As01ArmcFvboM/qn3BUWHh2n/aBHE7FQcOO96PbaBCJnSA6RWfPnTNrPFt40VkKe2TmBs/hLXHDT37jdfC9Y8Q4xPy39oxo6GUO5DnpZbifdJJPZrzRe/wOHAAfQnhMFzqwb7/sV2GUgfqSuMNH7Cn1NM4XxT3hx/dk6df/hBfiE9pbEDohMc9CgMY19IjEdBgSp10B4QT+1zWLZl7B3J9rKSiok6P7vyfj1JXZ6Cwm+idVMCK4fOlGtZxV0taogeTnKc3gonkdCklfw7Il0m5O9RDllR//WF595RWpv10jX/3935O7Vt1lGcDnq3gRvEAslnwbmflir776mi0+oAH8zGdesIbLFhioJHtXdqTR1GthPz9VKF6eXuM+BYmvlmA6Z4jwnbfflu9/97sySxsHLDnMfyooCqs38QcCZvvQ8ZKjginMAkKiUgVDPbYXhRs2nX5D48s8Nojjz/78z9tnvvRh+zoBfkSCao/qkTiicBA2/GCagCksbvWZehRc3TEEOVrJd7WSFr5SwlCjWU5S/uFPrHh4ljQQTdJtizM0PYQB2WThUZwfRr7XamVjz1A+9BoVHCAf8cQqvJTi8RD7I5pCarix40IaeG9Yq8lf4kZecx2/LZ4oWIiWIRyJc+/LYUh59JO/0cfgf4TFhbKm51WqaJOrKqRYb0O+OwPpBZZXdpYLqRjgVvOmXo8nzp2X201KSvQ3Okd84j9zm5KBhuWF6ikkxkYJ9HdtTY0tAGLhD8PNN6g3tFzyiTqGvdER0sWzVt4oAyQ15SeLVUJngy8BsVF7oX3pxhp3dc88SRpeygVuLZ/SUD+1XFEH4I5OIRW3Vfyq77YZM9Z/LYe2N6f6R7g0HPjNXpN0cmq1/LG3JlNSCjW+XOOTdtQNFFd0mzjyDP6wQp6N5UNDE8ozIM4UWeJBeTY9VfA8+kS+6UMaARUaLr3XpL0WSEIhfmsecLFSG3jCoG4hrqSFDaptD0ziRFgqMT3kHdfJB+ov4sRUH6zL1DHz5s2VuXPmmo5bfqtbSATfVv/ut79jHfP7tTPw4uc/J+VKrHMtMOo7xNi3hYVpYYcOLOmjjGmmS6m+l/FKpkeXab2hzmLnt7sgjJi27JjQOtxQAnNWyRNtH1rfSH5rG2g53ULehrqnq/mTHUYrqCd8M3/Dhg3yY23nrl2/Lr/6a79quyRQZjGQ8L4Abcv7770nL7/8stxUHXtSO0MvfeElyx9rY4geXqJfes2+aqXHZFoB93n/XDdipG4oZ5RVyu3rr78u6958Uz6nbdxzzz1n+mR1cUrYxgy8+462h9/7vqxevVo+97nP2a4S6Ax76hJ3/CVO6EGsB3OBd0w9j9+A5xA717w2fzS+EFd0fePGD+Sb//RP8sjDj8gLL3zG5mpHWP2SAM8B0wuFlX0NBwKbK0YxXJA818ilDhxDjRzbz9j2xToMUA+gt0F/mWteYDtxvPrqq/LTn/5UnnrqaXnkkYctjPJUfuIutH0YuNiSLNQ3MV9yxReYZVhjhDvyMTuvk3mixVf9ac+ntqBtJs+pr0oK82XqxPFSpj/RPbTDFnTqmfmof4a1RfPjjz6yeZFU8A/c/2D45JQmHisBAhFmhWm5Vvzjxo03xThx4qTN2+LFsXABBQgNSL1l/PnzF+TIkSNy4viJ8E3mw0dsZSYbPtfX19rmzrijgJy/qD23rVvk+LFjRmDN+qm5ekuJL997rhxVYQ0Jq26v8o3nQwfl4P4DcujwITl27Kj2Uo/Yp+3sZZUU2/Ohl4byNMnhQ4dlz949ls98p5hPeRVofGlc2DSaOISGJnzNIM49hWSyOnafNiZsnM0ehXwNhLlnRzVcGhoapwrtITOn9Pq1a3JWe6sHtTe8b89eOXRgvxzYv0/z9rhc03iziApLC24pq8j58+dk965dRiBQqCvaIB3R9BzXZ1i4cenSRWs4x1aNsfziGr3yI0cOy0nNTxY6URHyuT56n5BY8o7VevV1DTa/jricOHbc5o8dV7l44aItCMF6MUrzF39Jb5BUWUz97g3afzqWvAALRzODK0mLZtSKjhBV2uKeOu8Q6m5oWjQDQnpoFGiktOOWqgwhUVR4RVq+y0dVSsXoUVKk74+GgvLOO7T3aM8r9MQqvZA0mxeIFBVr+VM9pbI0y6c+T+cD8hY/w6g3NAZUcPqguiFOdVrBc+QLH8ydLNR3RPh85o7r9ar3lDk6SzSC9TSeGh+G9Yk7hHjUmDH2xSO9YC+XvUBxk6fprFddo+NG6Q1fPiFtBXLz1i3TVa4j4dN7mlZIrpb52Miit8TDmj3CRc80LsSDazRaNFL4TaNoYWmirQOrflG/kO+RlFvemFcaPz3HDXlZq/fzNf94F6fPnJF3331H3nn3XRuRwVrMhu+QWL5Jf/bUaXnnrbfk1KlTFhc+JTptxgzLD3sv/YbUS28PqVs0nFa2VKg779Q5mtSrH2kbt2f3bqtnx41jxGyZ3bP3q0KDf/L4cXl73Xo5pW0hn1G95567tfMw2yyPlBnLJvRDYR1988B+GjiNumh7s+rvMFcaixr3RG5pfXxM6/dz587K3Xevkrlz51iZw28INlZP2lcqgNta9sePHy8LFs7X4zgjgpQj7tOu8ZuOVYhSiAh+RILGFco4nUvibXHjnr7naDixF4JDPfLBB8rtwYMHzKK7ds0aWblypYURwSMRmc5s6LRY26MXrDyF/23ELMcalzgax3M8T1zi9DskxC3kNYBQkt4Iy191Q/uNnxi4IpGn07x8+XIZO7bKOAvXCAPEvAN0LCh78Z1G4C9xzVhPg2XVoBdC1DLPaLaGxCHdRNBBPOiaRXOYEM2M3zEkBFMzxAWsXXuvTJo4yQgUQi6Sp2Zh0RcZN1hmrtiVK1dtGIKP+LMhM36xRdHmzR/Jlk8+ls0sKtq+zYYitm/bZitNITtXlHRBgKZOmWKF+6ySy1deecUUvbmh0ZQRAnbuwjlbRTt+gpJbbch279wpG99/XzZ98IFs2bJF9uzZbUqxfes2I5yQQL5YMpvVuppWXiIFGeJ36NAhbewq5C6I5syZlhcU1lhgTDn0RVMIuYKFkhaKbzq/+pNXjQxCgo8pGd6gcfjo449tzhsFfNrUKfbVFBYSIcSPaQC7d++yRQS7tu+QM9rYkDbiRYNEw1avFQCV309+FHrQCCtxP/hgo3ys+bePRQoa75rqmzJOlQZi+9rrr8n777+nebxZ9u3ba2QeEquJkXFaKRVr402Dyju6cumyPf/mT9+UnTt22MpYFjrt27tPTmt8oBBYyah8cI9lDHUi/Qk96jEoD7kRS15ArFS4MtKJZghdY6Fl0cqmSuzAmf4ilFMqaa1graLSJ2hELB+zQP5QSZpbXKb8uKnlurA4vPvwXe9APs2zGE5K8IPGEOtlkzZQkD2b36jnRsrUDf5EqyRxYioLlTrX8ZY0cL2uvi7c10oe95ZGhY2IaFks0joHYodlkTtYKTQYhUUspEefofLn+dhomNUhVWFzjfme+BkIQqi2mYaDBauxqcHSHgmlEVLVGb5hjnsjlRZSyD8jhZrnWG5ZMFTHaIPGH3J5/tx5q1tuKSm4dfuWdQQhlge0I0xdQCcV6+3sOXPk7jWr7VvV1rjhZ78h5FW7SN2yssO74z0Pc6IZEN5Z8gpntEmfKNGEQJZpRwddWaMkqkjrGfSCtq1JyxsL6z75+CMtW40yedJEufe+e21OP36gf7duVcv1G9et808dW6LlhvUF5CP/LL/0/Ix2QDCoXLt+1coIlnkjLRo92r316960zsjiJUv0iZYwQqHhh9zQ0LRckhsYZKZOmSyzZs2yMAiTODDihX6wkwPGjdGjQ9tLecaIEeNbfbPawrty5bJZ3SFijM5BRtHdqHtRhwmXcrxD2wqMSEsWL9G2XcM2V6lc1R+4jSSTsnP9+jWbGkL7Szli9xjuWflKPZcU2h7SC6ku0XfAhyQgi4x+YNiB7HLOtm0YXjA2sXUbejRq1Ci7hqEEPR+lnW6ONpqodQJ5xJQZpjyQ59ev37DFfMGynG/PsQMHv5meEMpMJp6ET56yUwTchHwjLpxTx9joprrTCJt70N3ymkTQwVBv3SFEU/01r4P/MSRkz+49ZkWjl3PfvffKFC3cVLbEO8349ZyspVCS01jTTp8+LZVaCObNmydTJk/Re01W6b726mtKanZqwbthczj5qghi1suz54x0XTx/QRbMXyBTp0+zrVMuqGJeuXDR9tIr1kZpohJMCOycubNtQdIBJYsvf/8HRhpxM1fvTVGCF0noBfXjtFYk9XW1MkmfZYiOwovbQwcOyJHDR6yxo4fGcxBJ0k6lYr0qS2cYpqPnxMIhelwoK9uWnDp5yogmG2gzhEiDdVMJ4MyZs2SV+smK9XWvvy4nlIgS/8WLFpmld9bMGVq4r1raINKQSa5XqoLUa6OLxXOzVnC31U8sszRURSVFMnPWDLl6TZ9TcomV9Lr6sXnzJrPiYt2cPHmiNZIXL2jenTtnRWy29o5H2b5+DXJNOwEfvL9R3njtdbMUY+1kpTH5hWIyT2jX9p1SrfFZqhWeEQVV+DDgQ870Hu37wp3MXVPUlOKOdKIJYgzQJypyyiHvB6CLoXJiyDP2/kN1FGF5kRLyx57Fjf7WP3qaZ6QRyxq/Ia0I5Mfc4FvKbQSkzahq6nniZWUm5S8S48q7LIJIck8jEIhMeL80LmYN0mvWJKauW8OV8sOu6ZHrVvdkQYPXsDUs09fgjsfUtZ0TLxpVQLos/cSJRkKfIZ3UazyE7zTcHEkjsBDxMIoC90xXIAD0x6YTqH90sNlj9MLFC7aCn71BD1DfHDosp7WOZMN2puo89uQTMnfB/EDoNYS2qepLEOcQ75wgWRw1TeQ3eQ1BGK5EM6B1jga/QtgYNj7atMnSQ30Pmbj//vtszUFJmXY8ND6Mtr371nrbposGf9rUqfLAgw/YTgGntd5mZ4f3tIOPZZS27Yi2cxAh6tVira9YbEp+Es66deusY880s/0HmOtZY/N7J44fbx1/hufp0FCOMJhguNivnX++CMaiVKzxNUooX/7hD22B7NLly6VW27WXX/6RbN221crz2++8kzI27JOrSvLYBYG46CtUcltjxBp/t3zyiW2xRbgQ7gkYI9Rt6BBiVgj5ZPWKXqNN37hhg3Xw7l17r1lUKa2mj5rFlBfykPbxhrZJ699cJ1u2brVdItgakN0oaFunTZtmZQodua7tGPNjIdhvrV9v001YF3JVCeOkiROUQ5RbG/yJ+sHuMDxDW/fRR5vlnHbkMGZhiLp06bLyhPPywYaNslH9I294DqKNm6NHjsh7mrfnzpw1I9ao8kr58Q9/ZP6OHVNlCyHf1fuMbmLIgsBi/eTdQbA//uhjjeM6M4oRv4va3tNR+FDDojPCAmnyDmuM5Rp1T4/KagahLqdkjgCLJkQTkgPRpBfHS7LhHb1nBRB3qWf5Ta8KYgipxKowc+ZMWbhggRGwt95+WzZrAeFLJXyDmTkoS1l4M3eeLFy4wOYtHdYCAumkkWArlgpV+EkTJ9q8hFOq1CjkAw88IGvuXWMT/pn8/Mbrr8nhAwdtjtUXPv95efKJJ2T5ihWyREnSrJQF8/Spk/YlEno5kF/ijeWDoXuGK+jllWrFcunSBS1I++XooYO2UOiopuPokcNWeRw5fFhmKvmlUaKHhHV1qyorRBblZIED6brr7rtt8cDSpUtN+X7yk5/IMfWDnhTzaVh0xIIqFjRQeVQrKUWxiBQLDljdTgOHchzR+LF9DQX/Xq0An3/+Obn77nvM8nrmzGlp1B4eaWMY8TOf+Yw8+uijco82XhOUUFcrmb+svc9yJQ5MCWACNBUGlsvNH3yoynLR4vDUM0/ru71f5i9caCuJqYj5tCHbskB65+g1fZk21GgNs5WM3qF9H2LJCwgNXSAJTjQDiAWWl/YQYxljn0Sy8iN/knmE4+S1XAh5kO2rInUp3qHhyYa9BxWrQFPX2gNucqG96yCZtoj0tRz3QBv/cjnTa/FyPLaJhd6gTiCYaN2gwzlnDiM6VTbv9eaNm9ag04gzZ3PNvWtljdaps7WeG6O6eYv5daZf/QlSEFORA6lb9o40MZZ/w55ogvjG8CPkMT5Rr0O4xmg9RyhYqJ565hkpLS+TGiVl/P7wgw/koJK22TNm6ju8rm3aDPviFW0GW+K98fqrcl075fO1s0Adu4t9pvX6xEkTbPcSOim0a6+99qqR1nna3o3Rds2mK1mbNtHaOAgfU9WwuNOeYNkco0QJN8eUpK3WdgUrKSTnp6+/ruWtUZ54+mnbluwHP/i+WQMxQFDWaIuwJB7WdowFaXPnzLG2aOP7G+QnSswoi/PmarlTMsWoJdPdsI7SbvDuseba/MAUaDdoDyBmuLlH2yAW87GQTjOU/+SsGXMalBRDMplrypQbjELs7gIhZ+HVjOnTbbgf6+THygc2b/rQdqlZtGihVJZX2DaJtMtjIeBK4OhUv/7aazYKCAk8q2SRL2/BI4jja3oPEhwX9I3WfKNjh+WVtp53snvPbtmkRB8L8j2aj8Xann3rm/9sbTp85VbNLTPyMDoDwafDa1MoNC8gwkyDobwwegjp3717t30o46TmGzxjytTJlk+hfKbKaOrQUwQdpKx2jWiGUn0HgQyIkrpiQgXKy8eEDTBpY2Ym8xmOwyw9ShVsnhLP1WvXyDgtRKO1EMzSynjhosW2qt0mNOszKBMmaZQG8sUwBdZA5kZOmTRRFrCKtrxUzmnBYzUgYd+nROwh7WlOUzI43WSqLF++TNasWW3khAJ87uw5EmDvp1n/McxA3Jua6mXn9i2y7vWfyGsv/0DefOVleeu1V+RtrUTeevUnsv61n8g6lQatfDRidFgMWCGYRoBCMIn80Scelwcfekge0eM8VZwDSjBvaiFmaO2Bhx6U+zR+VapkVfTWxlXJvQ8/IHetuUea1L8r16/KxcuXrHI1i5AqA0fm8fC96seURDJkAWG9a+VKqzyYS0JhRKEee+xRm38ya9ZsI9oPPPiQkWn2CGS4ht4sltb9+/aaYjL14P4HH5SVqnjT5sySmdrgLdb8eu7Tn7L5YgwDvq89WI4Md5Lo8KYdQwUUw2wZDJjVMSHZ4FIUFilgL+mW6DNJP6JkI5cbpKuwuGdJMj3xcjZwQ0MerDrBCsrcU4bFmX/5la98RX7v935P/v0f/IH8zu/+rrz05S/Jp1580XazoDN9u65O9b2VCcLRD4hENbZd1IOs3J45fYYRIDoCWCNptzCuMP+efU+rRo2WWYy86TuCADU3N8rZs2fko482mbHgZ774Bfn0p56Xz3z60ya0D0wHw/qIIeAVbU8YXfrZr3zZ3D315BPy/HPPyo1r18yQwcgbQ9m0eWM0rCee0PvPPy+f/exntc5fbJZXSBY18InjR5Wk3dY2YZ4R31tapxNvOjhce+zxx+RJ9Z/dWS6cO29hsGhw944d8va6dUrmyuVTzz0vzz79jDz+6GMWX9YxvPPOO9b2QDLJJ4T2hyMEjD1oWTDFzgmMfkHOoj6E3CR/882f999/34xSP//z/8rS8bjGafXqe+T82bM2ZQzieuDAfiN1GH8++8IL8qS2mZ8yQ8oqqamptnzRjA5pPnHM9IpRgp/7+Z+VL2p+M0+Wd0f8+EAChrAX1Z/HVd/Y55rpZJBILI4c4SOMovIOWYtAmwihpO186QtfsOfYNeKKkl7cEt7OHTvlvXffNXcPabvO53J5J4xKsvMA72yi5gVfNWO4fbDqX3DHEU0QFZWKNShvKKCAORY256OYybgtqgSqPNpzosB99fd/31bU0cuncqbQMxfwmr5cFByzO5Y4FIdzlCiGYRP2lWwW6wvlPvM36K1hxfzd3/03psClxSW2kpBnyfgiLfj0XkZrT4nhOKyQtjpMBVjcNY4M21G4IMRsIE2vEsW6oT0/jiZ6j2TTc8OiRIHleSywYzQ9CxcutB4Vw2/MbWOuGhbCX/7lX5H/7T/9J3n6qacsfgx/oLgIaSJ9o5RQU1CpaPAzTeRVqHgWKNFEyXDLYim2tBk3dpyFTcW3XHtfDBPwDBUOwxrsAcp7QCFuVUP4821YnHliDBHN0R7sYo0zlor4ac0CDXKaVhCrVqy071LfuHZde+OHTIH4rKFj6IF3k0uGItANNA/i2B3hmfhs/wLt7lk+4tYWVahgbbJdIPTi6KoxtvUR01Nmzp5lw+RzFy6QkooyaVR9ratvCPNO0fchiJj3dxIgEYC5jGzVw/ZaU7Teo826cP6CJrjF5u99oB1tDB93a2ccsodFjLqXDj718c+89JK2Zy/K/LnzbBj80sUL1pZRn0/QtolhayyXbOi/ePEiWbZkiRETyOm9a9fKb/z6r8kaJWBY+niWhZv33rtWVirhwQ11O3U+VtFxVWO17Si2RZu0f5Bjplgx8hV/P/jAAzJv7hxrI2z+pxIkSB3WQ+bfX7p4SRbMmy/zsHBq4aQNnTFturWXLPqNoK1gGkw4x9JbK5cuX7K4YKnHwELHiNFN3Fk7qv8xFMUvXj322GNmuWT4metMFWNh4PHjx6ztI77kHeRyxozpmufNSgjPWT6STpvKosJCX9ordOrJp56QRdpmYX3F78OHD9tQPm0vH4SIQ95M26Pts90pmprN2ktbRp7SZrJQlvRjQX3qySdlzuw51l5iJOOdEf9rStBZ58EUuVV3rbI2lu3M2Plm1aq7jNMwCkv7THnhq2a5YPoTTvsVdyTRzAWKJS/okvYWKEiY/iuV6fMSIFczpk6V5UuWSokW0iMHDsi769bLN//+G/KX//NP5et/9TV5+Xs/MD/YwoGCxXmJKgIZSAHHhB4n3mKKv3btqr14eiRImZK4Q/v3yztvrpN/+vrX5W/+/C/kT//oj+WH3/6O4IvNDVPFwy/8JsZhwYsqTVGJfOXnf1H+X//Lf5L/+If/X/lf/vco/6f8ryr/8X///8j/9p//Dykqq9RKF8tDIIOQTcz3FHYKOQrIajfAfebx0MucOmmyXL9yVTa+9768pfH77jf/Wf72L/5K/vyP/4e8v+4tuXHpssZR/dTCynY1ytotrazAK9WCz5DJ6MpRlpfkLYpfVl5m6aEiRPlIV9jiibl1FG1+F9qCBnqqHJlvUnOrxiYyM93gh9/6tnz9z/5SvvHnfyX/9Jdfk7/+H/9T/uy//ZF89P4GKdHKpp73qT3xFm0MVdPtXYS8czjuNMSSzbH7Yo0zdQuNrsIslHotWIlUH7XeYgpKvuphrep2vRIASLQqrtUhQxFJkjkQjWV/I05NoG7GsHBdyQeEBCsd04jYsYA1CbQ/DH8fVSLDPHVGlPgqFXNoJ2pdXqqE8NbNW7Jt6zbbAvAb3/iGfP1v/07+6R//0cjp1UtXjGjS7kEMsSguX7pMieNYq7Pj1+0YgWKUivn5DAnPVMK1eOEiIzvsWMK1q9puQGggeAzbYixgfcFCjdNYJbsMx+Pn2tWrZfKEidZ+NGl9fVPTBxnGCEJ7wWp2pgIwx/BfvvUt+QeN81//1V/K977zHSvBt5RQheIYyjGA3DGMz2gkaxBYVAohg5CzI0UraAHBKAFpJg4fb/5Ivva1r8k3//mb8s/f/Ka8ue5NI8ashyguLpTbtTW2o8wbb7wu3/zmP8o//MM3bArAhx9+YO3U5MmTLB7s5MJ8asgk0+ziGgreEWliKJ85o5BLeAHW2ePHwjQ78hurI20e7jDA0Obv27vX0nbffffKJM0ziDBknHUULDoMU89uWt5CVuczjU/9Jl0YuiqUd7AYeLpyGt4x+Y9/HaG/9SeU7OEKfaO87JhJVvGE0/R5/A0olBRqnmA7IXomlFlIEEMH3//e9+Sbqozf+Zdvy+uvvmqrz5nTQi8H4kiFS6VMQaaXxgvE8sk2BFgiCctW2KniUiBYIcYk4LfWvyXf+Id/kO//4Afy6muv2XxQJiIf114aK95qWdGqz9pQgD5jK3c1riHOTOAvtC+kTJk+XabOmCUTpkyV8SmZoIWJ3xNVSssqrPGIgPQCCj8F0obQuI7oOUT54L798srLL8t3VaG/o8JeXkw+5vN2KAQrC62R0mfofUEyqQjtt8aR/IfQUiHiP9eDhMbNfpE4Q+qOtQ6x0QsCCWX1LAoEcW1UwsrK9gP79tlCqjDv5LAJignJZUgAUkrlQ1wcdwbSxWWg0ar8dk/isx2BdOWSriM71O5IBpE4on9IelGVXrP4YAVKWYwiuhfPgUG0Ig/FuPUGvAvqXUapmEfJnEzm/WEswIJG551dTKgD165eY88wZ5P5fsx/hOyxYwrDqpDEGdOmyZq1a2x/ScgMpIMpThAf5gpST2NwoE2jHm1orDejDO0ibmnfaP8mjJ9gi40Y+WKxCau2mfI0deoUa0uxsrHCGn8hVvjB8D8EiP1a7U1p+8GzWGDZQonpbAwxs6czljuILYYJhn/XrFlr6ygefeRR+fznPhfaF2szAqzcKqmlzWDBKxbWCePGW7yxhtJqJdskiBkjkywAxv/Zs2abBZC0r1yx3KyXy5cutSF0+7Q1pPzKFXU3y6aDseC4agxb8TXaji3l+l7OnDll1mVIuZE6OnAaJu+PdgriS7yYF00c6EAwqgrJJJ9YQ8FevQzlQ7xJD9MeaFMXKannGUb88OucchTaxvGaRrZ05DplYlTlKOMk9r70OvEn/cy1xh/a7Y6mvgyE/mRYyTAEGYRA/uxc30qro4qBt6XgRdHLQKGwxI0bP1YJZ5Etstm0+UMtXK/LgYMH5NLli7Yib8asGbLqnrvlcy99Xn7+l35BGrSA2XxACrIWKpQGKx49PKyF9OoAR6yI7PLPZODXf/qG7NyzW46dOilFFeUyfe4cWfPQA/JLv/nr8tLP/5w0aPzqW5qkWd9GKiUa5ZiCFimvKJOyslIr2CWlRbbFSZEei8uKpVDjzx6FxDdumG1zKFUoYIEQF1gBjQSTIeh8jfuhvfvknXXr5K1162UPvSgNK197T8vvvkue/cxn5Od/5ZflU5/9rBTqs7cg0+QreZkimgDFJ/9tZStXoyNzSCrCMfy2s/QxphDwmwoJgslcVyqYJ55+Sh59+km577FHTR568gl5+tOflqe0QnhUzx95/HGZrApvG/Trs47hj1ge4nEgEfXDGqeEcC0pOe8HLwz6s934x3sduRloxLluEXamkeNI+lDdbOI52MjkYdt4xfgPN0TjArh85bJ9ppWFlXyBiu2lIBNMLeKb++zLzK4hSxYvMjJJ/cvG6HzhCcLHCvF58+fLF176ojz3/PM2h49PGEM4aJuwfGIwwPACQQEMBbP3Jt++/973v2ufbMVqif8QpLLyUg2jXNs6ttgpMAPM5UsaxxnTzXADqWRTdqZgsJ8n8ySvXr1se7Ta4hiIUIpEs+MBI2oTJ020eDBkzBD58mVL5blnnpWnn3rahu8r9FmIKRZdkCynYZpZs5EwLIWMqmFdxaLKUDHT1EyU/DL1ivAxALGm4sEHH7R5qEwZW7N6tZaZFuMEy5cuVqJ2RnZs2yorli2z6QfPP/usPPbIwzJV30FRQb6MHT1aCe1Yaaivs51RUA3izToN2kasmWc037CqTps6zUb3WIzHaB/zMSHG7EiD1Zh1GVhymX5G/DC2kBbygwVATHHhvbMXNot7IJUs4uMdIrTvGLboDJA3vJPt23fI9Zs3bKEsfIVt3dpYeAcYQ751TlcaKto/4VcrBKtZ2olJcJt6FqR+szKNSbiQsNGjR9kWBXV1tdobYjPxI6YYbOfw5a98RX77d35bfvGXfkm+8MWX5P4HHpDpM2aoNy02X4mFOszn4MUalVLPsWSiRBBQ5lQyz4aVbKyaoxBQYfzmb/9r+S2VX1B/P/3ii/YpvqoJ46RJa/M8LQgUCiyaRBilt6P+pbdKZU+hYzUhPc4mJaZKTdP/KEzss8czURmxhgYCyPYsoWdDYeYuZHPP9p02RE1YTDT+6le/Kr/3+78vX/zSl+QRJXZLVNHYf08zTEq190WDhHIDs2BC7lINEIoUuCT+hzAyjQDHeJ48C7C76i0WSuZ2ss3RLK2snv/Up+WxJ56QJ555Sj714mfkyWeflqeee0YeVKVffe9aeUiPrKInvZDd/kPr+Gej/Tvto3vP9CSEoYCexbujN4mPuaS/MVDhDBayiWT614AluuOA7C4FI0fhiJfStzoqQP2KGMHuSAZxVIb6jJEuLIYsbGFLL/s8ammJWSA/2rzZ5jk+oR1tLIinGK7W+g/rJ2SG9oYV1XyedOKU8E1+5vPv3r3HVoFP0mvRIIEVlGeZT8j+yMx13LVrl7z99ttmxWQomVXrtEQ2r14JJEPMLMLkSAoWLJgvEyZOlHMXz9tnUVlAW1tfax/vID4MM7OoBre0SVhBCQfr6hhtbxnyhogSN56nzaJ93bZtm3y4abMcPnLE1hfEvIntG2SMdJBeyCbGFKytrLvAYAHZTArGC0b5jig5ZLi9RN0CthYiLAxRGGUg1pBF2nGsn/jLtW1bt5pFFGukDeFrgrDM0tYzv5K2EQ6A3xBKpuaxNqG5KfAGpofV1tZZ/EZVjrY2meeJOwSSkQYWDzfgnxJULL50DBohztq+sk8mxJupFDYdrrLCFkCx/ynv8MzZs7L5o4/soyy0x1Vjq6TA2um8NA8waPa1LX39iyFPNAHFKlAXEH7ZFQqdZqL1xFQhKCTcoZAwV4Gtg5hvwp6LrCbjs2qscmOiLtsaMTGXgk/vCkVjeJl9ytiCh+//UnhYWc4L5z49L8Ki58keWnpipJUCCd+FCPLiKUgsnmE4nv0kidPaNWttwi5zOTDXM3RBT4deoO0LSDpiylIlwAoDhUTDoyBRMdhwuP4PpC7Mr0JQOHtMr8ch7aiQdh3/9Sdi/uszfJP26qXLZm5nVRsbxjIMwtyOKiXirDY8c/pU5JI2jMIpz9tUAfvB9ul4F+KIhHwKeZW5FkhuUvA4xpPfLMJi/gk94OPHT9i2Eszh4VNcVEasROddbNu+Tf7l29+Wn7z6qu35ZoN+qfzre6QSbwjnluwoqUBJL+iqQsU0J31vixBKfI8xr5LnyWtDCzFOIQ1dkUy5af0m7RodHMpcO2L3Es8PhkTEFGUjWfaTMtiw+JN/GhUTPTed5V5w0kXElPdECKu1DmfuBGT/Dgjd2cG3uMaYx1zrjgRAUmJZwpLIb+o9sw4qOZswaaJ9nIKpVg8+9LBMnjJV31OzXLh00ayeU6ZNM6JGezRWO+z7Dx6U73zve/LjV34i3/72d7Su3GfWREirfXlKw1m1apVZFte/9ZZNnfrnb31L1q9fbyuj2REFC+WZc2eUSE6QcRPHWxvXoALJZB9pDCN8ZaumtkaOKbG8rQRzweJF1m5h1IEgsujUPrmoaWRxGXtMQkArlVyxVoKFTnPnz7dniO83v/XP8rdf/zv5wY9+aO3dM889K1XjxoUymZIISBor3sEBTe/f/t3fyv/4kz+RP/7jP5Y/+9M/lf/+3/6b/M3f/k1YvT9zlq3+Js/++mtfk2/9y7/In/3Zn8v2HTuUuGnb/8ADNg+WdoYV4GxD9P3vf88+p/nNb35Ljhw5quT/hr6PMER+4cJFI45LWNtRUhraQwVrEJi7ma9uZsyYqfVz+PjD8eMn5cLFS2ZNhuzTGeADMnzGdM6cuZau3Yws6nH6rJk2wmj6qL/pYECoWezF2g/ybeXKu4y8fu/735f/+n//V0vTh5s22TQEdtDhE5zcz24fYqmLMhAYFkQzA21Uk9WNvlAaen6dOnFSrl25IrW3bsktJXvnleVfViJ15sQJ+fjDTfJ9VbRDBw9YT3CZFgwmUDPETS+CDWjxBUsaQwT0uCCQ9Ex4uZij169fZ5ZFCjsVANchqUY0VfkxydfW1Qvf5r6ohen2rdvaqwqrz5lL06D3uJZHp1V7ONXXbsihfQdk/RvrjOiZG3VPz4VVfEWFxRL3a8NyiKKGlGYKTasCo/EiLpQprKvEk+dQVCPj6iauvI/APWZ55opgmm9SYsnelw1sDaEVwf5dO2XHli2ajjol7+HrB4ROWFQ45ENcMKAZaX4mYQ0H95F03O1HCtwP13A7YeIkmaGVAZ/oO3j4kPClpnP6HplLevPKVanRXvmh/QdsziuVwI6dO2zVrNJqU6iYF32P7HjnuuJoi5hLXREQj+0j+6ko8W9/oa/KVtt4DyS6H2JMd+fpj34nU9d1Qf/jz+xwuJxE/J1ynkbH8etnWER6F4NIoBjOZqgTowT7FmOZLK+skGkzZyhBmWykbPHSpfZZ1dvartRquzFmbJVMnjpVxilJmqfPPfep52W0ks09Wl/uP3RIKkePkYcffUyWLFsu+XyNxtqxFlm9Zq088dRTUqJtAJa+S5ev2BZ/jz72mA29EyOmZhF2JYYZ/U3bAlFl9xLcsIF8o7Yt+MnWc5M0HsVah1++elWm6Pm8+QvC7gxa19Om8ilYtgxkpwO+bIXVbfXatfLUs88qmZ0oR0+dlKs3b8osvf/5L35RVt69yiyotCVhy78A2h9+M4o4b+ECyw/2Ob3JULK6J3zmud5Qv25rG6YNnnzms5+VVWvWSKOm7NCRw+Zuzrz58mm9PlPJXmlFhUydPkPz6lEZP2mSkueTcvDIEfts7QMPPyKr771Pxo6fII2ad5evXtM8nyaz9LkGbbfzbOFukX0mljRirMKYhXGE8k0+spXgzDl6fexYJed12n5VydLly6SsotwIPHt/TtW8XqFpxg/7mpJe5z3MnjPbOh7kO235PdoR+PwXviBLly2zMJmfy8gr72PO3Dlhdxltm8mnSDYHS0fyCub+XEtBQZ0c3f89GadcoZSrxIZCD4FIgMuXblTLWe0VNSpL51viIeahoujvRCRjw/e3v/0v35L3331XezW3zFoIqcOM3dISVkcz6ZYhiKtXLtsejRWjwko6lIj5DygzL2DDhg22CIavBjHHhfkbbPfAHpTsp8XeW0y8hVje0h4UE5v/4A/+wHoMkMR39f73tVdRe7vW9qZbqM/efdcqI7/r33zTJirzVR+shvR+mFPBthJstnv+wnlTFgrFoiWL5bd++7eN/FFwXvnJq/LmunWmSP/qF3/RNoM3ZUsVPOspheyw95WnhZ90N9TWC5ve/sPX/95WsK1evVp+6Zd/Of1VFQgmQwkva0/tnbff1rTd1B7VLHn8ice1Yptkm7Tzicj3NF3MjyFPC7Ryuv+hR+RXf/03lFDX2bzTH3z/B2b1/cJLX7DeIAU6zpX88Suv2NcKeP7f/rt/a5O8wxuk4DdZ7/pPtNdJWp997jl54YXPmCVl7+7d8tZP19kn8BiyWHU3G8wvtPmpfH+eT2SSn+VaISxduUJ+9hd/QfI0bswtJXwQOx89RXy2vfIcUqGiZYe8QU1Ga/ymjK2UUj2Pz/cliAtV7Ilz56WmUQm+dpCscVaxzlcqshzai/dAI8SuO+iJFS2J/sj5Own9XTJ6nv8Ws/i4/shVdkLsW6weRvcQFmaUakd/ojbclSXaQVcX9m0lHHczOjwSwmgbPjXLDSWAZy9ftrZPtV472zTgjKIFl9QDST+6C5pcmwqmJywKMevjqEq9JmH4VtsyDAKsKsc4YFOZ1C3kzazj+pv6HYumDSfrdfywbeaUdPIMJBa/8S9aCHmOT6xeOM9XZ8qNHFEvY4Qhj5nziVGGuDEUjN8YM/CL0TOG34O7qzbqxW9AO0dYYe2CvivLn1Bn8hzGAe5hRCnVtgk/+bgHn4im7WbxEfHBCsocUMLHwMGwMc8bgdI8o/3kwyH8ZmQTY00k7bhnpxjSSxzZzgiDCRZV0kVaGckkHGLHVDqMRjY6qUQSv/GLBTj2vP4GZWXlZhC6rMSc+GC55Z4Nd2t+XtY0NNQ1yoQJE608EC/aTbYiJCzcALb3s/mp+hzhM++T6RCMpPIRF+LJu+DdM12NNJJnzOOkjWYYHcFIRrpYPc8c3hdefMH2O6U80N7y6dnAF+J76B3QQVtUqIS3pDBfpk4cL2X6k5mgaAdaoSUrhKN/hseXgVKIoURhVfKJ48wD0YKmcWIYmk+qMeTKam/MzbwoNidmovQiJXkPPPyQ9aYwfzNvgS12KDTVSiCZfE3h4lvgn2zZIh9++KEcPHjIMnTS5ClG+CgskJwFCxZaIYrzKjCB376tynrxkhKhCzJmdJUsW7rM5sbg99WrzKU4KxuVXH7y8SdyhP3GVAnnam+K77AzNEIv8J7VaywcCsY+7Y0eOnTYarBl2hOdpT0keoW2z6blfzI3KELhnVBQSTekjOELPqt19z13Ww/JKhd9nqFsNl0nn6pvVcvFSxeNVG/cuEF27txue3QxB2ThooXW62O4prxylKxctUrjk2/zZhi2pqe9SEkkw+5xiB9lYBgDN/RY+drQJCWkqv9W8JCLGq9NmzfZvFS+WsHXilBchskh2VhjGe5g3syOrVtl/+59tp0HnwDlm+vTZkyXn/nyl6Vi9CjrqZIvVqgVMUd6is6ejf6T/1RCvAa+H1zJ4iw97+z5noLGji8DNfD+TPdCTPg3FNH9WIX09FwcHSNXnvWl9BIJLzryOehcqv7TeowvA7G9S7E2eKGhSz2T/WA3kB0udUvHXwZS970IL4I0sVcjpILhUSxSDM/maV1Nu1CqbVl66FvdQySIB8O01MvEC6sZTQTDuWOVIEEyC82KqURM2z3qWIaIaWP4fC9pY9u7sePHG5Es1uealY1Yx13TBwkqLCqxMKjDjeBpWBBFdjphCDi6M8JE3aT3IWC2SFaP/MbiRzoYpiYMwsYt1lDSQr4S94LiIhk/caIZcggHa2uIi/63vNc4pBoTRhNL1S87smBW3WIggtgZIdf84kt+hI9786+8zFbyM4TNNC3aRay7xBPrMJZZc69xnaBusDoWptpk8rFIw+O+5oK13eUVlcK8SssTTS9tHryAqQFMe4C0WlnhHfI9dT1C9vCvRH9b/qs73jXumB7B+yHe+EO6cIO/xBHyzi4s7GSDe+a58vzpM6ftk5o899DDD+u753PWtBXkG6OdnKhH7aLDm60QdBBNw7h1p3yCMoVWoWj4l69cMRLJcAGfnBqjGcvwwfhJE2WsFlSGECYoQZw7f4E88Mij9mkuNieGMKGI9Eh5oSgflkyGbNlGiAKAUo1TQgPBZD7M0888a8O6uB2lyoh77oPKUWNs2BeCV1JSpoVTexn6HMR22vQZWjDL0gW3TJWnShV6ssbjfvX3saee1oJfbl/fIJ4UZMzxFKyr129YQWJi8JKlS2UiE441B7hmOWH5Hyo9u2IFKaw2p2cJmWP7A8zoxINCTb7hVkuKzX9kKAZLJ/ErrdTe7LgqqZowXiZNmyqPP/203LVmtbQoGWS4Zdqs2ZrXE2U0XwvR3iy9NoYHFi9ZYmmmMqDCID7Wm9beG5ZciCgmfyq6EMdQiTFpmc1tGQZing7JojKdrMSYCeXFmtdMusZTNgRmA1vm2YQFQs9IORZlDYvKjHwgYZQPpDfo7PkYhlV6pnBONLuL7BhbmXSMbCQKRSjZbcsJ6BLRzPVgN5AdNuWzv4mmpUf9CtZEUmEXjUQQPpOnOELsIDXU2dQJ1H9sgQfppN0ibswhNPJJe2DKpbHUdo348hs3/MZ/3Jl/6XTxGNbRcD34E9xzH9EfaaKYuZ95husxjOieOgsLMKQuxoGoGY/Q5yFuDLFDtPhNG6E1nR35TftglkBLkPlKCHpN/+KfuqPdww1rJFijgRtbuMMT6gFDzhhqiAOGHqbNYXDiXULkLI8sX9QPJdcERfBIOh16jKEjthCWMwuH+4Fw8g6trSbuep+jmb2Jl97jlj0X3Vm2Bat0BOSTePEMJJ6pcDHOp06fMkMUc0whmFu3brUwn9a2kcW9sQzhp5Ut/Ul4Fpc2IC3JY8foLtEcdkPnyRiR4VgzMSVjoSMy7F0ZCQCWO36biZe7pEnBfRti19LDy6BQYBEtVjLF91jZbZ9FPXHBjpnqtTAx1I7JHksgfrH4iK0J2LEfEzjEFRM3wFxvC3cUVBxY55jDCZioS08LhbC5hRp3/KHnwxAIPbIYV57F9I2y2ap0vU8h426omMIrsBfPNRW2d2BOaLkSZ8zpkCDiyOpB/CMNKJk+ZOlnuIB5qbdqqrVA5ElV1RiLIwWc3hTTBUIesgVFYXheSTXTBqjkrBepYeCeSpAwmM5gCqzCPYYHTBE1zgj5HdPIUA/7jNH14p3aCnpIu1awDbdr5fyp07ZKHrJp82E17wqVNDdr+Yxpt2OqcARfe45YxtrzJ+ZzLEeoiQ+ddx8xn4ZLfB39BysDiQIRSnbrMhLKSedD59RUoebtHjJhtA4b0Lr099B5DDUZLrCWDb9T9RzpjohnYW9iPWp9bm74oScZl11B+7E3bqe+xTobJOMBAl3o+HkjVCmkn7cVlfpe+Zs6j0iHYJe1veIZ9SwZMu0HIG60TbRpRkjJL4R78X7qd1tEHzM+WxnL4kARFs0EkvkCsp9KxyFxA36SAW0xSQvX8D+5WJB00dYQp7BvtBLNU6dk86bN9llKXLFjC1MC2f4II1KjdUiCVRRfObc44GFO4Et2zHOju0Pnw5poAogJxIahYsCQA5kQFs8EosZv7kMMw1wIkqe9n5QbwAuEcNr3zJXcUHD4DQGsqKi03/SW4uchIZsUcIbSIboUEPxgTiJDHsyN4Tc9S/wkO/EDqyeEEvCbYQziF4iyZrmWNguHBxQoTFzEY3t4KkxZVPAf4NJ+2y891wrQ9s/U+3yakcKKn7gjD0CMLwQwFHiUlAU+gTzy7rFAcgfCCEm0BU+aPpSAPcCYFoDV0eYEaf6iRKTXlMLSRB6GYQzSzW/Sx31A2AQQ9jijciiwsLAyB7KpPUx1V6FhNGuHgh0EiE+9xlMDskoV4Aa/LRX6w47c6CHS+Zg6ZoP7Jpp/TjR7jphPwyW+jv6DlYFEgQglu3UZCeXkzieauWD+ah0HYr3f1m3IEy7jMqdP5gV3su/qjWz2lEBoI1oTxUiCIjRb2oVN19LnYxpAJh0xbNyEY0R0wRE/IEs4zfbH2jG9hnUxGJeMtbVyF9vNDLLzICs96m/bvM5ykzraPMjUj9ZhZMDV5J3W7miPw7VIRsnfGL79Veccrf1UoZ2GH8BtzBClbrnG0Hr6ufhS9CfXsuPQGjwTnusM6GB3iGZP9HFIgIIEESQjeTkQPyTcKzCCCQGE+CDM17C5j5rZFDjTG/XDMkKPRq70HivBMP8zhF2v5IhFMLX6IsMcjHDdaJKGy8oznr2t9+uUFHFkGLxBXzr7Yuapn2zZUKv3sEEyh5FCxBwNnof0UkDNqqkvhniFYQIsf8GcTvysQOGX/tab6eIQigS/w7Xwiz+hwBlhUx+IO+kg7hYHc6LPaDxII2ESf1bOMTenvoF9zBhOUN/ylCiXlKl/TERuSZnvC+S6kkwjOuSnHlhRRzpQAkuTEcxis1xCMnnG8jgF0kJnAAuoWWj5rceYZrbNYIEPn8Sr0+vKcqVZC3RzgfbQ9BpzeajZ7V2SJssbPA4Hh8PhuFNAvd5VoQ60Y3fR27qzk+ez45SMM+QRgZAw7BqO4V78SIIRrxxhUPdDMG0IO3UehsdpizIS2wlrK/CH9pvGPC3cTwnu8Y9r9mxYwBSOGaGtQ0Da7w7QKs0pyaD1L3yyNlbFztVv2suwZ3ewzsI/aMdpezkSF1a+G7fRR9OS8m+wMGyJJi+9UTOdia+cR4TCEMSG1tmyR48QLAgXEskMJIkXwjHe53o8j4QQoUxyjUnY/OY8kDYt/ChASggPgViZ2Z3/eiQuvGriy5yioiK9TwHQQs2eZNzj+STY0BUPKPCIDY9oRDgqDUsdLQhVwvCbUsU5z1qPg+saF+LKHJhQALXgIpoPNtSi5xDJfJNAUO1ZwtbnGMY2pbN8CwS1EGKpEoms5SkF3+IS/OYnE9Lxx3q0ZKLFVuOo/kOmOTLvlXhaXPU3rvDP8oM81PzCHguBb9TrDZpn9tnOVHj8TQtZwEXHkEd8Zw6Ho3PENiaDdK1nv+I9/kbi1kbsPrU4rUFSuJblNiEB2n6po+CW+p02ISmhjo+SfD5YP8PzUZJxTyNW4Km2LEwJaCu0HUkJTUG4ztHIpl4LMc3hR3SfupK6kDqq2MV4LULT1eG/9NM5JRvxfZoYD9H/eoxpsgupPI35wHUMYZBJ0oWhKKbRwtD7kOp4ngw/ymCA1Dh6iEh0+gOtK5RQyHJKShlNEv+SbmJBpACGwpe6btGPbmJBbP1s8l/6esqPzDMMHUDu9arGO5DUiPAMYUVJK1FSUDAllIjplAJ/7ai/rcPJ0ZzrHxUjt4nf8TmHw+EYWaC2jDVmbxD9aS3U9lbVdijRXZDWfuRy357kaHN6KcTISFpKIHRxaDpD9tRl+reeKxvOxD5z7EuYnxpWCCfEM8Y5KbGtpm1lhDb7/lDHiCGa7b0SClVbhNfedQkwn3LdVskVylCH9WSjKPorDclgkmQxnsYedXQTkXzGJPXb4XA4HN1FukbNISAecyHpJre0R6MGC63ajMR5ezIYsHBTcRvOSG9v9O9+/yuZ7Y0sZ/VPVingMtsb3ayrM2M7/2IO2CPhdEhjuMSzJ+iNhTVj/M+N6HWHIbS6mVV4OoE9mipOSW/MF73QUdqyn+krdMVP5u8Qx5KiQhlVWmyTofsDxIXJF9erq1PbG4XedhqpHxy6Em+HYyghV1lOIlOutey3ZKZKcc5HKspLS6WYed16DX3sXu0TkAmjLbjO9kZ8dSY94MzYbCIkzjryo//Rk1RHdBzzZP2buy7u+Hm7p/9bGS/CJZPk2WCgs/avvxBD7CjktJsOo4deDFz80Tsz0mmYbC9VWV6W3tovaAcxSrVR+seHzh3dQChGXRMQjyMHIy/FDofD4XC0DyeaOQEP746MJCTJZGcysjDyUuxwOBwOR8dwotkGkThmk8n2BMSjw+FwOBwOhyNi2BHNbJrXXXEE5M6X9m1ycT5utFVmS7zXU8Rnk372xj+Hw+EYfkjWyt2RiFz3uiJdRa5nka6BL/8kpbvP9xbJEJMSkGxxBqb1sVA0ArG9a0+im47RuYv20ZtnO0dewdyftS8DHd773SH/ZaCBQ3dT0r8vaeDQWbrbf8d9lQPZ/g9GzsYwO0qrSV5eqy8DTRtbKSX9FGHicqd9GcjhiLAyG3VHf4SS3epSqlzfyV8G6g2yY+zoPrLf3MDlZ2dlpmsx6W3J63p60UHbxrDlDv8yUP8iKm1X5U5BrrQlJfdVpK/QX/46HA7HnQua9N4SjZGOwWt9skPOlq4h15Pdkf6DE02Hw+FwOBwOR7/AiabD4XA4HA6Ho1+Q3rD93/5e1zdsr86xYbvDcaegs0EE7keJG7YXD+CG7Y2pDdsj7Mz10HGnINX0RGkNNqYOOhfK/cBv2B6/cWOf3LV/AT0J0+EYjkhu2F7oG7Y7HA6Hw+FwOAYLTjQdDofD4XA4HP0CJ5oOh8PhcDgcjn6BE02Hw+FwOBwOR7+gIL9qxR/m5zXJV3/vy71aDGSPhFOH445AV8pzXAxUUlQolQOwGOhGdbU0pBYDtYpf6geHrsTb4RhKyFWWk8iU67AYKILzgVwMVF1TY22fiephMiTOOvLD4bhTkFwMVOCLgRwOh8PhcDgcgwUnmg6Hw+FwOByOHoEB8I7gRNPhcDgcDofD0S3Y8HkXUJBftfwP8/Mb5d/+7pelVJ/p0hzN+npp1nvJQOyRcOpw3BHotDxr8Q9zVURKCgdmjiYbtjeobraZo5mC66FjOMLKbFZ7k0SmXKfmaOI2pX8DOkfz9m1r+2yOprWRmZA47cgPh+NOQXqOppb2Aj1UlJdJkSofV3yOpsPhcDgcDodjwDBoRDP2/DoTh8PhcDiGNKyxwo4zWBjMsB0jHZ1xtfzBKJ5pneyCONl0OBwOx2CjpZ0VDwwh5udHm02ORmxABMSjw9G/yMvL2CiZQtKebkQMuEUzkkyOXZHo1uFwOByOoYtI+gZLHI7BQWcczedoOhwOh8PhcDj6BbbqPC+/Sb7anVXnvfgyUHTTFbdpdMuxw9E36FJ5jl8GGqhV57dupb4MlN86fqkfHLoSb4djKCFXWU4iU67jMF1onDgvLMiTitJSKernVee1DQ1h1bn6bmJfBspvFVZHfjgcdwriUDkLz/PyWqSyrEyKVfmC7vmqc4fD4XA4HA7HAMGJpsPhcDgcDoejR7C9bTuAE02Hw+FwOBwOR7/AiabD4XA4HA6Ho1+QXgz0+//mS11aDHS7rl5uphcDKU+12Z5M+sz9SbxsRDddcZtGtxw7HH2DzoudlvlmSr5ISdFAfYIyLgYKOhenX8fIcnB1cQw3tCqzOQpwslxnhulUBxKfoBycxUCpRbEmHfvhcNwpCGuBMiW/ooxPwMbfvhjI4XA4HA6HwzFAcKLpcDgcDofD4egXONF0OBwOx6DCBp5T48527nA47hgU5Fct+8O8/MYuz9Fkw3bmaDL+biPyXOxHxArIKx/HQKGzssb9tJvmMF9soOdo2k65KdhZP+uhw9HfaKVXiuzfAWzYHuaA2T09LxigOZp1iTmatH/M0QytoLWEDsfIgfLDqGdog8/RdDgcDsewREfkz+FwDA840XQ4HA7HkIaTTYdj+MKJpsPhcDiGLJIkM5eF04at9aIdHQ7HkIMTTYfD4XAMOWSTSjtXNhmv23wwPUGSczOdcDocQwvpDdt/73e6vhioOr1huzrgYupe6rRDRDddcZtGtxw7HH2Dzood6tHSHCY+Fw/gYqDG9IbtCaR+cOgs3g7HcIOVaWtu9F9K5xAW47FhewWLgQozi4F6go50h+txMZAtBEJabdge0JEfDsedgpawY7uV/Hwt8eW+GMjhcDgcQx2hkXI4HHcanGg6HA6Hw+FwOPoFTjQdjjsAeXl5Jg7HnQYbftOiPXSGpInJ0ImNwzHUkZij+TNdm6NZ73M0HSMDXSrPgzBHkw3b28zRTIG5M64ujuGMZPm181Yks+0czaK4YfsAzNG8dbtG2z4NVgPRoBWpdjCFjvxwOO4UJOdo0hL5HE2Hw+FwOBwOx6DAiabD4XA4HA6Ho1/gRNPhcDgcDofD0S/I79sFBHGGShSHY2TAF+I4HA6HY6QjV0vYxqJp9LBHbWYklkmS6WTT4XA4HA6H404GtLE96lhQMHbZH+ZJk/xuatV5etVsu18Gauhg1XmSWMbz5LX2rnaCbjl2OPoGXSl2cQVsCavOS4r6fdX5DV917rjDkSy/8Txzre2qc74MNFCrzvkykK0611B81bljpCJ71XnF4K46jyrnqudw9A3QpSgOh2Pw4HrocHQF3SKaaZVKdBmzVS38zv6Xudb6bnTfWpKIv7PduLgMnmTKbxLx/uAgxigZQ0dnyOTYwEp/IVdYfSFDHf0dx9z+Z3Io+S/zO+nCpT/FMbSRnzT7dwUt7Ti3F252Uz1LH9tKi0qzusZtV5XRNsftRMw/lRC2y50i8b3meuf9JU0pyXWP8mvx0ehZ+dRzK9uA++FsgIFGhRjZP9M1/Ulc7b4jFyxvLJ8GXvrjvfRneoZKOUpGizhF6U9E/60uSp0HhNBDi9Zsf0P9oNctkvHo0t/S+r04BhrNqaH09pBXPPcrLXl5tbJ72z9JVb5ICRe506wPKg1NgjliV6qr5cL1G9KgrgrzC9VZcEMwJi2mbupJCDgvLxpNM37Z1Jb079ZhROS+2gmIs+OOQ6qIDQhiOQYEmx10i5axOAcln7Kd+l1RUizTJ42TYr3eH/M0iVODyqnz5+VWfYO05BdIUypjmAmDnhETENIQ78WjI4n4juNxoMH76PN30s683STi3KokurJjQoxv8vm+3mkh+pYdw/g7hK2ibUw67OZmKS0skLGjR8no8lIp0svoX09ilh1u9CPMyRS5euuWXL5+LcyRzs+XJpukmW+6h2t0LuMHT3ccC9KQ6320h2zfsp/P9T664/9wQ3Z6c+X2nZz+iJgPA5lWeF6+tkFQxAKVqlEVMq6i1HQvX7UFjWjRvxYj/WNEs0CJ5s6t/yhjkkQzARxHuYay3ayW+ib1LC94a54BfRCSaUQzdTVDNAE+q3KYu2zEUFNNY1sHnUMT77jzkClNA4NIbFN9pVZIleB0aaWh43eJNnYzJ08wRYtEM+2mD0DJbtRwzl68KLUNjabKTRY/QuFE9QaymQo0EUNDX8blTkDXylPvci1XxR8bBfurt1uH0LVY5UJ475TE/kFfx7db0EBjSDEe8Qq/C7W1g2hWlpX0imjmAqEg6N/1mhq5Xq1tX2ODNEPyLBT0LoRo7tLvXH/H0wRykcH2kF1+zEDa0fO5KqwRAsuVkZv8AQdlk7KYr8JivNGjyqWqvMQIZoG+iEA0g05Ay4xoFirR3LXlH2WU6gsWmSQ1BOY4nNqquxu3a6VeGzvt1klBAd4GcDRF0AIfyGbqdzhLScav1sjc7zFyabZj2KOrb7U7lXh7yA4rl4+hiVGX+p/79O6KCvJl4tjRpjuxJPc+NhkQL4jmles3pA7dU1LZjEUl3ZELIVIBWBoSgWenyRHQ4fvJzkRD2yfyskZ9AFbvroAnM2VW31svO8rZnYuhHt/ugkYtg0yc0cXRFRVSVlosheok6mBfgFCQRk1rbV29tX+NTU2Sl273QmiWFRpoJJohT4M+JtHa8MLvtjFNPpM8x2W2+0yetGjYA/s+HEMLrfWjf5Esl0WFhVJRVialpQVSqL8xPVLKWxHNEoimKNHc+o9SoUSzSG/gOImkqjQ0NkpDfZM0qSKRsKg45mHqrI3ypHpaoSIMgedGuN9TDFw2OwYS7ZeXgYeVeT2GIh5iRnmn/Q6KHvQCKPfsMxASk1LqtLFrZDTBpq2EhiXTeIVwk4h5N5TycCgg5lTbHAtoSwD43Z7rvgBvqC/f0nCLb26kU5AMStuT+H7CMehcRjf1qO7z49BCLxGDbmhsknqmragmFhRoK2lhJyUgm1wazOQc3OQi+0l0lfi3Bs/05DnHnYCOS1Q/Qcs5JS4/P1/y2VpM9Y145CSapfN+tqWw5bYRzbIU0USSSBZfdMgSlaNMx2G7XMjh3OEYlshVzLlm7U4/IlhLuh+I615uZOdkf7+/9tDfwfb1+x+kbBp0wP9sFEFR2Je9SIdjmAOtCJrRLtH8ihLNWtm55R+kXIkm1sxcRLOjyipWPF11118YqRWgY+AwqGUMoomCZTMiL/gOR78h2abF81ztXK5rDsdAgCYA+2JH48X9A0YRUoTSfiGsIOBaDotmV4gmkguxnYv3c7mLbvqrTexv/x2O9kB5H5By1x7RdPQdPGsdWUi2Z/E8eS2CopN9PVdxyvVsEp0Vwc6ed4w8hLIXbYoDjUAqY7ll1XkgnwmiWTbv51oKjGh+o9cWzYhWyU09mAg29TuFVh53pmIOxxBELLZxriRluh+Kcpz71Xb+YCslyo2cTlzfug3PshGPqEqZuZjxqM1rK91MNr8BqaV6aWQvqGqzWKiT57OR7d4xUkC56Lhs9BcCswvsDnSZaCKt1aVnSYh6iF+RZGaio+iJpw7HUIMpi/4ZskSzHyLjcIxwdE40c6G1rnZMNHP51ZmuO0YmBrNcBFIZS/KAE82I1urS2rfuNYG47t4TDkd/g9I8pEtlagFDW6RirYfe6LfDMRDonMQNLnKuNnfNcgwQBks9AsnMBJ6TaJYq0SxsqZHtn3zDtjeCZOpBHWdgjsNph+h6OnuqfB2H4CrtGCxQMgdJzx0OxxBAbqLpcAwMBpdoZtAu0SxQormjl0QzpnGwGtsYP1d1x2BguBJN15f2MRzfp2PwMNhEc6hbfB13Jij1rYkmFJPrGaKZ5pN9oSKDWcxdxRyO7sFJpsPRd4DoDaY4HEMVScOlw+EYQaBpcmlfHI7uAIvmYIrDMVSRHjrf9vHfS2U3h86zK+PBrpw7UzVXRUd/wcmJw+EYTDjXHNkYLKN25Iex+OUaOk8Tze1ZRDMZ56QnEfF+u2nLfgAMUkaAGJ1c0XI4eguK9iAW757BlcEx1DHslMrhGKronwo/EsqIsAa99WKgYLhUZe4sCrEhjQLicTggGWcXl76WYYlhG3GHw+FwDBcY0WzNRtsi2aAmpUN0+4H+R64oubj0lQxL5EqIi8tQEYfD0UfIpWB9Idloe63NYqD2HnU4HA6Hw+FwOLoDm6OZLzWy/aO/l1EFIsV60ZeiOxwOh8MxfOArz0c2BmuLK5uHGU4NYXYm1xKLgUrm/awSzduyffPXZbQSzSK90Wo83c2bDofD4XAMaTjRHNkYFkRzmxLNMSmLpkXXiabD4XA4HA6Hox10hWi2WQzkcDgcDofD4XD0BYbYdMzIjftKHI6BRK4y2HfS+b9cT7n0VByO7iFXKRpIcTiGJobgup9cCtQbcTgGArnK3mCIoy/gM4Yc3UVzS/OgSYuK679joNHVEmdzNPNYdb4pLAbq8hzN7BD6pGbG075WFm8yHAOBvi63PUWu8u464Bh5GGiNbGlpSp0NDvJtMYjr+khFmBk58GhLBduZo9kn6DOtjsrSV+JwDARylb3BEIfDEZsja+gGSPLzCgZVXP9HMsK7z1Uu+1u6gm5bNJubWyQ/3wu0w3GnoKuVxZ2GoVaLdfc9DHb8h3q5Gcj4xXcxmO/EdzcaueDVY9AeCkUgl0WzW0STfboaG5ulqKggeBAutzofTAymkjsGBx2Vu6FQJgcKpDUod2t0RSdGUj4l4fXFnYvBKNOUp8EsUyNVjx2D++5juc9IDqJZPPcrYR/NTX9n+2iyYTuO0jG3HwHNzc3S0NAoN6tv6698ac7Pk2a936xUuklJqP3T3y3qrr+Q59ZURztoaR7ZVW1u3ciTPJTSMayRazPuwdqguSsYjM3DY35YyN3Imr6KaapZDT8GAR3Vf3faZu7dKft34kb2bdKvvwO9617e9AZ5efkUOmlpapJJ48akvyrZM6IZoRebGpukrqFBqm/dlgZ9uEEdNalANKGWkE1t1cw5njscjsGHUs3UmcNx56IV0ewq9JFuuc9ChsSkmvmEZ2El+MDBGn7HyIUZGvq/ro9FvMDKW7PkNzfLxLFjZVRRgYSZwr0kmijVzeoauaFEs76pWerVhya93hw9TEn/J9XhcHQJLdDMnmukVQPovv0aurAUaiT7tO7pzDIQAx0Q9MFLGND4DgJSFj3726cFoSvQZtXKS+78TdlfHI5+QYuWvdDp6f+CH4tyQUF+sGCq3k0aP1bGlBT2DdEE127clGtKNusam6S5sMCIZosGRxrDI4NnP+msXXDceUgbFHJgpA2ldzytpOfK0de52B9vhdQNmPpbAvRPXwfYoX/xZgeO2lOGQYnvIKCd5A8c2s///s8qbdZV/wc9CxyDBIhm6rSfEYNhUXhhvpLN5maZMn68VCh57JOhcwDRvHrrltxOEc3mlNdpBym0/jVQGJxQHY6hiqDGQ0svLE59HSX1dCBTGcLKriTbonMX3UHPU9jV+HYVfZuu4YOBLGPdQ/gymGMkox9Lp7JYyleeEktjtHnBoFioxyKVKeOrpFT5Y6dEc+uHf9slonn1phLNaiWaTU3SUqBE0yaERrIZkDkbDAxu6A7HUMNQa4AsPn2tptR9qdP+RnfDGez87698GWrlqr8xtFsWJ5oO0PelFB8Zlqd85SvR5NymieiRofJivTZlQsdEk+vdgj0YTjPnWWmL111cXAZfhhqsusgV0V5IVhXUr8gRfIcy2MgVp76QkYZceTCUxOEYCLQpa3qhs/q3W0QzGYAV7tSkSC/kDoejO6Dm6EtxOBwOxwBAK9zu7pjXbYtmKxjD9Gre4XA4HA6Hw9EWqTmaNbLlg87naHIpOUezuaAoWDV7uYVK38FJr8PhcDhGInxscWSjf/gPvrL+h9LFSvNmPWOOJtsaFeq1orx8mTqxSkr6co6mw+FwOBwOh8MRYaQynLaBE02Hw+FwOIY9fERv5GJov3snmg6Hw+Fw3BGAcLiMPBnacKLpcDgcDofD4egX5BXP/bJt2P7Jxr/p0mKgazdvypUhvRhoKMTD4XA4HA6HY3gDRpXZsJ3FQHpNL+bpSVgMlGeLgYpTi4Hy1aU9o39t3qYvBnI4HA6Hw+FwdAnKHvMShsiumPacaDocDofD4XA4OkUklpDNrpBM4ETT4XA4HA6Hw9EldJVgRjjRdDgcDofD4XC0gc2zDKc9Rh8Rze7y2/7AUIiDw+FwOBwOx8hBZ0Q0ver84w1f696q8+Zmac4vlJY85ar+CUqHw+HoJXpbf/XW7tBdDLX6dqDTP9Tg7d/IRu/Kfyw9uXxh1TnIz8+383yWnfMJSj0U6vmUxCco+3HVOd4OBXE4HA6Hw+Fw9CfSjEtPOqO4PkfT4XA4HA6Hw9EpemLWc6LpcDgcDofD4eg1clk3CwrGLvvDPGmQ3/r1z0mJ0tQ088zhmku1dXVyu75eGhizZ36m2U2HxgxNh8PhGN7oaU1K7Zyriu9vDJWaf7DSP9TgLfHIRH+X/+g3fC/MweQax4K8PKkoL7H5moCZmZzGGEEVlSn2Z+QcDofD4XA4HHcyOmKSPnTucDgcDofD4egXONF0OBwOh8PhcPQLnGg6HA6Hw+FwOPoIqQmbKRQUjF36h3nSKL/1a11fDFTbUC+NzS3Swuad5mFrTx0Oh8MxkKB2Hsnz7Ud6+h0jG70v/5HF5fIlbtgeln2nXLAoyDhjWAxUoOfcbWcxkMPhcDgcDofD0fdwoulwOBxDBmk7QDdlsJArLoMhjoBceeNy58tgIXxmsjM40XQ4HI4hgezGo7sy0MgVh8GUkY5ceeIycmTgEUJVsslYeQcoKKgKczR/sxtzNG/XN0hDc7N6nq+iD3UWisPhcDgcDodjSCMXZU3P0TS+F+Zm2uTLFCpLS6RQySOXfY6mw+FwOBwOh2PAkOagIMNPHQ6Hw+FwOBx3NqLtsTMES2VHaM+XNhZNJ5sOh8PhcDgcdzqSjC+b/WWMkBmSiZuWNi5BvNasZ0H0XC8iKaLpFk2Hw+FwOByOkQWYX1fYX3AXXSafaG+ZTnRTUFC1LLUY6LNSrI4L9KI9kyNcLrVZDIRrXwzkcDgcDofDMYwAq2tN9tpSPyWXtvCHRUB5trjHNm7XEyyVsL+4YTtILgbijy8GcjgcDofD4XD0G5xoOhwOh8PhcDi6jRY+R27my/bhRNPhcDgcDofD0S9woulwOBwOh8Ph6AMwQzM1YTMFJ5oOh8PhcDgcju6jNafMidQnKJvkN371xR6sOi9IrSzqQkgOh8PhcDgcjiECGFxrsteW+uVedc5GRzA/rJUV5aW5P0Gpf3zVucPhcDgcDoejy4BERsTz5LVsONF0OBwOh8PhcPQA2DA7hhNNh8PhcDgcDke30RnJBOk5mr/Z1Tma9fVyWyV+GSjcsCccDofD4XA4HMMCsLrWZK8t9dMrWXM0jSXqSWR+fBnI52g6HA6Hw+FwOAYcSjQDU3U4HA6Hw+FwOPoS+ZLnJNPhcDgcDofD0ffwoXOHw+EYKIRJTG3FkOtGd8ThcDiGHgoKxi4JG7b/SvcXA0levl5T13rDqzmHw+FoHx2OHfVJHdphCA6Hw5EF6oxQb7Rf/4QN2+0+f6yu0hNfDORwOBxDC1YpW+3bVkKFneNGt8ThcDiGHpxoOhwOxwAh9PjbisPhcNypcKLpcDgcDofD4egXFBSMXWxzNH+9m3M0G1vChu1xjqbD4bhTkBmKjWe5Jdri+OXoPdrmcPfE4XA4uoO2dUfbmiTM0QR50D07DSexBfA5mg6HoxsIFQrQ6iXWHCnEHwhVR6xSwrmjE6Qq5twS73EMEt5FRqjks69lBMSjw+FwDB040XQ4RjxyEBZ6p/l50tTUnLoW0ZoeBYnInDc0Nkpjc1Pq1/BGpjefTGv3kG+mAJFGzRfO2b44T/3NT4u6iUd1n52znSPEMXN0OByOoQEnmg7HiEaSoARpSTEciFVBgZIiPUK2IEjFxcVSWFSkBLRJGhsapLm52e4VFBRKfn6Bkstmcw9JzUYkT0NWSG9hgZSUlIQ0pNLd1JJKU65nVCLiM9kSSGawWNp5c7BYcl6Qn2/nms0q6h43+rdJCWmD5m/I8xKLVzKM3HCy6XA4hh7yiud+viW/pV42rP9zqdS6rFgvGvvMrqu0buPStepquVp9S2qbm6Q5v8jmadIyhSrS4XAMLwRyGRFJJgQLQlNWWip1tfVGIvNV12tqauTypcty9dp1qb19WyorK2Xc+HEyevQoKS0tU3f5SkIb7FlIaJMKZCpiqNYS5EB+QYGRu5s3bki11nNFSqjHT5hg8S/Qe6QnO/7J3GuPAHIV4lig+Yc/9fX1Uqpktrm5UZo0vBs3bkptba0er0tdXZ35V6L5PmXqFCOZGgGpqKhUPxrMPwtT308MF6TjkPjbFu1ddzgcIxNWm9hZ4HBh0k5ArC+Yoxnqvvx8rdfVQZ7xPu0s21N5MnnCGCktDGt8WLkTus/mRPRRJZpzlGhKvby/7s+6TDSvKdG8rZVuc36hE02HY1gjo+iBZIbfDOsCyGVzU6OcOX1WNn+4SS5evCg3b95U7lMgtUqKIEyl5WVmBawaO04eefhhmTlrphImyGmeDaFDwPBnqFcREMkL5y/Ix598IidPnDCiR3rGjR9vllysuNkgl2IOdkQ0WxIklSOWyssXLsiRw4dlz969cu3qFQv/9u0a86ewsFDKlVxOnTZVli1fIfPmz7dnIfJ5SlbVUTrs2DmI6PirwrnjCOKd7Mfbu+5wOIY7Mlod9Lw9opkahckimgXmpnOiWVBQFb4MZKvO9VkcRu9bIXWRVee19Q3SqD4YybTK1YmmwzGckSQrkEx+FhcWSL2Sya1KvN59+y05euSIVN+slts1t6RGO5z1tXVy69YtI0mXLl6Sq5cvy+nTp41UTZk82UiRsifqKas/8DOcpn70OfA9c0iGQYWX5IFYaOmlp6H3IX/Xr16TnTu2y7Fjx6SosEiWLVkioyorzQOLv3kUHkkG0B7JBNyxIXN9VnPEiOC502fk9ddfl927d8tFJZw3rt+Q69evmUW1QevYOs3bG9evy9kzZ9L3Z0ydSkBSqPlqhBMvYwAJtB+TCG0ESAdnqXgTPyRcbY2Mf+Fudl62jy45cjgcQwBBW5M621p/+UV9QS2QrjfCVaksL7VV51a/mSSe1QecaDocIxzp4XKVoM2hHmhWPf9k88fynpLMy5evGDkpVII2b/48WbBgoSxZsljGjh0r5WVlSkjr5TbE8+ZNI0bTp02TsVVV6ksAxIhKCUtoQYH2ebVb3KxErzlBeBhmv337tpQowcsmPZEYAdzynMVXz224Xp/Fv8KCAv2tKdDz0AvPADdIkYaflyJrwdLIXFSqyBYjd0eOHFbifFHGjBolCxculNF6BEacCTX8D/5r+LHSjbD8VIlXObLIhyH45sYm2fLRR/LGa6/KKSXl169ds/uTJk/SPF0gd929SubOnSvjx43TOFzQSLdo3tbKubNnlHzWyowZM6SiosLCz0Y6eok8Bcm8y8QqoKi4SAoKC21Orb2f1BSBpuYm/ZWnZLvQ3OFH8CW8tyZNR1NTo10j75hDSnjJsAjJ2wWHY3ggaGpSX1vrrukzOp46AieaDoejU1BptLJmqlhloTcuX7ooe3bslBPHT+i1fFm+fIW8+LnPyf0PPCCzlQxNmjxZFi1aJIuXLLUh3pMnT0r1rVs2pH675rYsWbxEiouYjNMiJSxoMRLTZOQOcsOcSKqP5tTKdsK2+ZAQRWKWiJdB6xwIKM/aIpoUcSS+hUp0IEUQzZYmJUGNjTyg/nOXR9WdPlPEohr1mjgwJSCQIz1X4sTwPvMkjx09KteuXJGq0WOMTI8aVRn80fCJL0SWtEDOiGIgy+GeRToVbw4ZabE4ndY82vD+e3LixEmzXpJvL33xJXn44Ydl9drVMmXqVJk+c4asXLlCw15i+Xn16lW5VXNLifxtI6BjqsZaHCDLhAkFDGHwV4mg/gtXwz+LW+oIwt8As16rXy0aN94D+ZKv+VisBJS8hkyGfMd1CIe8J78Z3ueakUv9H8KI6e0IHd91OBwDj6CVSd1sraf86inRDLWww+EY0aDySArVxI1r1+XM2TNSe7tWKitHyb333SfTps2wDmZJeYWMHjtOikrLZcz48bJ46TJ56rln9XeJzWW8okTt/Pnz6ksAi1walbRArrgPmYS81NfWSm1tjTRqB5YqrKw4ENMgVGh2UAQiZ9SJeZ9KiLiF1bXu9m2pra6Rupoaaaits+uQTqynnFt6UoSzEULF83qxqKDQnq25ecuIZgsWOiVbRmb1Hs8iEWbV07gztE28WXVPWjJWPD3mnCDJtRapr6+TrVu3yOkzZ4xws4jqueefl/kLFsjYCeOkRaNYrqQWySsskumzZ8szn35e5i1cqDV1gZw7d06OK+mvqalO+2kkUI9W/WPFhciTFpW85hYpzFNCiEgQcgRjQqFmfhGkXJ9vZAGS5icr33knTfq7lqkRdbUpSyxBxWF/DUPziUVMSJH6hoS8U7Ku7pN5lhvE3eFwjBS4RdPhGOlIqG5Smy+dvyC7d+60VdIzlGAuXbpcyWWVFCoZxApaoGSIvTJZKFNQVKRktEJ27d6lpKVFiooKbVHQxAkTrPfb0Nggly9dkps3rhs5w/IIGT156qQcOXRYTpw4IQ1KcPK1SoHosbgISyOUpNGIqbEdI4TMG2We6Iljx+T40SO2oOb06VNy8/p1JYHqhz5vwjOWIP60yPVrV+XKpasaTq2UFhXLxQvn5bA+e/bsWSXA9VI1Zoxcu3pVTimZu8zQ+ZjRsnDRIhk9erSlAT8atO6DeLMq/cbNmzbUzzBzIRZADS9DochD/QVB41HFwQMHZO+ePXJRCThE9me+9DNKMufbYir8ZmuoJrNG5lm+QkbLy8uFyfcHDh3QPC2WOk3ftOnTbajdVsFbvDQsdVNdfdN2BMAie+jAQUtLza1qC79Y3w/kPhUVe45HmZN6/eoVyzveI+lmLu5BfZ5pBFguyzQOkFjeAeSyTtPPjgMH9u5Xd/vljBLner02ccLElO8K9ZvQLHb2J3XsEJ06cDgc/YSgfUkdbK2P/KLeoI4L9Q6WSrvaqUUzrDpvqZf31v1pL1adh+AcDsfwgql5SrfjX6qSQj09d+q0vP/2O7Jrx04jG88895ysuOsuGTN+nNTU1kmzPsdwMMO4LK5h+52dSkxrlPBUVlTKogULjShBKo8q+XnttdfkgpK7JUuWqn9lsmvPbhtmb1Tyhh/FhYUyaeJEuUvDWLp8uVSNH28kjn0sS5RkQTAP7T8ge3bvNiJ1u6ZGCVu+ksQwBM5Qbr4SoyWLF8vyFStkkR7z9DdrxbE+vvyDH8qeXbtk8qRJslLvb3h/g9Qp6azT8Hn2kUcfkXlz5siG996XA/v3y/SZM+Wzn/usTNOjze1UN4cPHpL169abVbKwtMSmEDz57NMyXkkWRDNdEatAdpuVJBdB0DSfXv7hj2Tnjh1yTcngoiWL5cXPf04mT55s2yhBqY1WpyyvZj1twcraIjdu3JAdO7bLubPnZJnm3fz582XsuCppVNINEbx+/Ybs2btHPtr8kc2PjZvs0xZAaItLiuWhBx+SVZqvzKklTyDvDL1/tHmzvPrjV2zrpZUrlssZJd3Xr1y1+bZMbZgybap88Utf0vSNN0sy+UL+4A7y2awsGJKbV5AnEzUtK1aulGXqT6WSc6ZLNMKSiUuKbeM+N0K+ORyOgUZKR63Wir9A+M0V/qGhtguJOjCiqXUUIyT8mzRxjJQqf8RQ2cmq8xd6adF0OBzDEqq8UX/DMVQeTXX1Nvx9/tw5WxHNtkbM/wsbtYdh8FIlMSXFReqa+qBFJio5nDdvnq06h7wxl5FhWRa2YCm7oiQGv44fP67B5MmoUaPMEspcwQYNj9Xs58+fM+sZVruCogIb4saCefTIUXn91Z/IgX37zHI5pmq0WeGw1o0dO07qGliMdMtWdN+8eUPmzV8gJWWlRoixwH20aZNNB6hmNbe6wWIH2WJxCyRyvsZ79JgxcvLYcbly+bJUVFYq4V1m2xthXYXcrl+3To7r/XpNO4uiHnni8bR10eZtWn0Y8pFTrJJUzliFd2zfIefOnLV5opDFpcuWSnFZanP49FMc9LcKQ/+FRYUajwolcZNktpLg6TNmKAGkKm/RvC+1dwJh3vD++3JW/a6sqJCy0jIZq+ko0bzBesyWVBeUgF7Sdzlp4iQpVzdsO8X7oQNwYO9eG/q+oO/owrnzlt8Qz5vaYWAe7sq7Vlp427ZsMZJ57NhRs5ISPvunljFdQv3DMnxU7zE9YTLhlOt71XQwZYKOSITlTZYEZM4cDsfAoi2La/3bdFX1GQIZ6zkIJf8qKrq8GMiJpsMxIpFS3rQOa8VAhTFKCU71jRty+cplsx7eUhJ45NAhOXTwgA07c495fDzH/RIlfDxXouSTNcwQLyySWN7YAmn//v1So4SR4dcSJSfLli2Txx57VB568EFZuGCBEth6G96+rCSPYfbZc2bLuLFVZinE8rlt6xbZp6SIYfm5s+fI408+IQ/osytXrZTZs2dpuKX2PISVxUhTp09TkjjBElOr8Ty8/6DUKAG9dbPayBD7ZE6fNt2I5HiVtffea9ZX0lhdfcsI2YoVKyVfa9CjR4/Ju2+9bdsNsfJ+7tw58pnPftYsmWVKZiFtZt3T+jBZF/IL6yRD2Lt37bKFVcWFRXL/A/fJLCWObKGE+yhppCpyW02v/haXlFh8uI41kvDIy7179srHmzfLJc2z8vIymT1rtu39ed+998nMmTNtw/2wBdU1qb9do8RT061prqgcZZaJkyeOy4E9e6S5scHe1bixY2X2zNlm+WRh0j2r7zF/mKv7yo9/LFeuXra9U1mo9PAjD8u9a9fKQiX0Y8eNtXCYDnFNOxKjR4+RyZOn2MbzkF3IfGyc2kdn9x0OR3+hLYtr/ZtfTjQdDke3kdbbeKKVQtDoMNw8eszo/397Z/oexXWl8dOLdgnJAoEQu23A2DgYYrxhdoyXOE7ybRYnmflP/GGeeTLPTJ75Mv/AzNexJ7bjxLETjz3xEm8x2ISwSGIREqsACdDe6p7znlu3N1W3Wgto6fcHV1VdVffWraru02+fe++55nG8rUIJfS/hnYJoghC0PofHj5unEn370E8PnskGCCKzDygDzdpis+20q0DFEp62+zdskL379ktbW5v1gcQMQ8tblkmfCsVBjFwfGrJ869dvMK8eZs/56MMP5abuX7t6tezZu0c2b94sjSpEG1XUNCxpkLVr1lo+eOXggWxsbJTVa9dIhZ4PfStPHDsmN6/fMC/s8uUtcvDQIXl61zMWUgjeSXgO4TmF5/KmiusmLXvT5k16zj75/LPP5ML58zaABiGPdqlAXq3iFuGBUFGE+7GbByscAIEJzyzqcrv/lt2rYRXkDXW1skMFXLNeL1yDuNfZ+CJMtGp+dB3AOVAehCI8pLg3CI305Rdfyvnz53R7SnY+sVP27dtn3tLm5iZZ1tJio9rRTeHatasyhGeo/2rrl0jL8hVq/KNy4UKXdJw6aV8QEMx7du+W5547LI88+qg8uGmjlTGiPwA+/fRTC/uE69y6davs1uM2I7xVU7OJ9Ba9n/CkYoQ8+pEiYP9Kfba4p7hAC9ifJv+KPYW2E0LuNhMtUe5rvJqB0NxsQvMff6ZCU/elhWaWwfRgkwlNNTxj+qWTUiPoiyWELHTQDyf4ROuHHQNvELMRzdLLW1fagJ4KFWnj+tmHt9LEpK6jT9+d27flbOcZS2gqr6mplVoVVPDIoTx4uU6dPGXN1UuXNsvePXssXiS8lebtUlOCvoTVKnbOtneYyEXzLTxq5i3UusBbtrK1VR595BHZ8vDDJiAxmhxlwDMIAYz4l33Xe02srmhrlbUQg9UQmqM2sKlfhRD6TWKQz1O7d0l9Y4ML51NVYR49eOU62t1AGnjjEDj9yDd/VjHdaaOqt5gn71nZvOUhtYHjdscw0hvXYBbSDLAznhCF6EOJ86G7wGkVdDeu9apBjspj2x6zPqgQ48gBghJsG4AoxF70X0XTPO53LOLCOKFZ/cqlS/LNN0fkhtYVPwgOqXCGuEO9o7G4CcQafQaIcdmnorQXXug7AyrKl8iDGzdpnVNy4dw5aT/phCbE/84nnlRxuUIiFTHr71pVUyNdXedMaEIwY6T8/gP7LZ4nPJURFavJlIu5Wd9QJzdUyEP44lmhCwU8nSgHQhO9UB3u/mRj1+1WCSH3lNxPXuinU20FlvicZ/a77wtQV1sl8cCW4QgsLUeQFfaFEFLmOAMRJDUMWEL4gWoVG6tWrZLDL7wgL7z4ohx87pA1e8OztW7dOhvAMzw4JEMqYiBm/u+DDy0oOZqo4yp4AARhQoUqBBk8jU1ZwdwBjFKVilOMqK5vqLdZhwa1vEEVsDBuGFT0yssvy49eeUW2bdsmFSq0eq9ele4LXfLd0W/l26NH5aimM50d1sQP4elAyVikXIxNPX+NXg9GekM0j2idEphzXPfDe4ijMXAFIgz9PP/8zddysafbPHmIYbl9+3ar49DwsA3e8U3CEMXuV35wPiX9q1/LgzD0I+ERr9JEMjo65hNsQt9OFxTdzW+O8nHNKAOHQLx2d3fb4B+wft16FZtNJuVwLUPo0qDnQhxONJNvuH+DXRPyodvA+JgLUYTXKA/l4gcFPMPj+gZI6WvI6JHEqP2wQB9MTI8J8Yz+nhh81N5x2gYHnTh5Qs6cPSM9F3tkbGzErhs/DtAvFedC3XHfHZn7QwgpDyg0CSlzQuSONati+5gKC4TdsT6C9XWycvVqeXrXLuuf+MIPXpL9Bw/Kjscfl0ceecT69aGpHELv2LffWV9HxF0EEBpo+sUIa3jFGlRseiA9kOBRRFgjNMGaMFGBc+cW5lVHf8+oxeJEGJ72k6fkjx99JB9/9Ef56A8fyO/f+5389p1fy//+/gPrAwkv5J3BgXS5/vogfAC8fE2oq4oseOWsH6QeCWEGTHBqpjEVesiD82JE/G2tC8Ia4Z6YJ0+PhdA0sanr7o5lgLiGxxdeSPRrBN4LiJHkHl/PHCBc9TgMdoIHE10SIPYwsh+hmNAVYECFOIQijkHzdU1djf04sFHsevyI1h3exOraatuP+49rxuh9zDLkTpzxSqxYsdzuPfKO68ZIhYpFFZq913tNYEKM9t24KZ99/Im8/+7v5N1335XX3/hvefvNt+SN13X51ltig7wUDBQb1Tra/dX7g+4PhJDyhEKTEGIgWLktITR0/eq1Xrl08bJcvnTJBtOoZjJRhgTR2dS8VDZu3iT79u+XH//kJ7J3714bGQ1BAg8nmsrhvYMncTyhZUcjFjKntqbOBvR4j5/3dqFfH4BQikVdWJ7BoWETRwnN332hWz5Sgfn666/LBx98YGL21ImT0n+zXwVQnyxTMYX+mmjuRRm4DshMLJEsNqduQdmNjU26DZ5D3YCNrgpuVeuD49HXEHVc0dqqdY+ZwDxy9Kh5EnEcmswBBKaL84mzZcSmXZ8eiLKqq6uluqbWtmFgE4RwNv6+Qpi6ejsvJuoFzzC6BMBri/6j6O+J7gmoPEZz45wxvZ+Iszmm9+qWimF4XBHGCLFOMTAL125o+QgnBY8pKofa+via8AJDnEJkw5s5Co+qVgVCG4IbzxDrA0MDlrCOue8HBuHpRDSCcRXACQtthS4X8IACvAcgPN1ZkAgh5QSFJiFljpM1EET468QAQuUgxuI777xj8S/xGkLIPG0qqhAeBwIQTayReFyqa+tk244dsnHjpkDAJWVoYFDXXdgfL2rAwMAd1wdTy7Gz6fFYwnOXGEvKzZt9FsIHzb4QTxCZV1VoffzJJ/LNkaMW97KhodEGq2zfsVMOHDgoP/rRj+Xwc89ZfEwI5lgMTefuWqCcUCeIMqub/oHgQ3J1cOs41os8NHFjzvWnn3lGnt29W1atXmV1OXfuvDXRQ2AhtidENXK6fFgDbmnb7e5GzOuI+J0YbQ/Bffz4Xy10FPqO+nO682seE73o6STmTUYQdoQw+u2v37HwSl1nz5mHs6pChbPuRw0q9HpHhkZsif6ZeC5YWngpLQfTV+JKUV8IS59sKktcg1YW4hyDm/CkUA/cA4hy9LVFn1uUs3r1Gtm7Z6/e8wO2fPmHr8gLL7woe/XHxv6DB+TQ84flwMFDcvjw8xYvFfcM14IfKVhC9Lo08R8hZHGSFpowdqUk/VNgHYUwMTEtxISPsHnUxvF5Fomr6Lh+46bFrrzQdUE6OjpM1EAMwpPlPXAIlm5BuCFKVFQgyDnywnOJogEkBAQOmnnR9Ium9YHbt605Fvvg8cQ0iYmRMWvSRZ9AFAlvmsXB1ApdunRZOjrP6PFRG4iyR4UN+oweOnxI9uzbJ8/s2iUPbXnYPHioF0SSNV1rXiyRTChD0gSv3ejtIEF0qbi1i1dhCtGFAOSbN222WJqPbt8uVbW1Jvy6e3os8Dr6kWLKRfQ9hWA0LyEu2spwCeeH6Mb9WL1mtSxdtsy8fz0Xu81TjK4GwNVX74YudcXKQfO4byJHkHqcA/1WMfIfxyLOZxM8s/oPg2+c5xFN1qNSWaX3TW8iAq9DZN7We477iILRRQHXni2Scawt9RDcfxyKfpaY8xxho/AaorVhSb1s/d6jJr6ffPopeeKJnfLUU0/K07r+/ccft7BHa/T5PPjgg9LcvNTuIzKj+wDKwDkKJf2D6jExMd2zpH9yUtg+LNx65nOa2Z69r1CiR5MQYkBo+cEtaOpd1dZmwhIxMhG/8pyKToiZtAHRPOjbiCZlJIQeau/sMDEGbyeEEAbBQIDhaHj10Ix6U0URvHnwPKIcFISR2HU1NXqOMzbyG17T1tZWaVZhBiF36fIlGR0bRXuytK1bKw8/ulXq72uSmoZ6SalITKpiwuhrlIvzw+OqSsrqiGTiLb2u/3VfDioux8fQnIyBLBWSTCB+ZVKFa1zqG5pk05aHZO0D90tFVZVcVdH73TdHpAf9EfUaKs2LGLNzWPl5+G1tbaukbVWb1etW/y05cuSINYkPDg2ZCESTtfn2AvGL1323bslXX38t165es/IxTWbr8hVW3n1LmyVeVSlD+ky6L16Um3osRChG7kNEom8kBh3dGRiUo99+Z57guP4YQLcBeC5xfRC9GHRkz8FkZ+a+2Jpub9bj6+pqbVt//225fPmq/sBA31QV2fprwGb/0foix/G/npDfvfeevPf++9Kjghz5cT9d+YSQcoRCkxBieHEDSQBRCG/ZkoYGmwGm91qv/OH938vJ4381jyQEpMkSaAxkVrHx2aefWWxN9PPD3N8YPW7iVQWNO8bF5rx+vdfNL64CEsITfRHRhN2rYgoByNGHEU2/Dc33Wd9DeOkwv7YPig7vGLygGGCDesAraHEyNS/KRf2tWR8rWjY8eBDP1iSMeui6XSvWgV2I8+JiBZ5XC5IedwNrIAxr9T7s2LnTYl+irKuXVWwe+VYSwyPm6fOj6wuBc0F4r123XlauXKllRuXUqdPy+Z/+JF2YaQij2EcTKr4TEtWKwzD3XrkmR7/6WrpV0NaqCK+rrrGuCa0r20wAQ4hb/1EFzevHvv3WBGxiZNQG/MT1WvpU1J87e056LvRYmCPM1gOBijpDH6Je3oOMdSf8cY/1/sClrDtaWlpsznrccwR+P/bdMRWbep/h/dZ/eL4Iyn/sL3+RL7/4Qs6dOyenT7eriB03DzN+KLifJYSQciQ3jqZat8kCtuNLZFi/LBCw3Zp5bIf9Drf9hJCFjJvPFh4xiBusd3ddsDBDQ4N3LGj5+c6zJo6u9FySTsw9rgLjqy++tOb1/v5+8+5teOB+eebZ3dLQsMSES/+NPhVWJ0zEQbBcu3bdAqGjmRfTKLa3t8snn36qyw4Z0m2tq9rk8PPPW8geNBdjasmuri6psr6IQzI8gHBKg1avnvNd8ucvv7LpEQcGBm1QC7yamG8bc6bDqwexioFD8AxWqWB76OEtcl/zUr1eFZ96rbbUfwN3tLxzXSbQmpubZeNDD6nIrNfdEWlsUuGrdb/UfdEGO0Fw36fbrDlcrxETWGRbQb/uLSRm9kEMS/QVvXz5ig3KuXrlqnlI0e8SQe+v6frZjk5pP35CvlYRelrrPKzX1FjfYDE8H9u+3UbM22j0SjfA53rvNYv7iWsbvH3HpvGs1HOgTOT/6ovPratCTOXr1q2PyFNPPyM1tfXmBT6v966z47RgNif0eW1tg8c1agLbBDrsvIpNDPDpwLPRe3/rVr815+NeQdCiC8AXn38hx479xbo4VGq9tj22zYK++1iqTtxn351ClHIMIWS2cRqu0OfP9bH3n2Gv91yOiNTXlhiw/R9++lLJAduH1EDCkMOwmpGGG4QQsqDxzacwFgDNpYhfCc8jpoXEiGgIDfQHvAxxdKFLLl7sUbHk5kNH0za8gitXr7LBIOiTaMZEi4UQOnXihAlLsyMjw9bXEfNrQ0CeUXF1VsUrmnvRXI7BJWvWrrN51GtralX7RG1Akp/mEE3kXSp6IX7aT7dLZ2enHYO5uUcQWkfrnNCyIHbqlzTI4OCgCuLjNu83vHqIAwrBljFzru8iPIIdp07JFb0+zBv+wMaNlh+iDM3b8FzCu9rTo2JzeMTEZpMKUsy0Aw8eysuUmbGl2GZ9NVUAwtOLUdnwQiI/vIRYom493d12XZga8grmHY/HbRpJTC2JKR8x5zm8tKgLvIiICeqmoxw0wY4ZgHq6e+RM5xm7Nx0d7TYbU3V1lWxSIfn0M7ssWgCCzcNre1mfW6eKfHiRMac5ZlKCt9eqDTcn6qxXAUGNq0DczuvXb9rMUBe6L9izw3nOnDkr/Xoe3B9EItix4/v6HJfqj4643YRSRKa9/Qgh9xB86HI/eGEfQ/fd4AZRBl8TdqD/VCNgewyfc113bUeuHByLlCs01b5MxaNpMwOZAYF+Dc1FCFlgmChQ44C+lQisjr6FS1uWWUxGCEH0CYxXVjpBN+4GAyGoO7x1u/fslV27nzWPJJp3YWVgpG7cvCGnT5+2KSARAgkitP/2LRukglHm6PuJfqGbNj8kew/slwdV4MFrh3NgEFJDQ4MsW94iN/puSh88aiqsICYHhgbNoC3Reh44dEi2fu97Nlodc39jxp+1G9ZbgHhYp5MnT1r8SvRhxKw1S1tanF1DAbrEyPkbmq/95GmbWQjTVJrQbNL8KhBRnxoV3hCUiC0JYYXA6Aj6vl6vyZres8wgVrNemmBHM3xDfb0sW9YirUHcSojEW3du6w/4EbOvuMcQezE935p1a/We7pHv79wp96kAj8RiFl8T9yuhzwdhi5bp9kYVzfgR0K9iE/f1li7Ny6rnXblqtTy6bZs89eRT5sWtqa8zBwFEYPvpUybSIeYf2LjJ9uM68OWA9wH+WTcFFaoY3IN5zxFbE31HR/XZ3LqNUEpD1g8TonfnE0/ITq3rGhWsiL0KcY33kY9RWgzUNfeOEULuFU7DFfr8ZTyaZi5N97mjkW8yj2akcv0rqWhqVD56/z+kXlUmglGYSXCf+gyaD5v68cUwMCjD6MekhsrmO0/hN29WwYSQBYcfsOFHI+MDj1iR8JhhQBA2QOBAHGJKQwzqQR9J9OOsVKHR3HyfE5yNSySmYgXHWr/KxLic7eyQX73xhs31vXnLFpurHEIQnlJ48+CNbGlZYUKuWQWeBflWYYvg6+jHif6Y8OJhqsvrel4IQcSXrNTzYI50iDc02TY01Etv73UTWUkVdU1aJ1wVruPqpSsmmiCAISLRKpMhZT+e8av8xuVrGEpu3sQlzc1q59S4av1czEqx0ea3VexiEA26F0BMQQjjeuxcga3Esf4+YgV1sn6LWq+aqhq7F2jmxrX09ferUESsUhfUHgKyWcUjPMooo2X5cheiSOuA54TBVwjgDvGLMtEnFcHkTWyqCIR3FHWCyMbzwDOKB55K/EjAIC7rWzk6JtcuX7L99oWxBNehQlNPau8CPVccolQTnufwyJCMjiZUYA+ZmMV7wOq3tMW6BqA/p12DngOX7aMToK+uCdfgC2oibjam4I4RQu4JZrFszVl9JyTzQZg67IGdRkOHfY516YRlRFYsa5Qq1Y9wVKL3OxShlax/NCuFJiEkF2duAnLsgL4I7ACwY9wml4IdaGa2pQoMi+OoIg9zoP/PG6/bXOdooj1w8KDzKAa4LM44Qf753jgQHxA5+aS3hJwfIL8vB5utDLWQOA7Yof6FYufVAlBGXBPCFgFcS0L3updOKCFbOqvuyBQTGFfdEGRPH2vb9V8iELC23R+UJlNSMWDsITYhMLHuySkuq6h0XXIqFXECUJe41szXiwpQHKBiGOAvaotn6a4g7zxK5lRuLb0/qCcEfXGR6ZlsPyFkdsl8or1ly/98g5kKzYyVIoSQADMSSLAn6QTjkZsgWLCEGUrqOlJ6nwKBgnLspRopF+goKA/7g2Mtv71WXFbbD4Nmx+clJ3s0oRxNeI1pE60vZfDalwPMW6uvff7sdXttIIO7hvEgoV5oxnbJFYjj88/vrlmxYzIp+17gNTy8IJ3Xr9txflk8+VHiaM7P3p4+p9Yhe3s6Ybvfh/zm4XTb/HNAwn/cL58gFHG92ceEp7zrQX5dmic4OFfxRAhZjMRiTZusj+bPX53mYCAcDYtJCFkU5AqGiSkU3ZG/308tia0YYX7ixAkZHB6SFStWyPoNG2xEss+D5IRM5vVkZB+XnS8/gexloaQVxl9jwj4j185l73PrmfqH4SS2P9bh113KF2uzkzxh27OXNsJc8U/Ng83Y41PebSiyv/j9IITMNRM/oZlX/oPsfnBiaT/87QDdpyvOCzr5YCB6NAkhM8dZmJwEQwOvG5pozehogsfPewP9YJXs5DybIYUVSU6ghadix2Xvc7hlfp0c2cfkpkyZ2ceHUShfbv6Zpqni8/h6+Lql66jPzDy1PgX70qngfkIIodAkhNxN1MJAWGJwyIqVLsB4TV2dxCqrTMSgD6ZPJngCjTIdwTQzvDDyIik/Ab+cGtMVgPcSq6Nenm/OB/O9zoSQOaZEkxipXP9DGwz04XscDEQIKR184kv51GPQCuY1TyQSKijdtIiVVZVmMzBa2oMQOM6TFpSrBodW5d7h73WI6Z8RFKyEzFfw6XSfUKfhXMuKI2MRJg4Ggu5zAzUxyJCDgQgh9xzE1kSycD4Wh7FG6hrqLXwRQt2gjzeChqe9aEhBXjI32BeDW83Bb59uIoSUNxSahJBpU0hU4BcwAnV7Eoj7OK6/ivUXMKaoxNzmFlonnWiKCCFkIeB9nTD66fUipGcG+vmrL5Y86nx4dEzGOOqckLLGi8sQU2FNJtiDJhY3YjELfWmb8lO+ybJthBBC7g5p45tmotmdOOrcmtmTaCJ3Vpujzgkhdx0YlWyiUTVFE7yUzgwh5f9z2wkhhCwoTFEWt+AUmoQQQggh5K5AoUkIIYQQQu4KsVij66P5s7+fTh9NHI0dbPYihOQSUbPgEwgxKRPwxxSyQYQQQmYLGNlcQzvR7OqWsD6augI7jW2l99HEHkIIIYQQQmYJE5pQnoQQQgghhMwmUS8yKTYJITMFdiQslYQe6Jpe3DohhJC7TdrqToI7zh9ZSg5Puuncn2oqmQkhxFNMWJasGwsVQAghZJbJVnz56i9jjF2vS+BUYu6RkxvsWLRp02vRvMFARkhebEoPBsIElhawHTvyT0wIIYQQQhYqpu9U3CVtMBDW9UUwCMgt3XpdDQO2E0IIIYSQOYBCkxBCCCGETBmENzIXZhEoNAkhhBBCyF0hq4/mC1Pqo5lIpSSV1UeTEEIIIYQsPLyKy5Z+fj2FjpZKNOijaegSnkpsQx/NqO5CGeyjSQghhBBCpo0TlJk0GRSahBBCCCFk6qQmF5sqNAM3KCGEEEIIIbMIPZqEEEIIIeSuEAwGSspP/266g4E0E52ihBBCCCELmmw5l1l3azY1j2o/11TulthWW1vCYCDbgBVCCCGEEEJmCTadE0IIIYSQuwI9moQQQgghZBpMrh7p0SSEEEIIIVPHddgsCoUmIYQQQgiZBpN7NGPRpo2vRWTcRp1XqTLlqHNCCCGEkPIjW875dUxB6UaUQ++5qSf96HPMSllbUz1h1DnAiHOUQY8mIYQQQgi5K1BoLiLcr4nyTYQQQgiZX5jQ5Bc1IYQQQgiZCWFaMhJf91IqKmPy3q//XZao7KzERuzJ73epG7Gp/84d6RsclOHxpCRjcd0CrUqpSgghhBCyEPEKLl/6gWTQHzMagd5LScwEYcr6ZcY1LVvaJBW6C8fEdH9U11BOUv+gnyY9moQQQgghZMaEaclAgxJCCCGEEDK7mEeTEEIIIYSQ2cK7MSk0CSGEEELIrAOxaQHbo5KUV//2eanULaUEbB8ZG5Mx9PL0AdsVNsATQgghhCwOvAxEwHZgAdu96vMDhDT5gO0AqhDYX/2THgw0NSgpCSGEEELI5HgROkUoNgkhhBBCSHGcRzPlpCPlIyGEEEIImSpOQ05UkrFY48bXIjKe7qOZbkt3zew5YBP6aA5bH82kpNBHEzOqpyhRCSGEEEIWIl7FZUs/W4fECzZaEzj6ZgZL6EWs+j6a/rXt1oQ/GM4zjT6ahBBCCCGEhJMWm0qO0PQbDS9LfUoTCRyY+kf/+zxYZidCCCGEELIwMamngg4TSrpJJb26K13l4cgJHs1i2f1p3KB2f+pAeOal0qsxkZDimEpIhBBCCCFTxeu7fJy+8EITpIJj/XJyQpvOwzKGF5aRN/6EPmFXeB5CCCGEEDL/ySi5fIeWF5vZhDm9YrHGTa9FJSGv/s3zUqFHFOq06QUkBgIhJZJ6gugkXTxnoDTDKksK458PIeWN/yQstkQIIXOBar1gNFDEBn8H9giDgmybSF1NtcQC/ehVoVku/cPBQISQRURgABcli/naCCELl4xtMgdh8NJEaJAoNAkhZEEAC14oEULI/MSEpqnQu0SYSUSajLA8TIUTIaScoRUghNwtsu1LoDoipduc9BSUsy02rQrebxqSilUxuIwFmLJG4s9BIuVN3keMiYmJiamsUpgyKC2FKZpMAn4J3PZUIDaz94QxbY/mZAUDV5XwhJOGbWeaXiIE5Bqdckv4F7Z98Sf/lxBSvuTbhVIT8MtcZkdlRCrXvJSKy4j89q1fSq3Kzjg2BikbnAajh24NDknf4ICMJPV1NKrbnXEnwV2b2fOYEXwO5Y29A+fw/TenpN/85XkDYIfdkhBSbnjz5zyTU2ViiKIM2ONHnUckFo3petJGn2MblnFNLc1NUhXox6g71IBmHNeU8WhiZ8n4y0ImpkxS9Y1v+rlIofWZaiILFf+JtJVyTISfYELKlNzPfpiBLCWFkXuMa2SHg9GlzL7imEezQkbkN2/+UmpjpXo0B2U0mZTxII6maR1bKx1/Y9x0louFubsY9wxmcn6fd1E9kDLD+7XKFW9Vyg9cubv68r0HhJQPE7+vnQws8Xs820zoof5lId+m92hG0YodeDKxROzMmK4vb15S1KOpQvMHKjSHSxKa40kIzUHpHxqSMX2RDIYShR1fCiiTzBbBW2zayt3nm25+Mrfg08RPVPkCa46/fA8QsjjJ/27OvLZGTWOy7/FC2wPbUWB3CopRd0ZjKjR1PaLaz5Z6fIWuL1OhWa1CUyVkEaGZGpZ33vy3SYVmQoXmwNCI3BkZloSq2URqXLeqwMEJcYypXkf2ehjF9853plN75Ck1X9gTCGPiMcW9mpOXWcpZyfwg91kHEiPYBAOw0JjEZBRH85bLe3eyZ4svAULIYiPvc51t8ItSmj1IppIF/VReaMZUaCZVB6IxOwHPo+apjMdl2dLGtNBED0I7pSZkS8A2O6E5okLzX4sKTYD8wypPRxIJO1MsFksfl8JJscS3hd2AMAruWGAUuI7Mz4rZpaCXstD2MPyxhfNk9iyW57T4sCnACoBdYR6tYnnmmmI/SCf7sZrP/L3Ke0O07O8AIYuVmXy2C+dVaRmsZb47wr5D0h5NazrHd0qwXV9E9UV1ZVQqbEOQgv1QhYHQfFmF5qgKzX+RGhWa5vrUhON89ZDPL1ViWmYc4/HH+uP9cjrMJG8545/RTOC9XxgUetbFdNlURdtCpZzfw9lfGoSQxUghCzfzz36YwMyg+1IRiQR9NCE4oQGhBZPJpP3IjaMlBUUgBdVMC81420upqui4vP3mL6Q68GjmC02AvFYoMukOCFJ7rckf6/NNF18OmTp4PtnLqeLvO+//wsJ+aKbJe/plIi5JgLkZ+MwJIVPEvvghNcPth/8qiUYw2jyjExLj49ayHfOuR589MEXQh9ZHM9p6OFVTEZPfvP3PJh59yhaNPi/AevZrHOMT8MupMtP8xJH9bKYC7zshhBBSfkxHN3jN4PNiGcjNHIZGkhKJrzycqqyMy9u/+ifLaElzwAtqBUGS5oEug9khjfwSJ8rfNhXuVhdHQghZLJiZDDGwud5tQgi5N6AxBSYpFihN77AcTohUx3VftGlXKl5fK//1n7+Q+rqIVFWqIVNxCVepz1hoEGP2Zr9e4NBQpnIsIYQQR6masthx2EdtSgiZKtBu2Y5B/MiFZoRWHB5KSWVlxJa1qilNhEZXHEhF4zGprIjqwQlJjI3awfEYAnImJZkcn2CMMifASmYvAoZOJGybq+hCw9V5OjVfiFdLCJk7wm2Gt6ZhVrWQQ6AQ4ZaZEEImZ2IoRbwOtqWiEovEJRavkLHxcYnEVr2o9iapG2K6M6n/0VaeMUGFbVepZmqxmbMpWnNCCJkWpdsajASdeHRmS7lEHSCE3Csy9iXMUkVNaMZkLJGQSMXal9UCpcwQuVhJ88UglW5k5wc05ISQ2QQ2sFQ7GGZ/ppKfEEKmT/4Ym2g0IvFYXEa90ESfzAg6cVrfTByda5ymLaFmZOMWmoGk0CTzHL5Fy4DshwwbyodOCJkL3ExC467p/AdqiXRDXJUmbFJaaE4mNoMtRfTgxDxhhBVQpNB5jd1At1qGzHXz3HyeAedeMNn9Rwy0xfz2tF/MZYx7/uVrfwgpb+bf9180FpHkeDIjNG1kkP6ZNbEQXPPUS9OMC9FWMjYTmcfw7bn4odAkpFzxInP+iU0Rkf8HU6Doith28UYAAAAASUVORK5CYII=)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96rwnbdqrMGP",
        "outputId": "26212523-47af-4178-9ef1-088045a40e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=d85eb2d1e8b07ddf3d587a5fcd751d8ce75da5776bd0ab5b687462b043d8d24d\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkB3GeatYvk"
      },
      "source": [
        "# **1. `createDataFrame()`**\n",
        "\n",
        "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HyVaJlMtNIS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R2osOHm1vBL",
        "outputId": "df6742e9-00ae-4461-d6a0-3a3f2f1d74d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|Maheer|\n",
            "|  2|  Wafa|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, \"Maheer\"), (2, \"Wafa\")]\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= ['id', 'name'])\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQWRkOkwEdfD",
        "outputId": "0a628331-c31e-453c-e95b-2bde3794e1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKdSmrBlFCJM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_F_lyROFyvD",
        "outputId": "f1d6cd97-450b-464c-80e3-d87419b6701d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class StructType in module pyspark.sql.types:\n",
            "\n",
            "class StructType(DataType)\n",
            " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |  \n",
            " |  Struct type, consisting of a list of :class:`StructField`.\n",
            " |  \n",
            " |  This is the data type representing a :class:`Row`.\n",
            " |  \n",
            " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
            " |  A contained :class:`StructField` can be accessed by its name or position.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import *\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1[\"f1\"]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  >>> struct1[0]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  \n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
            " |  >>> struct1 == struct2\n",
            " |  False\n",
            " |  \n",
            " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
            " |  using class:`StructType` and class:`StructField`:\n",
            " |  \n",
            " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
            " |  >>> schema = StructType([\n",
            " |  ...     StructField(\"name\", StringType()),\n",
            " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
            " |  ... ])\n",
            " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
            " |  >>> df.printSchema()\n",
            " |  root\n",
            " |   |-- name: string (nullable = true)\n",
            " |   |-- languagesSkills: array (nullable = true)\n",
            " |   |    |-- element: string (containsNull = true)\n",
            " |  >>> df.show()\n",
            " |  +-----+---------------+\n",
            " |  | name|languagesSkills|\n",
            " |  +-----+---------------+\n",
            " |  |Alice|  [Java, Scala]|\n",
            " |  |  Bob|[Python, Scala]|\n",
            " |  +-----+---------------+\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructType\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
            " |      Access fields by name or slice.\n",
            " |  \n",
            " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
            " |      Iterate the fields\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Return the number of fields.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
            " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
            " |      The method accepts either:\n",
            " |      \n",
            " |          a) A single parameter which is a :class:`StructField` object.\n",
            " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
            " |             metadata(optional). The data_type parameter may be either a String or a\n",
            " |             :class:`DataType` object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field : str or :class:`StructField`\n",
            " |          Either the name of the field or a :class:`StructField` object\n",
            " |      data_type : :class:`DataType`, optional\n",
            " |          If present, the DataType of the :class:`StructField` to create\n",
            " |      nullable : bool, optional\n",
            " |          Whether the field to add should be nullable (default True)\n",
            " |      metadata : dict, optional\n",
            " |          Any additional metadata (default None)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
            " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |  \n",
            " |  fieldNames(self) -> List[str]\n",
            " |      Returns all field names in a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
            " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct.fieldNames()\n",
            " |      ['f1']\n",
            " |  \n",
            " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: Tuple) -> Tuple\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
            " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
            " |      \n",
            " |      Below is a JSON schema it must adhere to::\n",
            " |      \n",
            " |          {\n",
            " |            \"title\":\"StructType\",\n",
            " |            \"description\":\"Schema of StructType in json format\",\n",
            " |            \"type\":\"object\",\n",
            " |            \"properties\":{\n",
            " |               \"fields\":{\n",
            " |                  \"description\":\"Array of struct fields\",\n",
            " |                  \"type\":\"array\",\n",
            " |                  \"items\":{\n",
            " |                      \"type\":\"object\",\n",
            " |                      \"properties\":{\n",
            " |                         \"name\":{\n",
            " |                            \"description\":\"Name of the field\",\n",
            " |                            \"type\":\"string\"\n",
            " |                         },\n",
            " |                         \"type\":{\n",
            " |                            \"description\": \"Type of the field. Can either be\n",
            " |                                            another nested StructType or primitive type\",\n",
            " |                            \"type\":\"object/string\"\n",
            " |                         },\n",
            " |                         \"nullable\":{\n",
            " |                            \"description\":\"If nulls are allowed\",\n",
            " |                            \"type\":\"boolean\"\n",
            " |                         },\n",
            " |                         \"metadata\":{\n",
            " |                            \"description\":\"Additional metadata to supply\",\n",
            " |                            \"type\":\"object\"\n",
            " |                         },\n",
            " |                         \"required\":[\n",
            " |                            \"name\",\n",
            " |                            \"type\",\n",
            " |                            \"nullable\",\n",
            " |                            \"metadata\"\n",
            " |                         ]\n",
            " |                      }\n",
            " |                 }\n",
            " |              }\n",
            " |           }\n",
            " |         }\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      json : dict or a dict-like object e.g. JSON object\n",
            " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
            " |          each of which must have specific keys (name, type, nullable, metadata).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> json_str = '''\n",
            " |      ...  {\n",
            " |      ...      \"fields\": [\n",
            " |      ...          {\n",
            " |      ...              \"metadata\": {},\n",
            " |      ...              \"name\": \"Person\",\n",
            " |      ...              \"nullable\": true,\n",
            " |      ...              \"type\": {\n",
            " |      ...                  \"fields\": [\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"name\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      },\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"surname\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      }\n",
            " |      ...                  ],\n",
            " |      ...                  \"type\": \"struct\"\n",
            " |      ...              }\n",
            " |      ...          }\n",
            " |      ...      ],\n",
            " |      ...      \"type\": \"struct\"\n",
            " |      ...  }\n",
            " |      ...  '''\n",
            " |      >>> import json\n",
            " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
            " |      >>> scheme.simpleString()\n",
            " |      'struct<Person:struct<name:string,surname:string>>'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from DataType:\n",
            " |  \n",
            " |  typeName() -> str from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(StructType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21I5ldKKF3qZ",
        "outputId": "a2c7d63f-223e-47f6-b6fc-050f8469b863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|Maheer|\n",
            "|  2|  Wafa|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, \"Maheer\"), (2, \"Wafa\")]\n",
        "\n",
        "schema= StructType([\n",
        "    StructField(name= \"id\", dataType= IntegerType()),\n",
        "    StructField(name= \"name\", dataType= StringType())\n",
        "])\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= schema)\n",
        "df.show()\n",
        "\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5H7qin4HyOF",
        "outputId": "fd94b4a0-ab99-42e6-8dac-5100cde4ad24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|Maheer|\n",
            "|  2|  Wafa|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [{'id':1, 'name': 'Maheer'},\n",
        "     {'id':2, 'name': 'Wafa'}]\n",
        "\n",
        "df= spark.createDataFrame(data= data)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrGWi1FIlH1",
        "outputId": "9a6d65c1-e6be-43b5-dd11-2d871e510f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8a7jlwDTgje"
      },
      "source": [
        "# Read **.csv** File into **Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUP42TW2UlAa",
        "outputId": "6b133b2d-4ef7-4499-bb0c-6fc5f5116bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
            "\n",
            "class DataFrameReader(OptionUtils)\n",
            " |  DataFrameReader(spark: 'SparkSession')\n",
            " |  \n",
            " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
            " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
            " |  to access this.\n",
            " |  \n",
            " |  .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  .. versionchanged:: 3.4.0\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataFrameReader\n",
            " |      OptionUtils\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, spark: 'SparkSession')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
            " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            " |      \n",
            " |      This function will go through the input once to determine the input schema if\n",
            " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |          string, or list of strings, for input path(s),\n",
            " |          or RDD of Strings storing CSV rows.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a CSV file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  format(self, source: str) -> 'DataFrameReader'\n",
            " |      Specifies the input data source format.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      source : str\n",
            " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.format('json')\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.format('json').load(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
            " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
            " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
            " |      \n",
            " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
            " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
            " |      is needed when ``column`` is specified.\n",
            " |      \n",
            " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      table : str\n",
            " |          the name of the table\n",
            " |      column : str, optional\n",
            " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      predicates : list, optional\n",
            " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
            " |          each one defines one partition of the :class:`DataFrame`\n",
            " |      properties : dict, optional\n",
            " |          a dictionary of JDBC database connection arguments. Normally at\n",
            " |          least properties \"user\" and \"password\" with their corresponding values.\n",
            " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Don't create too many partitions in parallel on a large cluster;\n",
            " |      otherwise Spark might crash your external database systems.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |  \n",
            " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            " |      \n",
            " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            " |      \n",
            " |      If the ``schema`` parameter is not specified, this function goes\n",
            " |      through the input once to determine the input schema.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str, list or :class:`RDD`\n",
            " |          string represents path to the JSON dataset, or a list of paths,\n",
            " |          or RDD of Strings storing JSON objects.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.json(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list, optional\n",
            " |          optional string or a list of string for file-system backed data sources.\n",
            " |      format : str, optional\n",
            " |          optional string for format of the data source. Default to 'parquet'.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      **options : dict\n",
            " |          all other string options\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Load a CSV file with format, schema and options specified.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     df = spark.read.load(\n",
            " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
            " |      ...     df.printSchema()\n",
            " |      ...     df.show()\n",
            " |      root\n",
            " |       |-- age: long (nullable = true)\n",
            " |       |-- name: string (nullable = true)\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds an input option for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          The key for the option to set.\n",
            " |      value\n",
            " |          The value for the option to set.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.schema(df.schema).option(\n",
            " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds input options for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **options : dict\n",
            " |          The dictionary of string keys and prmitive-type values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header.\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     spark.read.options(\n",
            " |      ...         nullValue=\"Hyukjin Kwon\",\n",
            " |      ...         header=True\n",
            " |      ...     ).format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a ORC file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a ORC file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.orc(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      **options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a Parquet file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a Parquet file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.parquet(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
            " |      Specifies the input schema.\n",
            " |      \n",
            " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
            " |      By specifying the schema here, the underlying data source can skip the schema\n",
            " |      inference step, and thus speed up data loading.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
            " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
            " |          (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the schema with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
            " |      root\n",
            " |       |-- col0: integer (nullable = true)\n",
            " |       |-- col1: double (nullable = true)\n",
            " |  \n",
            " |  table(self, tableName: str) -> 'DataFrame'\n",
            " |      Returns the specified table as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tableName : str\n",
            " |          string, name of the table.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.createOrReplaceTempView('tblA')\n",
            " |      >>> spark.read.table('tblA').show()\n",
            " |      +---+\n",
            " |      | id|\n",
            " |      +---+\n",
            " |      |  0|\n",
            " |      |  1|\n",
            " |      |  2|\n",
            " |      |  3|\n",
            " |      |  4|\n",
            " |      |  5|\n",
            " |      |  6|\n",
            " |      |  7|\n",
            " |      |  8|\n",
            " |      |  9|\n",
            " |      +---+\n",
            " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
            " |  \n",
            " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
            " |      string column named \"value\", and followed by partitioned columns if there\n",
            " |      are any.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str or list\n",
            " |          string, or list of strings, for input path(s).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a text file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a text file\n",
            " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the text file as a DataFrame.\n",
            " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
            " |      +---------+\n",
            " |      |alphabets|\n",
            " |      +---------+\n",
            " |      |        a|\n",
            " |      |        b|\n",
            " |      |        c|\n",
            " |      +---------+\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from OptionUtils:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(spark.read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKG0vbb6Td4A",
        "outputId": "d79e6bbd-cb71-4d91-8886-dfaa16c3e6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000| 237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000| 204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000| 218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000| 441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000| 599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000| 138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000| 107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000| 595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000|8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.read.csv(path= '/content/sample_data/california_housing_test.csv', header= True)\n",
        "df.show()\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCzPj_TVIpsj",
        "outputId": "f7de3a51-dbf9-4c7c-a807-55f92fbc2d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XH8nzWqhV8w7",
        "outputId": "8f54b3a2-f205-4d9c-c54f-7922adb7e437"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSAXt9JbV_jm",
        "outputId": "f586140f-6ffd-4ce4-a195-426deff96609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|        _c0|      _c1|               _c2|        _c3|           _c4|        _c5|       _c6|          _c7|               _c8|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= spark.read.csv(path= '/content/sample_data/california_housing_test.csv')\n",
        "df1.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWs9iDpvWUNh",
        "outputId": "bc85f001-7b2f-445c-a04d-988e44af1574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dd= spark.read.format('csv').load(path= '/content/sample_data/california_housing_test.csv', header= True)\n",
        "dd.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njrtj1JzWxYU",
        "outputId": "91cdb49b-16a5-479a-9ad4-f9efdd28f62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dd1= spark.read.format('csv').option('header', True).load(path= '/content/sample_data/california_housing_test.csv')\n",
        "dd1.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwYp-7sXXJG3",
        "outputId": "e736c1f7-7cae-47ee-a64d-fc3eb8bcc1ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000| 237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000| 204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000| 218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000| 441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000| 599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000| 138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000| 107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000| 595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000|8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.read.csv(path= '/content/sample_data/california_housing_test.csv', header= True, sep = ',')\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSTzJHshXjq2",
        "outputId": "c0631443-bdb3-4f7c-9dae-083ca4dde5e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000|472.000000|     1.493600|      66900.000000|\n",
            "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000|463.000000|     1.820000|      80100.000000|\n",
            "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000|117.000000|     1.650900|      85700.000000|\n",
            "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000|226.000000|     3.191700|      73400.000000|\n",
            "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000|262.000000|     1.925000|      65500.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.read.csv(path= ['/content/sample_data/california_housing_test.csv', '/content/sample_data/california_housing_train.csv'], header= True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J_MAud8YEEA",
        "outputId": "b44bbfeb-3793-4a8f-c09c-441be1c2e325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wa8TY3ZChU"
      },
      "source": [
        "```python\n",
        "# Read all the from a folder but they must have same schema\n",
        "\n",
        "df= spark.read.csv(path= '/content/sample_data/', header= True)\n",
        "df.show(3)\n",
        "\n",
        "df.printSchema()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsAy99ZzZAVj",
        "outputId": "c60a83d4-09d0-478a-ebd6-2bb1aed34e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+\n",
            "|longitude|latitude|housing_median_age|\n",
            "+---------+--------+------------------+\n",
            "|  -122.05|   37.37|              27.0|\n",
            "|   -118.3|   34.26|              43.0|\n",
            "|  -117.81|   33.78|              27.0|\n",
            "|  -118.36|   33.82|              28.0|\n",
            "|  -119.67|   36.33|              19.0|\n",
            "+---------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "schema= StructType().add(field= 'longitude', data_type= DoubleType()).add(field= 'latitude', data_type= DoubleType()).add(field= 'housing_median_age', data_type= DoubleType())\n",
        "\n",
        "df= spark.read.csv(path= '/content/sample_data/california_housing_test.csv', header= True, schema= schema)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPwU_PlgYnvg",
        "outputId": "c15ea43b-7b50-4993-8bd7-eec91de102f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+\n",
            "|longitude|latitude|housing_median_age|\n",
            "+---------+--------+------------------+\n",
            "|  -122.05|   37.37|              27.0|\n",
            "|   -118.3|   34.26|              43.0|\n",
            "|  -117.81|   33.78|              27.0|\n",
            "|  -118.36|   33.82|              28.0|\n",
            "|  -119.67|   36.33|              19.0|\n",
            "+---------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "schema= StructType().add(field= 'longitude', data_type= DoubleType())\\\n",
        "                    .add(field= 'latitude', data_type= DoubleType())\\\n",
        "                    .add(field= 'housing_median_age', data_type= DoubleType())\n",
        "\n",
        "df= spark.read.csv(path= '/content/sample_data/california_housing_test.csv', header= True, schema= schema)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QwdkwaLbzpn",
        "outputId": "6449e5ea-34c3-43d6-90fc-e4074e27e64f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq2vXLiCuQYd"
      },
      "source": [
        "# **3. `write()` csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nCzJbvpuPSK",
        "outputId": "78a83394-f456-4cf4-eb98-d773cee585cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=0d64dc14d0d5a435d276c72929315b95946c8cbd36b66a7b3f2c67e5e46aa295\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LPaO0LMb53z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXRMYNY6u73R",
        "outputId": "7e3fec03-d255-4ecd-d563-dcca244deee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|Maheer|\n",
            "|  2|  Wafa|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import dataframe\n",
        "\n",
        "\n",
        "data= [(1, \"Maheer\"), (2, \"Wafa\")]\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= ['id', 'name'])\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ay5M9rUxws7E",
        "outputId": "dbc57968-ff02-4e14-d77a-1c75eb37d015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jen9FTudwu8o"
      },
      "outputs": [],
      "source": [
        "# df.write.csv(path= '/content/test.csv', header= True)\n",
        "\n",
        "df.write.option(key= 'header', value= True).csv(path= '/content/test.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oJeXIJhYudg"
      },
      "outputs": [],
      "source": [
        "df.write.option(key= 'header', value= True).csv(path= '/content/test1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E_CCyKxZtwv"
      },
      "outputs": [],
      "source": [
        "df.write.csv(path= '/content/test2.csv', header= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpZ_Px0SaAXc"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/test2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWCj3ElZaUvP"
      },
      "outputs": [],
      "source": [
        "df.write.csv(path= '/content/test.csv', header= True, mode= \"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQh7gv5FkSPz"
      },
      "source": [
        "# Read json file into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guvym3xRaulP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1I3yjdIkW6c",
        "outputId": "eaf1afbd-9291-46fc-96bd-ce02eeda9a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Series: string (nullable = true)\n",
            " |-- X: double (nullable = true)\n",
            " |-- Y: double (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n",
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.read.json(path= '/content/sample_data/anscombe.json')\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4AbM0iMnHVl"
      },
      "source": [
        "## Read Multiline `.json` file\n",
        "\n",
        "```py\n",
        "df1= spark.read.json(path= '/content/sample_data/anscombe.json', multiline= True)\n",
        "df1.show(10)\n",
        "```\n",
        "\n",
        "## Read Multiple `.json` files\n",
        "\n",
        "```py\n",
        "df1= spark.read.json(path= ['/content/sample_data/anscombe.json', '/content/sample_data/anscombe1.json'])\n",
        "df1.show(10)\n",
        "```\n",
        "\n",
        "## Read `.json` files from folder\n",
        "\n",
        "```py\n",
        "df1= spark.read.json(path= ['/content/sample_data/anscombe.json', '/content/sample_data/anscombe1.json'])\n",
        "df1.show(10)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knCh3gMblKuL",
        "outputId": "00502d49-849d-42f4-f847-7a49c24fd1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= spark.read.json(path= ['/content/sample_data/anscombe.json', '/content/sample_data/anscombe.json'])\n",
        "df1.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEI61KTenrel",
        "outputId": "64d93d1f-e0b6-44d7-8cec-5d4b7b9e77f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= spark.read.json(path= '/content/sample_data/*.json')\n",
        "df1.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yks4OgzFn87c",
        "outputId": "97dd443e-06e7-4abf-9349-613a0d779df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|   X|    Y|\n",
            "+----+-----+\n",
            "|NULL| NULL|\n",
            "|10.0| 8.04|\n",
            "| 8.0| 6.95|\n",
            "|13.0| 7.58|\n",
            "| 9.0| 8.81|\n",
            "|11.0| 8.33|\n",
            "|14.0| 9.96|\n",
            "| 6.0| 7.24|\n",
            "| 4.0| 4.26|\n",
            "|12.0|10.84|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "schema= StructType().add(field= 'X', data_type= DoubleType())\\\n",
        "                    .add(field= 'Y', data_type= DoubleType())\n",
        "\n",
        "df1= spark.read.json(path= '/content/sample_data/*.json', schema= schema)\n",
        "df1.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDQEXn_0pEul"
      },
      "source": [
        "# Write `.json` File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RGTd_lLofUm",
        "outputId": "773d9c09-1205-4557-9a2c-f211c572edff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000| 237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000| 204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000| 218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000| 441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000| 599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000| 138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000| 107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000| 595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000|8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.read.csv(path= '/content/sample_data/california_housing_test.csv', header= True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh7uEk9fo0Ch"
      },
      "outputs": [],
      "source": [
        "df.write.json(path= '/content/test.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDuUmenapt9g"
      },
      "source": [
        "# Read `.parquet` File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26kIJXmBpKmW",
        "outputId": "41758912-f005-4186-fcfd-3692238507b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+-------+-----+------+------+------+---------+----+\n",
            "|movieId|  imdbId| tmdbId|title|genres|userId|rating|timestamp| tag|\n",
            "+-------+--------+-------+-----+------+------+------+---------+----+\n",
            "|      1|114709.0|  862.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      2|113497.0| 8844.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      3|113228.0|15602.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      4|114885.0|31357.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      5|113041.0|11862.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      6|113277.0|  949.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      7|114319.0|11860.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      8|112302.0|45325.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|      9|114576.0| 9091.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "|     10|113189.0|  710.0| NULL|  NULL|  NULL|  NULL|     NULL|NULL|\n",
            "+-------+--------+-------+-----+------+------+------+---------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dd= spark.read.parquet('/content/file.parquet')\n",
        "dd.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLxWUxe_qaKY",
        "outputId": "234be614-5284-4181-84b8-b7a4a620be51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "124003"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dd.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YonlH2IVrKrk"
      },
      "source": [
        "## Read Multiple `.parquet` Files\n",
        "\n",
        "```py\n",
        "df= spark.read.parquet(\"path/*.parquet\")\n",
        "df.show(10)\n",
        "```\n",
        "\n",
        "# Write `.parquet` Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV94tOovrJfL"
      },
      "outputs": [],
      "source": [
        "dd.write.parquet(path= '/content/test.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juy_0XoOrBcc"
      },
      "outputs": [],
      "source": [
        "dd.write.parquet(path= '/content/testoutput')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50cRaQyu0Lzm"
      },
      "source": [
        "# `.show()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpUlBkFesKy-",
        "outputId": "47a36542-445a-4290-a7b9-390c3490acf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|             comment|\n",
            "+---+--------------------+\n",
            "|  1|sfsjfjsgcdjhsvhsc...|\n",
            "|  2|mcjkcshcksbnvsdvs...|\n",
            "|  3|cnzcsdcsdjcgzdvcz...|\n",
            "|  4|msjcbsvhcvscsdjcg...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from textwrap import shorten\n",
        "data= [(1, 'sfsjfjsgcdjhsvhscscccsvcsd'),\n",
        "       (2, \"mcjkcshcksbnvsdvsvsdvcsvcs\"),\n",
        "       (3, \"cnzcsdcsdjcgzdvczxvczxcvzxc\"),\n",
        "       (4, \"msjcbsvhcvscsdjcgvsdcvsdkcs\")]\n",
        "\n",
        "schema= ['id', 'comment']\n",
        "\n",
        "df= spark.createDataFrame(data, schema= schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUsMrIOW0sXZ",
        "outputId": "ff4a7984-64a2-4485-d89a-00f1697c7780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------------+\n",
            "|id |comment                    |\n",
            "+---+---------------------------+\n",
            "|1  |sfsjfjsgcdjhsvhscscccsvcsd |\n",
            "|2  |mcjkcshcksbnvsdvsvsdvcsvcs |\n",
            "|3  |cnzcsdcsdjcgzdvczxvczxcvzxc|\n",
            "|4  |msjcbsvhcvscsdjcgvsdcvsdkcs|\n",
            "+---+---------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(truncate= False) # Show full lenght value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SLQVfJZ0-Pl",
        "outputId": "2060d5ca-94d8-4377-ba1b-769d90789a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id| comment|\n",
            "+---+--------+\n",
            "|  1|sfsjf...|\n",
            "|  2|mcjkc...|\n",
            "|  3|cnzcs...|\n",
            "|  4|msjcb...|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(truncate= 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFIWuMU61BBX",
        "outputId": "dec1acb4-0be9-4a50-dac0-df772eebceeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------------+\n",
            "|id |comment                   |\n",
            "+---+--------------------------+\n",
            "|1  |sfsjfjsgcdjhsvhscscccsvcsd|\n",
            "|2  |mcjkcshcksbnvsdvsvsdvcsvcs|\n",
            "+---+--------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(n=2, truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCUmbNIn1Flh",
        "outputId": "156e9214-faf5-4c9b-b9bc-839c01ba009a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-RECORD 0-----------------------------\n",
            " id      | 1                          \n",
            " comment | sfsjfjsgcdjhsvhscscccsvcsd \n",
            "-RECORD 1-----------------------------\n",
            " id      | 2                          \n",
            " comment | mcjkcshcksbnvsdvsvsdvcsvcs \n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(n=2, truncate= False, vertical=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJSHlMfN1YbH"
      },
      "source": [
        "# `withColumn()`\n",
        "\n",
        "**Add new Column or Chance Value in Column or Change Datatype in Dataframe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZfOqG-I1PWI",
        "outputId": "6b4ae08a-d704-40a6-fd44-cd581be863b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+\n",
            "| id|  name|other|\n",
            "+---+------+-----+\n",
            "|  1| Wasiq| 6000|\n",
            "|  2|Zainul| 3000|\n",
            "+---+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, \"Wasiq\", '6000'),\n",
        "       (2, \"Zainul\",'3000')]\n",
        "\n",
        "columns= ['id', 'name', 'other']\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuneFLt92Kfd",
        "outputId": "08ba945d-b8f5-4f3b-9b36-923e45b95374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXoYnETG2Q-K",
        "outputId": "abffc7d3-dda3-4373-a171-6cffe49f7ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+\n",
            "| id|  name|other|\n",
            "+---+------+-----+\n",
            "|  1| Wasiq| 6000|\n",
            "|  2|Zainul| 3000|\n",
            "+---+------+-----+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df1= df.withColumn(colName= 'other', col= col(\"other\").cast(\"Integer\"))\n",
        "\n",
        "df1.show()\n",
        "df1.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxTMM94T3odc",
        "outputId": "2a550c2e-78ef-47f0-de3f-f7e6e1116e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+\n",
            "| id|  name|other|\n",
            "+---+------+-----+\n",
            "|  1| Wasiq|12000|\n",
            "|  2|Zainul| 6000|\n",
            "+---+------+-----+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2= df1.withColumn(colName= 'other', col= col(\"other\") * 2)\n",
        "df2.show()\n",
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXd3YLuk2-E7",
        "outputId": "e7e8d691-22b4-465e-d8c8-1cdac5256aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+-------+\n",
            "| id|  name|other|country|\n",
            "+---+------+-----+-------+\n",
            "|  1| Wasiq|12000|  India|\n",
            "|  2|Zainul| 6000|  India|\n",
            "+---+------+-----+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            " |-- country: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, lit\n",
        "df3= df2.withColumn('country', lit( \"India\"))\n",
        "df3.show()\n",
        "df3.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgWNDc1a4alB",
        "outputId": "28ac06cc-80d4-4f9a-8ab8-c2ccc1832157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+-------+----------+\n",
            "| id|  name|other|country|my_country|\n",
            "+---+------+-----+-------+----------+\n",
            "|  1| Wasiq|12000|  India|     India|\n",
            "|  2|Zainul| 6000|  India|     India|\n",
            "+---+------+-----+-------+----------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            " |-- country: string (nullable = false)\n",
            " |-- my_country: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df4= df3.withColumn('my_country', col(\"country\"))\n",
        "df4.show()\n",
        "df4.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1STeX7nQHDib"
      },
      "source": [
        "# `withColumnRenamed()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9uH4P-DG_hM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJ_9duqHd24",
        "outputId": "130734b4-2e50-4e26-a894-51786a3487bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "| id|  name|others|\n",
            "+---+------+------+\n",
            "|  1| Wasiq|  2000|\n",
            "|  2|Zainul|  3000|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, \"Wasiq\", '2000'), (2, \"Zainul\", '3000')]\n",
        "columns= ['id', 'name', 'others']\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3KJne8SIYHo",
        "outputId": "fd880c2d-5d66-4b12-fc2b-d5723933bf2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+---------+\n",
            "| id|  name|new_other|\n",
            "+---+------+---------+\n",
            "|  1| Wasiq|     2000|\n",
            "|  2|Zainul|     3000|\n",
            "+---+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumnRenamed(existing= 'others', new= 'new_other').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE9lpEa1H9_5",
        "outputId": "5b43bee4-2fbf-4344-af7a-f6fd015fb2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------------+\n",
            "| id|  name|latest_other|\n",
            "+---+------+------------+\n",
            "|  1| Wasiq|        2000|\n",
            "|  2|Zainul|        3000|\n",
            "+---+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= df.withColumnRenamed('others', 'latest_other')\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPytorZEKhav"
      },
      "source": [
        "# `StructType()` and `StructField()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO_E13M2Ilvv",
        "outputId": "c9be070c-ce77-4c1c-a620-35686f8b40a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-----+\n",
            "| id|  name|other|\n",
            "+---+------+-----+\n",
            "|  1| Wasiq| 2000|\n",
            "|  2|Zainul| 3000|\n",
            "+---+------+-----+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "\n",
        "data= [(1, \"Wasiq\", 2000), (2, \"Zainul\", 3000)]\n",
        "schema= StructType([StructField(name= 'id', dataType= IntegerType()),\\\n",
        "                    StructField(name= 'name', dataType= StringType()),\\\n",
        "                    StructField(name= 'other', dataType= IntegerType())\\\n",
        "                    ])\n",
        "df= spark.createDataFrame(data= data, schema= schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGEWMHqILu8l",
        "outputId": "493ae232-49c7-40e8-efd8-13dc1b065357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----+\n",
            "| _1|    _2|  _3|\n",
            "+---+------+----+\n",
            "|  1| Wasiq|2000|\n",
            "|  2|Zainul|3000|\n",
            "+---+------+----+\n",
            "\n",
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            " |-- _3: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, \"Wasiq\", 2000), (2, \"Zainul\", 3000)]\n",
        "df= spark.createDataFrame(data= data)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9XI34kLUDUg",
        "outputId": "49b9a029-5494-4778-a323-5902f7478466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------------+-----+\n",
            "| id|             name|other|\n",
            "+---+-----------------+-----+\n",
            "|  1|{Mohammad, Wasiq}| 2000|\n",
            "|  2|  {Zainul, Pasha}| 3000|\n",
            "+---+-----------------+-----+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- first_name: string (nullable = true)\n",
            " |    |-- last_name: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Complex Columns\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "\n",
        "data= [(1, (\"Mohammad\", \"Wasiq\"), 2000), (2, (\"Zainul\", \"Pasha\"), 3000)]\n",
        "\n",
        "struct_name= StructType([StructField('first_name', StringType()),\\\n",
        "                        StructField('last_name', StringType())])\n",
        "\n",
        "schema= StructType([\\\n",
        "                    StructField(name= 'id', dataType= IntegerType()),\\\n",
        "                    StructField(name= 'name', dataType= struct_name),\\\n",
        "                    StructField(name= 'other', dataType= IntegerType())])\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJfpNE7WO05"
      },
      "source": [
        "# `ArrayType` Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxSSywurVUil",
        "outputId": "650d0ebb-00aa-4c5a-d8a7-29649b8ff5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "| id|numbers|\n",
            "+---+-------+\n",
            "|abc| [1, 2]|\n",
            "|xyz| [4, 5]|\n",
            "|mno| [7, 8]|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- numbers: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [('abc', [1, 2]), ('xyz', [4, 5]), ('mno', [7, 8])]\n",
        "\n",
        "schema= ['id', 'numbers']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X73irYU3W9jZ",
        "outputId": "b91a9b86-bcba-4834-829e-794406c4a717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "|abc|numbers|\n",
            "+---+-------+\n",
            "|abc| [1, 2]|\n",
            "|xyz| [4, 5]|\n",
            "|mno| [7, 8]|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- abc: string (nullable = true)\n",
            " |-- numbers: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "data= [('abc', [1, 2]), ('xyz', [4, 5]), ('mno', [7, 8])]\n",
        "\n",
        "schema= StructType([\\\n",
        "                    StructField(name= 'abc', dataType= StringType()),\\\n",
        "                    StructField(name= 'numbers', dataType= ArrayType(IntegerType()))])\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bYUJLc6Xvco",
        "outputId": "a9cecd20-3434-4248-cb29-9dd5eb026245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+------------+\n",
            "|abc|numbers|first_number|\n",
            "+---+-------+------------+\n",
            "|abc| [1, 2]|           1|\n",
            "|xyz| [4, 5]|           4|\n",
            "|mno| [7, 8]|           7|\n",
            "+---+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn('first_number', df.numbers[0]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242thlJRYrWX",
        "outputId": "40e2e43e-1f74-48d7-9728-190a968dba8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+\n",
            "|num1|num2|\n",
            "+----+----+\n",
            "|   1|   2|\n",
            "|   3|   4|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 2), (3, 4)]\n",
        "\n",
        "schema= ['num1', 'num2']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FRQ-A_h3UlT",
        "outputId": "4a3cc512-ca8a-473a-d427-155565cdfa9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-------+\n",
            "|num1|num2|numbers|\n",
            "+----+----+-------+\n",
            "|   1|   2| [1, 2]|\n",
            "|   3|   4| [3, 4]|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import array, col\n",
        "df.withColumn('numbers', array(col('num1'), col('num2'))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5GeJmXl94P7"
      },
      "source": [
        "# `explode()`, `split()`, `array()`, and `array_contains()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOc3Ottr30TE",
        "outputId": "fc785690-cdb0-410f-9da6-1831d31e0a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+--------------+\n",
            "| id|  name|        skills|\n",
            "+---+------+--------------+\n",
            "|  1| Wasiq|   [DS, GenAI]|\n",
            "|  2|Zainul|[MS Office, C]|\n",
            "+---+------+--------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', ['DS', 'GenAI']), (2, 'Zainul', ['MS Office', 'C'])]\n",
        "\n",
        "schema= ['id', 'name', 'skills']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6gFEBX0-3gA"
      },
      "source": [
        "* Use **`explode()`** function to create a new row for each element in the given array column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWVX8bvI-vpe",
        "outputId": "ddfb60da-547c-434f-8903-4545f0ad2aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+--------------+\n",
            "| id|  name|        skills|\n",
            "+---+------+--------------+\n",
            "|  1| Wasiq|   [DS, GenAI]|\n",
            "|  2|Zainul|[MS Office, C]|\n",
            "+---+------+--------------+\n",
            "\n",
            "+---+------+--------------+---------+\n",
            "| id|  name|        skills|    skill|\n",
            "+---+------+--------------+---------+\n",
            "|  1| Wasiq|   [DS, GenAI]|       DS|\n",
            "|  1| Wasiq|   [DS, GenAI]|    GenAI|\n",
            "|  2|Zainul|[MS Office, C]|MS Office|\n",
            "|  2|Zainul|[MS Office, C]|        C|\n",
            "+---+------+--------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "df.show()\n",
        "\n",
        "df.withColumn('skill', explode(col('skills'))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB3zZAi_iyK",
        "outputId": "027a76ed-a75f-4716-a30c-56584cae9fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------------+\n",
            "| id|  name|      skills|\n",
            "+---+------+------------+\n",
            "|  1| Wasiq|   DS, GenAI|\n",
            "|  2|Zainul|MS Office, C|\n",
            "+---+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', 'DS, GenAI'), (2, 'Zainul', 'MS Office, C')]\n",
        "\n",
        "schema= ['id', 'name', 'skills']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hufz2X-zAC3m",
        "outputId": "b4e56707-ef2a-4398-806c-4df7609cdbe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------------+\n",
            "| id|  name|      skills|\n",
            "+---+------+------------+\n",
            "|  1| Wasiq|   DS, GenAI|\n",
            "|  2|Zainul|MS Office, C|\n",
            "+---+------+------------+\n",
            "\n",
            "+---+------+------------+---------------+\n",
            "| id|  name|      skills|    skillsArray|\n",
            "+---+------+------------+---------------+\n",
            "|  1| Wasiq|   DS, GenAI|   [DS,  GenAI]|\n",
            "|  2|Zainul|MS Office, C|[MS Office,  C]|\n",
            "+---+------+------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.withColumn('skillsArray', split(col('skills'), ',')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy3o3tBEAbCm",
        "outputId": "e9700190-8a52-4624-e1f5-b179325a0e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+---------+-------+\n",
            "| id|  name|  skills1|skills2|\n",
            "+---+------+---------+-------+\n",
            "|  1| Wasiq|       DS|  GenAI|\n",
            "|  2|Zainul|MS Office|      C|\n",
            "+---+------+---------+-------+\n",
            "\n",
            "+---+------+---------+-------+--------------+\n",
            "| id|  name|  skills1|skills2|   skillsArray|\n",
            "+---+------+---------+-------+--------------+\n",
            "|  1| Wasiq|       DS|  GenAI|   [DS, GenAI]|\n",
            "|  2|Zainul|MS Office|      C|[MS Office, C]|\n",
            "+---+------+---------+-------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split, col, array\n",
        "\n",
        "data= [(1, 'Wasiq', 'DS', 'GenAI'), (2, 'Zainul', 'MS Office', 'C')]\n",
        "\n",
        "schema= ['id', 'name', 'skills1', 'skills2']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "\n",
        "df.withColumn('skillsArray', array(col('skills1'), col('skills2'))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7zhyziqM9ea"
      },
      "source": [
        "# `MapType()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ny0wIZGugJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Colab PySpark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbcyTAOINBog",
        "outputId": "fb17df67-e72b-4c87-f5df-6a6d4d0b7bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "| name|          properties|\n",
            "+-----+--------------------+\n",
            "|wasiq|{eye -> brown, ha...|\n",
            "|  xyz|{eye -> blue, hai...|\n",
            "+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [('wasiq', {'hair':'black', 'eye':'brown'}),\n",
        "       ('xyz', {'hair':'black', 'eye':'blue'})]\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= ['name', 'properties'])\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOQVxcYBURS8",
        "outputId": "82499f66-3600-4dd9-eddc-2641079e749f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+\n",
            "|name |properties                   |\n",
            "+-----+-----------------------------+\n",
            "|wasiq|{eye -> brown, hair -> black}|\n",
            "|xyz  |{eye -> blue, hair -> black} |\n",
            "+-----+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PazafyomUZGk",
        "outputId": "25fd642e-74ec-4e4d-9ebd-e41f0c1442a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+\n",
            "|name |properties                   |\n",
            "+-----+-----------------------------+\n",
            "|wasiq|{eye -> brown, hair -> black}|\n",
            "|xyz  |{eye -> blue, hair -> black} |\n",
            "+-----+-----------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import MapType, StringType, StructType, StructField\n",
        "data= [('wasiq', {'hair':'black', 'eye':'brown'}),\n",
        "       ('xyz', {'hair':'black', 'eye':'blue'})]\n",
        "\n",
        "schema= StructType([\n",
        "    StructField(name= 'name', dataType= StringType()),\n",
        "    StructField(name= 'properties', dataType= MapType(keyType= StringType(), valueType= StringType()))\n",
        "])\n",
        "\n",
        "df= spark.createDataFrame(data= data, schema= schema)\n",
        "df.show(truncate= False)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFFvLMS_U9la",
        "outputId": "ee727503-9019-47b9-8ea5-9c8a5c73ec5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+-----+\n",
            "|name |properties                   |hair |\n",
            "+-----+-----------------------------+-----+\n",
            "|wasiq|{eye -> brown, hair -> black}|black|\n",
            "|xyz  |{eye -> blue, hair -> black} |black|\n",
            "+-----+-----------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= df.withColumn('hair', df.properties['hair'])\n",
        "df1.show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Jw4YgkVH8i",
        "outputId": "ffa5e52e-de49-4570-c2d8-6275b0f2efca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+-----+-----+\n",
            "|name |properties                   |hair |eye  |\n",
            "+-----+-----------------------------+-----+-----+\n",
            "|wasiq|{eye -> brown, hair -> black}|black|brown|\n",
            "|xyz  |{eye -> blue, hair -> black} |black|blue |\n",
            "+-----+-----------------------------+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1= df1.withColumn('eye', df.properties.getItem('eye'))\n",
        "df1.show(truncate= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k02AozJVigL"
      },
      "source": [
        "# `explode()`, `map_keys()`, `map_values()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s68jZHzVc2N",
        "outputId": "beeb088b-6aed-4dd8-c3c8-487363b1ec0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+----+-----+\n",
            "|name |properties                   |key |value|\n",
            "+-----+-----------------------------+----+-----+\n",
            "|wasiq|{eye -> brown, hair -> black}|eye |brown|\n",
            "|wasiq|{eye -> brown, hair -> black}|hair|black|\n",
            "|xyz  |{eye -> blue, hair -> black} |eye |blue |\n",
            "|xyz  |{eye -> blue, hair -> black} |hair|black|\n",
            "+-----+-----------------------------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode, map_keys, map_values\n",
        "\n",
        "df.select('name', 'properties', explode(df.properties)).show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L9Ojuu9WOkV",
        "outputId": "a0881750-6b28-4d0d-96bd-dba4c6a3e740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+--------------------+\n",
            "|name |properties                   |map_keys(properties)|\n",
            "+-----+-----------------------------+--------------------+\n",
            "|wasiq|{eye -> brown, hair -> black}|[eye, hair]         |\n",
            "|xyz  |{eye -> blue, hair -> black} |[eye, hair]         |\n",
            "+-----+-----------------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('name', 'properties', map_keys(df.properties)).show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blwa-re-XLCA",
        "outputId": "0be59086-9548-4907-dca6-02ba51639226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+-----------+\n",
            "|name |properties                   |keys       |\n",
            "+-----+-----------------------------+-----------+\n",
            "|wasiq|{eye -> brown, hair -> black}|[eye, hair]|\n",
            "|xyz  |{eye -> blue, hair -> black} |[eye, hair]|\n",
            "+-----+-----------------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn('keys', map_keys(df.properties)).show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqo9fz_xWbtc",
        "outputId": "4eb40264-c1b6-4fb6-c41b-03ccb0ce1f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+----------------------+\n",
            "|name |properties                   |map_values(properties)|\n",
            "+-----+-----------------------------+----------------------+\n",
            "|wasiq|{eye -> brown, hair -> black}|[brown, black]        |\n",
            "|xyz  |{eye -> blue, hair -> black} |[blue, black]         |\n",
            "+-----+-----------------------------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('name', 'properties', map_values(df.properties)).show(truncate= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQgBTf_DW9sT",
        "outputId": "831fe816-23d7-4a88-9bc4-5fbecfcc46f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------+--------------+\n",
            "|name |properties                   |values        |\n",
            "+-----+-----------------------------+--------------+\n",
            "|wasiq|{eye -> brown, hair -> black}|[brown, black]|\n",
            "|xyz  |{eye -> blue, hair -> black} |[blue, black] |\n",
            "+-----+-----------------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn('values', map_values(df.properties)).show(truncate= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSLdZ3CIXmMD"
      },
      "source": [
        "# `Row()`\n",
        "`pyspark.sql.Row()` which is represented as a record/row in DataFrame, one can create a Row object by using named arguments or create a custom Row like class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVCBGoGoXj_B",
        "outputId": "aee1deac-340b-4139-eac7-30a73de96d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasiq 2000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "row1= Row('wasiq', 2000)\n",
        "\n",
        "print(row1[0] + \" \" + str(row1[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRVfkLM-XDSV",
        "outputId": "b8394310-a2e7-42b2-9b84-56976ec7fa6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasiq 2000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "row1= Row(name= 'wasiq', other= 2000)\n",
        "\n",
        "print(row1.name + \" \" + str(row1.other))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQfXub11Yt3G",
        "outputId": "a951098a-18fd-4680-91de-2a6515b0eb33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|  name|other|\n",
            "+------+-----+\n",
            "| Wasiq| 2000|\n",
            "|Zainul| 3000|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "row1= Row(name= 'Wasiq', other= 2000)\n",
        "row2= Row(name= 'Zainul', other= 3000)\n",
        "\n",
        "data= [row1, row2]\n",
        "spark.createDataFrame(data).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akjZ2GA3ZGUX",
        "outputId": "8b1053b0-969f-44b0-dfa4-421104f28211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasiq 17\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "Person= Row('name', 'age')\n",
        "person1= Person('wasiq',  24)\n",
        "person2= Person('zainul', 17)\n",
        "\n",
        "print(person1.name + \" \" + str(person2.age))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KD-Pa0PZnZK",
        "outputId": "042b886b-e42f-4358-873c-e6a1729161e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---+\n",
            "|  name|age|\n",
            "+------+---+\n",
            "| wasiq| 24|\n",
            "|zainul| 17|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "Person= Row('name', 'age')\n",
        "person1= Person('wasiq',  24)\n",
        "person2= Person('zainul', 17)\n",
        "\n",
        "data= [person1, person2]\n",
        "spark.createDataFrame(data).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxXYQ5xGZze_",
        "outputId": "94b1f113-4fa8-4092-9f1a-2bc397cd9c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---+\n",
            "|  name|age|\n",
            "+------+---+\n",
            "| wasiq| 24|\n",
            "|zainul| 17|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "Person= Row('name', 'age')\n",
        "person1= Person('wasiq',  24)\n",
        "person2= Person('zainul', 17)\n",
        "\n",
        "spark.createDataFrame([person1, person2]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I3SE2KmZ2GC",
        "outputId": "0ae59d78-8279-4f36-b6c2-b08dfe1d52bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------------+\n",
            "|name|         prop|\n",
            "+----+-------------+\n",
            "| abc|{black, blue}|\n",
            "| xyz|{grey, black}|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data= [Row(name= \"abc\", prop= Row(hair= 'black', eye= 'blue')),\n",
        "       Row(name= \"xyz\", prop= Row(hair= 'grey', eye= 'black'))]\n",
        "\n",
        "spark.createDataFrame(data).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th1vL5IRahhI"
      },
      "source": [
        "# Column class in PySpark\n",
        "\n",
        "* PySpark Column class represent a single Column in a DataFrame.\n",
        "* `pyspark.sql.Column` class provides several functions to work with\n",
        "DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or partof ævaiue rom a DataFrame column.\n",
        "* One of the simplest ways to create a Column class object is by using\n",
        "PySpark `lit()` SQL function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UcG5lx5aRSs",
        "outputId": "340d61ae-7eff-428e-9ab7-03bc0b1554ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.column.Column'>\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql. functions import lit\n",
        "coll= lit('abcd')\n",
        "print(type(coll))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teCSC6K9bNP2",
        "outputId": "160a13b0-1697-4988-fb37-c54fd24994b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+\n",
            "|  name|gender|other|\n",
            "+------+------+-----+\n",
            "| Wasiq|  male| 2000|\n",
            "|Zainul|  male| 3000|\n",
            "+------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql. functions import lit\n",
        "\n",
        "data= [('Wasiq', 'male', 2000), ('Zainul', 'male', 3000)]\n",
        "\n",
        "schema= ['name', 'gender', 'other']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18hIdRoenyMS",
        "outputId": "58d83f11-96be-4a09-cfb5-ac4350958569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+---------+\n",
            "|  name|gender|other|   newCol|\n",
            "+------+------+-----+---------+\n",
            "| Wasiq|  male| 2000|newColVal|\n",
            "|Zainul|  male| 3000|newColVal|\n",
            "+------+------+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn('newCol', lit('newColVal')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7OzXB0bn8cI",
        "outputId": "3c0a5eec-fc55-4d7a-cc3d-7453d892b1d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "| Wasiq|\n",
            "|Zainul|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df.name).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9EcYnJCoJHq",
        "outputId": "ff786f8e-3797-489d-cb3c-fb8dde3fa9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "| Wasiq|\n",
            "|Zainul|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df['name']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQZreLueoToW",
        "outputId": "6c6fc165-7b5b-4b86-99e9-5401bccd57a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "| Wasiq|\n",
            "|Zainul|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.select(col('name')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9putmV_oWrg",
        "outputId": "bb27eaa9-adfc-4854-ff15-5d83e249c0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+-----+--------------+\n",
            "|name|gender|other|         props|\n",
            "+----+------+-----+--------------+\n",
            "| abc|  male| 2000|{black, brown}|\n",
            "| xyz|female| 3000| {black, blue}|\n",
            "+----+------+-----+--------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- other: integer (nullable = true)\n",
            " |-- props: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql. functions import lit\n",
        "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
        "data= [('abc', 'male', 2000, ('black', 'brown')), ('xyz', 'female', 3000, ('black', 'blue'))]\n",
        "\n",
        "prop_type= StructType([\n",
        "    StructField(name= 'hair', dataType= StringType()),\n",
        "    StructField(name= 'eye', dataType= StringType())\n",
        "])\n",
        "\n",
        "schema= StructType([\n",
        "    StructField(name= 'name', dataType= StringType()),\n",
        "    StructField(name= 'gender', dataType= StringType()),\n",
        "    StructField(name= 'other', dataType= IntegerType()),\n",
        "    StructField(name= 'props', dataType= prop_type)\n",
        "])\n",
        "\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUYOHqgupU8x",
        "outputId": "28fd63d9-2069-4877-84f9-3d463f8f2862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|  eye|\n",
            "+-----+\n",
            "|brown|\n",
            "| blue|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.select(col('props.eye')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhh3sS_CqIhI"
      },
      "source": [
        "# `when()` and `otherwise()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9bxic0UpnBk",
        "outputId": "154c2497-9d7f-46d9-c692-17177995c819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+-----+\n",
            "| id|  name|gender|othes|\n",
            "+---+------+------+-----+\n",
            "|  1| Wasiq|     M| 2000|\n",
            "|  2|Zainul|     M| 3000|\n",
            "|  3|   abc|     F| 4000|\n",
            "|  4|   xyz|      | 4000|\n",
            "+---+------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', '', 4000)]\n",
        "schema= ['id', 'name', 'gender', 'othes']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EMizgYK2DNM",
        "outputId": "d6448f97-2ac1-4d05-f627-b3cfef84183a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+-------+\n",
            "| id|  name| Gender|\n",
            "+---+------+-------+\n",
            "|  1| Wasiq|   Male|\n",
            "|  2|Zainul|   Male|\n",
            "|  3|   abc| Female|\n",
            "|  4|   xyz|Unknown|\n",
            "+---+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df.select(df.id, df.name, when(df.gender=='M', 'Male').when(df.gender==\"F\", 'Female').otherwise(\"Unknown\").alias('Gender')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmOT9A_J3tZm"
      },
      "source": [
        "# `alias()`, `asc()`, `desc()`, `cast()` & `like()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO0LNq_12oTk",
        "outputId": "3051bfaf-1bf9-4b11-a988-754db8a91b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', 'M', 4000)]\n",
        "schema= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJvShA8D4lh5",
        "outputId": "ed62d1fd-e112-4bb3-ad9a-2004ddbea5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| ID|  NAME|GENDER|OTHERS|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df.id.alias('ID'), df.name.alias(\"NAME\"), df.gender.alias(\"GENDER\"), df.others.alias(\"OTHERS\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpf3AQHd5AlN",
        "outputId": "2513f580-7f83-4132-da1f-df7338bc7e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  4|   xyz|     M|  4000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  1| Wasiq|     M|  2000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(df.name.desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxhnk6ma5Q1m",
        "outputId": "b135ee29-543e-468b-f527-b2bcc7eb316f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(df.name.asc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypsXw1Wy5S4n",
        "outputId": "b063542e-6cba-4f4c-c10f-5ca2f63b6600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|others|\n",
            "+------+\n",
            "|  2000|\n",
            "|  3000|\n",
            "|  4000|\n",
            "|  4000|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df.others.cast('int')).show() # Convert the data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGPuuDfe5hVL",
        "outputId": "88dd71bc-ebc4-4aac-b148-c5412166ecf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+------+------+\n",
            "| id| name|gender|others|\n",
            "+---+-----+------+------+\n",
            "|  1|Wasiq|     M|  2000|\n",
            "+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.name.like('W%')).show() # Similar to SQL LIKE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dPaJ6DP5vbo",
        "outputId": "cbf935b0-c327-478f-c328-db1e06c55b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+------+------+\n",
            "| id|name|gender|others|\n",
            "+---+----+------+------+\n",
            "|  3| abc|     F|  4000|\n",
            "+---+----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.name.like('a%')).show() # Similar to SQL LIKE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkFXf6xE6Cpi",
        "outputId": "00480398-ab3d-4e9c-dbce-08d7bbc89e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.name.like('%a%')).show() # Similar to SQL LIKE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02vHcw4c5Kjq"
      },
      "source": [
        "# `filter()` and `where()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWsHz2SO6F4H",
        "outputId": "89aa201c-e415-4a7c-8f4f-22b86b12b469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', 'M', 4000)]\n",
        "schema= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzU5uQFi2cCj",
        "outputId": "ba62869e-3811-4009-8fcd-df0b06f0d17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df['gender']==\"M\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNjCitEL5cBD",
        "outputId": "6dc74731-892a-4f86-fc29-d952d5895af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.gender==\"M\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YZM0D4I5jf4",
        "outputId": "c56fa8cf-9701-4c48-d30e-df1a583ce54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(\"gender == 'M'\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK_30lrO5uwo",
        "outputId": "8e729269-3777-46c8-add6-92d4b8c77c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter((df['gender']==\"M\") & (df['others'] >2000)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG3Xd7sx6Iy6",
        "outputId": "71d241ac-8d4e-43ba-e284-705146d36ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where((df['gender']==\"M\") & (df['others'] >2000)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SSK_AF6mCa"
      },
      "source": [
        "# `distinct()` and `dropDuplicates()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdAILAFA6SNC",
        "outputId": "100d970c-fa9b-4855-fecb-532af1ddcb70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', 'M', 4000), (2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000)]\n",
        "schema= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1FWWlu6zGF",
        "outputId": "94a2b46c-f7a0-4583-95ed-91ce1f542bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SafwBLgK62LU",
        "outputId": "12787061-8d9e-4b8a-d552-51c4482acdad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrlcfZ0j66dU",
        "outputId": "b9b29ce5-837e-48fd-8c76-965eeb775ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+------+------+\n",
            "| id| name|gender|others|\n",
            "+---+-----+------+------+\n",
            "|  3|  abc|     F|  4000|\n",
            "|  1|Wasiq|     M|  2000|\n",
            "+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.dropDuplicates(['gender']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fmyLDOI69S8",
        "outputId": "a6ba83b7-d881-42e5-aac2-7633045a515b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on DataFrame in module pyspark.sql.dataframe object:\n",
            "\n",
            "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
            " |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            " |  \n",
            " |  A distributed collection of data grouped into named columns.\n",
            " |  \n",
            " |  .. versionadded:: 1.3.0\n",
            " |  \n",
            " |  .. versionchanged:: 3.4.0\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
            " |  and can be created using various functions in :class:`SparkSession`:\n",
            " |  \n",
            " |  >>> people = spark.createDataFrame([\n",
            " |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
            " |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
            " |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
            " |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
            " |  ... ])\n",
            " |  \n",
            " |  Once created, it can be manipulated using the various domain-specific-language\n",
            " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
            " |  \n",
            " |  To select a column from the :class:`DataFrame`, use the apply method:\n",
            " |  \n",
            " |  >>> age_col = people.age\n",
            " |  \n",
            " |  A more concrete example:\n",
            " |  \n",
            " |  >>> # To create DataFrame using SparkSession\n",
            " |  ... department = spark.createDataFrame([\n",
            " |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
            " |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
            " |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
            " |  ... ])\n",
            " |  \n",
            " |  >>> people.filter(people.age > 30).join(\n",
            " |  ...     department, people.deptId == department.id).groupBy(\n",
            " |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
            " |  +-------+------+-----------+--------+\n",
            " |  |   name|gender|avg(salary)|max(age)|\n",
            " |  +-------+------+-----------+--------+\n",
            " |  |     ML|     F|      150.0|      60|\n",
            " |  |PySpark|     M|       75.0|      50|\n",
            " |  +-------+------+-----------+--------+\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  A DataFrame should only be created as described above. It should not be directly\n",
            " |  created via using the constructor.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataFrame\n",
            " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
            " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __dir__(self) -> List[str]\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import lit\n",
            " |      \n",
            " |      Create a dataframe with a column named 'id'.\n",
            " |      \n",
            " |      >>> df = spark.range(3)\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes column id\n",
            " |      ['id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming']\n",
            " |      \n",
            " |      Add a column named 'i_like_pancakes'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('i_like_pancakes', lit(1))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes columns i_like_pancakes, id\n",
            " |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            " |      \n",
            " |      Try to add an existed column 'inputFiles'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('inputFiles', lit(2))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't duplicate inputFiles\n",
            " |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            " |      \n",
            " |      Try to add a column named 'id2'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('id2', lit(3))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # result includes id2 and sorted\n",
            " |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            " |      \n",
            " |      Don't include columns that are not valid python identifiers.\n",
            " |      \n",
            " |      >>> df = df.withColumn('1', lit(4))\n",
            " |      >>> df = df.withColumn('name 1', lit(5))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't include 1 or name 1\n",
            " |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
            " |      Returns the :class:`Column` denoted by ``name``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Column name to return as :class:`Column`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column`\n",
            " |          Requested column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve a column instance.\n",
            " |      \n",
            " |      >>> df.select(df.age).show()\n",
            " |      +---+\n",
            " |      |age|\n",
            " |      +---+\n",
            " |      |  2|\n",
            " |      |  5|\n",
            " |      +---+\n",
            " |  \n",
            " |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
            " |      Returns the column as a :class:`Column`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      item : int, str, :class:`Column`, list or tuple\n",
            " |          column index, column name, column, or a list or tuple of columns\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column` or :class:`DataFrame`\n",
            " |          a specified column, or a filtered or projected dataframe.\n",
            " |      \n",
            " |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
            " |      \n",
            " |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
            " |              filtered by this given :class:`Column`.\n",
            " |      \n",
            " |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
            " |              projected by this given list or tuple.\n",
            " |      \n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve a column instance.\n",
            " |      \n",
            " |      >>> df.select(df['age']).show()\n",
            " |      +---+\n",
            " |      |age|\n",
            " |      +---+\n",
            " |      |  2|\n",
            " |      |  5|\n",
            " |      +---+\n",
            " |      \n",
            " |      >>> df.select(df[1]).show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      \n",
            " |      Select multiple string columns as index.\n",
            " |      \n",
            " |      >>> df[[\"name\", \"age\"]].show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice|  2|\n",
            " |      |  Bob|  5|\n",
            " |      +-----+---+\n",
            " |      >>> df[df.age > 3].show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df[df[0] > 3].show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
            " |      Aggregate on the entire :class:`DataFrame` without groups\n",
            " |      (shorthand for ``df.groupBy().agg()``).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      exprs : :class:`Column` or dict of key and value strings\n",
            " |          Columns or expressions to aggregate DataFrame by.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Aggregated DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import functions as sf\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.agg({\"age\": \"max\"}).show()\n",
            " |      +--------+\n",
            " |      |max(age)|\n",
            " |      +--------+\n",
            " |      |       5|\n",
            " |      +--------+\n",
            " |      >>> df.agg(sf.min(df.age)).show()\n",
            " |      +--------+\n",
            " |      |min(age)|\n",
            " |      +--------+\n",
            " |      |       2|\n",
            " |      +--------+\n",
            " |  \n",
            " |  alias(self, alias: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` with an alias set.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      alias : str\n",
            " |          an alias name to be set for the :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Aliased DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col, desc\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df_as1 = df.alias(\"df_as1\")\n",
            " |      >>> df_as2 = df.alias(\"df_as2\")\n",
            " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
            " |      >>> joined_df.select(\n",
            " |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
            " |      +-----+-----+---+\n",
            " |      | name| name|age|\n",
            " |      +-----+-----+---+\n",
            " |      |  Tom|  Tom| 14|\n",
            " |      |  Bob|  Bob| 16|\n",
            " |      |Alice|Alice| 23|\n",
            " |      +-----+-----+---+\n",
            " |  \n",
            " |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
            " |      Calculates the approximate quantiles of numerical columns of a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The result of this algorithm has the following deterministic bound:\n",
            " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
            " |      probability `p` up to error `err`, then the algorithm will return\n",
            " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
            " |      close to (p * N). More precisely,\n",
            " |      \n",
            " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
            " |      \n",
            " |      This method implements a variation of the Greenwald-Khanna\n",
            " |      algorithm (with some speed optimizations). The algorithm was first\n",
            " |      present in [[https://doi.org/10.1145/375663.375670\n",
            " |      Space-efficient Online Computation of Quantile Summaries]]\n",
            " |      by Greenwald and Khanna.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col: str, tuple or list\n",
            " |          Can be a single column name, or a list of names for multiple columns.\n",
            " |      \n",
            " |          .. versionchanged:: 2.2.0\n",
            " |             Added support for multiple columns.\n",
            " |      probabilities : list or tuple\n",
            " |          a list of quantile probabilities\n",
            " |          Each number must belong to [0, 1].\n",
            " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
            " |      relativeError : float\n",
            " |          The relative target precision to achieve\n",
            " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
            " |          could be very expensive. Note that values greater than 1 are\n",
            " |          accepted but gives the same result as 1.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          the approximate quantiles at the given probabilities.\n",
            " |      \n",
            " |          * If the input `col` is a string, the output is a list of floats.\n",
            " |      \n",
            " |          * If the input `col` is a list or tuple of strings, the output is also a\n",
            " |              list, but each element in it is a list of floats, i.e., the output\n",
            " |              is a list of list of floats.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Null values will be ignored in numerical columns before calculation.\n",
            " |      For columns only containing null values, an empty list is returned.\n",
            " |  \n",
            " |  cache(self) -> 'DataFrame'\n",
            " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Cached DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.cache()\n",
            " |      DataFrame[id: bigint]\n",
            " |      \n",
            " |      >>> df.explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      AdaptiveSparkPlan isFinalPlan=false\n",
            " |      +- InMemoryTableScan ...\n",
            " |  \n",
            " |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
            " |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
            " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            " |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
            " |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eager : bool, optional, default True\n",
            " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Checkpointed DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import tempfile\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
            " |      ...     df.checkpoint(False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
            " |      \n",
            " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
            " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
            " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
            " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
            " |      it will stay at the current number of partitions.\n",
            " |      \n",
            " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
            " |      this may result in your computation taking place on fewer nodes than\n",
            " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
            " |      you can call repartition(). This will add a shuffle step, but means the\n",
            " |      current upstream partitions will be executed in parallel (per whatever\n",
            " |      the current partitioning is).\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          specify the target number of partitions\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
            " |      1\n",
            " |  \n",
            " |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
            " |      Selects column based on the column name specified as a regex and returns it\n",
            " |      as :class:`Column`.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colName : str\n",
            " |          string, column name specified as a regex.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
            " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
            " |      +----+\n",
            " |      |Col2|\n",
            " |      +----+\n",
            " |      |   1|\n",
            " |      |   2|\n",
            " |      |   3|\n",
            " |      +----+\n",
            " |  \n",
            " |  collect(self) -> List[pyspark.sql.types.Row]\n",
            " |      Returns all the records as a list of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.collect()\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
            " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
            " |      Currently only supports the Pearson Correlation Coefficient.\n",
            " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column\n",
            " |      col2 : str\n",
            " |          The name of the second column\n",
            " |      method : str, optional\n",
            " |          The correlation method. Currently only supports \"pearson\"\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Pearson Correlation Coefficient of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.corr(\"c1\", \"c2\")\n",
            " |      -0.3592106040535498\n",
            " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            " |      >>> df.corr(\"small\", \"bigger\")\n",
            " |      1.0\n",
            " |  \n",
            " |  count(self) -> int\n",
            " |      Returns the number of rows in this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Number of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the number of rows in the :class:`DataFrame`.\n",
            " |      \n",
            " |      >>> df.count()\n",
            " |      3\n",
            " |  \n",
            " |  cov(self, col1: str, col2: str) -> float\n",
            " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
            " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column\n",
            " |      col2 : str\n",
            " |          The name of the second column\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Covariance of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.cov(\"c1\", \"c2\")\n",
            " |      -18.0\n",
            " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            " |      >>> df.cov(\"small\", \"bigger\")\n",
            " |      1.0\n",
            " |  \n",
            " |  createGlobalTempView(self, name: str) -> None\n",
            " |      Creates a global temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary view is tied to this Spark application.\n",
            " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            " |      catalog.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a global temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createGlobalTempView(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      \n",
            " |      Throws an exception if the global temporary view already exists.\n",
            " |      \n",
            " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
            " |      Creates or replaces a global temporary view using the given name.\n",
            " |      \n",
            " |      The lifetime of this temporary view is tied to this Spark application.\n",
            " |      \n",
            " |      .. versionadded:: 2.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a global temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
            " |      \n",
            " |      Replace the global temporary view.\n",
            " |      \n",
            " |      >>> df2 = df.filter(df.age > 3)\n",
            " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
            " |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createOrReplaceTempView(self, name: str) -> None\n",
            " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a local temporary view named 'people'.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createOrReplaceTempView(\"people\")\n",
            " |      \n",
            " |      Replace the local temporary view.\n",
            " |      \n",
            " |      >>> df2 = df.filter(df.age > 3)\n",
            " |      >>> df2.createOrReplaceTempView(\"people\")\n",
            " |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createTempView(self, name: str) -> None\n",
            " |      Creates a local temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            " |      catalog.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a local temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createTempView(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      \n",
            " |      Throw an exception if the table already exists.\n",
            " |      \n",
            " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Returns the cartesian product with another :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Right side of the cartesian product.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Joined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame(\n",
            " |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
            " |      +---+-----+------+\n",
            " |      |age| name|height|\n",
            " |      +---+-----+------+\n",
            " |      | 14|  Tom|    80|\n",
            " |      | 14|  Tom|    85|\n",
            " |      | 23|Alice|    80|\n",
            " |      | 23|Alice|    85|\n",
            " |      | 16|  Bob|    80|\n",
            " |      | 16|  Bob|    85|\n",
            " |      +---+-----+------+\n",
            " |  \n",
            " |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
            " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
            " |      table.\n",
            " |      The first column of each row will be the distinct values of `col1` and the column names\n",
            " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
            " |      Pairs that have no occurrences will have zero as their counts.\n",
            " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column. Distinct items will make the first item of\n",
            " |          each row.\n",
            " |      col2 : str\n",
            " |          The name of the second column. Distinct items will make the column names\n",
            " |          of the :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Frequency matrix of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
            " |      +-----+---+---+---+\n",
            " |      |c1_c2| 10| 11|  8|\n",
            " |      +-----+---+---+---+\n",
            " |      |    1|  0|  2|  0|\n",
            " |      |    3|  1|  0|  0|\n",
            " |      |    4|  0|  0|  2|\n",
            " |      +-----+---+---+---+\n",
            " |  \n",
            " |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
            " |      the specified columns, so we can run aggregations on them.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          columns to create cube by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Cube of the data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            " |      +-----+----+-----+\n",
            " |      | name| age|count|\n",
            " |      +-----+----+-----+\n",
            " |      | NULL|NULL|    2|\n",
            " |      | NULL|   2|    1|\n",
            " |      | NULL|   5|    1|\n",
            " |      |Alice|NULL|    1|\n",
            " |      |Alice|   2|    1|\n",
            " |      |  Bob|NULL|    1|\n",
            " |      |  Bob|   5|    1|\n",
            " |      +-----+----+-----+\n",
            " |  \n",
            " |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
            " |      Computes basic statistics for numeric and string columns.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      This includes count, mean, stddev, min, and max. If no columns are\n",
            " |      given, this function computes statistics for all numerical or string columns.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Use summary for expanded statistics and control over which statistics to compute.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list, optional\n",
            " |           Column name or list of column names to describe by (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            " |      ... )\n",
            " |      >>> df.describe(['age']).show()\n",
            " |      +-------+----+\n",
            " |      |summary| age|\n",
            " |      +-------+----+\n",
            " |      |  count|   3|\n",
            " |      |   mean|12.0|\n",
            " |      | stddev| 1.0|\n",
            " |      |    min|  11|\n",
            " |      |    max|  13|\n",
            " |      +-------+----+\n",
            " |      \n",
            " |      >>> df.describe(['age', 'weight', 'height']).show()\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |summary| age|            weight|           height|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |  count|   3|                 3|                3|\n",
            " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            " |      |    min|  11|              37.8|            142.2|\n",
            " |      |    max|  13|              44.1|            150.5|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.summary\n",
            " |  \n",
            " |  distinct(self) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with distinct records.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the number of distinct rows in the :class:`DataFrame`\n",
            " |      \n",
            " |      >>> df.distinct().count()\n",
            " |      2\n",
            " |  \n",
            " |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` without specified columns.\n",
            " |      This is a no-op if the schema doesn't contain the given column name(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols: str or :class:`Column`\n",
            " |          a name of the column, or the :class:`Column` to drop\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame without given columns.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      When an input is a column name, it is treated literally without further interpretation.\n",
            " |      Otherwise, will try to match the equivalent expression.\n",
            " |      So that dropping column by its name `drop(colName)` has different semantic with directly\n",
            " |      dropping the column `drop(col(colName))`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import col, lit\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      \n",
            " |      >>> df.drop('age').show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |  Tom|\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      >>> df.drop(df.age).show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |  Tom|\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      \n",
            " |      Drop the column that joined both DataFrames on.\n",
            " |      \n",
            " |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
            " |      +---+------+\n",
            " |      |age|height|\n",
            " |      +---+------+\n",
            " |      | 14|    80|\n",
            " |      | 16|    85|\n",
            " |      +---+------+\n",
            " |      \n",
            " |      >>> df3 = df.join(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+------+----+\n",
            " |      |age| name|height|name|\n",
            " |      +---+-----+------+----+\n",
            " |      | 14|  Tom|    80| Tom|\n",
            " |      | 14|  Tom|    85| Bob|\n",
            " |      | 23|Alice|    80| Tom|\n",
            " |      | 23|Alice|    85| Bob|\n",
            " |      | 16|  Bob|    80| Tom|\n",
            " |      | 16|  Bob|    85| Bob|\n",
            " |      +---+-----+------+----+\n",
            " |      \n",
            " |      Drop two column by the same name.\n",
            " |      \n",
            " |      >>> df3.drop(\"name\").show()\n",
            " |      +---+------+\n",
            " |      |age|height|\n",
            " |      +---+------+\n",
            " |      | 14|    80|\n",
            " |      | 14|    85|\n",
            " |      | 23|    80|\n",
            " |      | 23|    85|\n",
            " |      | 16|    80|\n",
            " |      | 16|    85|\n",
            " |      +---+------+\n",
            " |      \n",
            " |      Can not drop col('name') due to ambiguous reference.\n",
            " |      \n",
            " |      >>> df3.drop(col(\"name\")).show()\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
            " |      \n",
            " |      >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
            " |      >>> df4.show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|a.b.c|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  Tom|    1|\n",
            " |      | 23|Alice|    1|\n",
            " |      | 16|  Bob|    1|\n",
            " |      +---+-----+-----+\n",
            " |      \n",
            " |      >>> df4.drop(\"a.b.c\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Can not find a column matching the expression \"a.b.c\".\n",
            " |      \n",
            " |      >>> df4.drop(col(\"a.b.c\")).show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|a.b.c|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  Tom|    1|\n",
            " |      | 23|Alice|    1|\n",
            " |      | 16|  Bob|    1|\n",
            " |      +---+-----+-----+\n",
            " |  \n",
            " |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            " |      optionally only considering certain columns.\n",
            " |      \n",
            " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
            " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
            " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
            " |      be and the system will accordingly limit the state. In addition, data older than\n",
            " |      watermark will be dropped to avoid any possibility of duplicates.\n",
            " |      \n",
            " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      subset : List of column names, optional\n",
            " |          List of columns to use for duplicate comparison (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame without duplicates.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(name='Alice', age=5, height=80),\n",
            " |      ...     Row(name='Alice', age=5, height=80),\n",
            " |      ...     Row(name='Alice', age=10, height=80)\n",
            " |      ... ])\n",
            " |      \n",
            " |      Deduplicate the same rows.\n",
            " |      \n",
            " |      >>> df.dropDuplicates().show()\n",
            " |      +-----+---+------+\n",
            " |      | name|age|height|\n",
            " |      +-----+---+------+\n",
            " |      |Alice|  5|    80|\n",
            " |      |Alice| 10|    80|\n",
            " |      +-----+---+------+\n",
            " |      \n",
            " |      Deduplicate values on 'name' and 'height' columns.\n",
            " |      \n",
            " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
            " |      +-----+---+------+\n",
            " |      | name|age|height|\n",
            " |      +-----+---+------+\n",
            " |      |Alice|  5|    80|\n",
            " |      +-----+---+------+\n",
            " |  \n",
            " |  dropDuplicatesWithinWatermark(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            " |       optionally only considering certain columns, within watermark.\n",
            " |      \n",
            " |       This only works with streaming :class:`DataFrame`, and watermark for the input\n",
            " |       :class:`DataFrame` must be set via :func:`withWatermark`.\n",
            " |      \n",
            " |      For a streaming :class:`DataFrame`, this will keep all data across triggers as intermediate\n",
            " |      state to drop duplicated rows. The state will be kept to guarantee the semantic, \"Events\n",
            " |      are deduplicated as long as the time distance of earliest and latest events are smaller\n",
            " |      than the delay threshold of watermark.\" Users are encouraged to set the delay threshold of\n",
            " |      watermark longer than max timestamp differences among duplicated events.\n",
            " |      \n",
            " |      Note: too late data older than watermark will be dropped.\n",
            " |      \n",
            " |       .. versionadded:: 3.5.0\n",
            " |      \n",
            " |       Parameters\n",
            " |       ----------\n",
            " |       subset : List of column names, optional\n",
            " |           List of columns to use for duplicate comparison (default All columns).\n",
            " |      \n",
            " |       Returns\n",
            " |       -------\n",
            " |       :class:`DataFrame`\n",
            " |           DataFrame without duplicates.\n",
            " |      \n",
            " |       Notes\n",
            " |       -----\n",
            " |       Supports Spark Connect.\n",
            " |      \n",
            " |       Examples\n",
            " |       --------\n",
            " |       >>> from pyspark.sql import Row\n",
            " |       >>> from pyspark.sql.functions import timestamp_seconds\n",
            " |       >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            " |       ...     \"value % 5 AS value\", \"timestamp\")\n",
            " |       >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            " |       DataFrame[value: bigint, time: timestamp]\n",
            " |      \n",
            " |       Deduplicate the same rows.\n",
            " |      \n",
            " |       >>> df.dropDuplicatesWithinWatermark() # doctest: +SKIP\n",
            " |      \n",
            " |       Deduplicate values on 'value' columns.\n",
            " |      \n",
            " |       >>> df.dropDuplicatesWithinWatermark(['value'])  # doctest: +SKIP\n",
            " |  \n",
            " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
            " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4\n",
            " |  \n",
            " |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
            " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      how : str, optional\n",
            " |          'any' or 'all'.\n",
            " |          If 'any', drop a row if it contains any nulls.\n",
            " |          If 'all', drop a row only if all its values are null.\n",
            " |      thresh: int, optional\n",
            " |          default None\n",
            " |          If specified, drop rows that have less than `thresh` non-null values.\n",
            " |          This overwrites the `how` parameter.\n",
            " |      subset : str, tuple or list, optional\n",
            " |          optional list of column names to consider.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with null only rows excluded.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      >>> df.na.drop().show()\n",
            " |      +---+------+-----+\n",
            " |      |age|height| name|\n",
            " |      +---+------+-----+\n",
            " |      | 10|    80|Alice|\n",
            " |      +---+------+-----+\n",
            " |  \n",
            " |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
            " |      not in another :class:`DataFrame` while preserving duplicates.\n",
            " |      \n",
            " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
            " |      As standard in SQL, this function resolves columns by position (not by name).\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          The other :class:`DataFrame` to compare to.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame(\n",
            " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.exceptAll(df2).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  a|  1|\n",
            " |      |  a|  1|\n",
            " |      |  a|  2|\n",
            " |      |  c|  4|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
            " |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      extended : bool, optional\n",
            " |          default ``False``. If ``False``, prints only the physical plan.\n",
            " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
            " |          specified.\n",
            " |      mode : str, optional\n",
            " |          specifies the expected output format of plans.\n",
            " |      \n",
            " |          * ``simple``: Print only a physical plan.\n",
            " |          * ``extended``: Print both logical and physical plans.\n",
            " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
            " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
            " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
            " |      \n",
            " |          .. versionchanged:: 3.0.0\n",
            " |             Added optional argument `mode` to specify the expected output format of plans.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Print out the physical plan only (default).\n",
            " |      \n",
            " |      >>> df.explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      *(1) Scan ExistingRDD[age...,name...]\n",
            " |      \n",
            " |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
            " |      \n",
            " |      >>> df.explain(True)\n",
            " |      == Parsed Logical Plan ==\n",
            " |      ...\n",
            " |      == Analyzed Logical Plan ==\n",
            " |      ...\n",
            " |      == Optimized Logical Plan ==\n",
            " |      ...\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      \n",
            " |      Print out the plans with two sections: a physical plan outline and node details\n",
            " |      \n",
            " |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      * Scan ExistingRDD (...)\n",
            " |      (1) Scan ExistingRDD [codegen id : ...]\n",
            " |      Output [2]: [age..., name...]\n",
            " |      ...\n",
            " |      \n",
            " |      Print a logical plan and statistics if they are available.\n",
            " |      \n",
            " |      >>> df.explain(\"cost\")\n",
            " |      == Optimized Logical Plan ==\n",
            " |      ...Statistics...\n",
            " |      ...\n",
            " |  \n",
            " |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            " |      Replace null values, alias for ``na.fill()``.\n",
            " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      value : int, float, string, bool or dict\n",
            " |          Value to replace null values with.\n",
            " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
            " |          from column name (string) to replacement value. The replacement value must be\n",
            " |          an int, float, boolean, or string.\n",
            " |      subset : str, tuple or list, optional\n",
            " |          optional list of column names to consider.\n",
            " |          Columns specified in subset that do not have matching data types are ignored.\n",
            " |          For example, if `value` is a string, and subset contains a non-string column,\n",
            " |          then the non-string column is simply ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with replaced null values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (10, 80.5, \"Alice\", None),\n",
            " |      ...     (5, None, \"Bob\", None),\n",
            " |      ...     (None, None, \"Tom\", None),\n",
            " |      ...     (None, None, None, True)],\n",
            " |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
            " |      \n",
            " |      Fill all null values with 50 for numeric columns.\n",
            " |      \n",
            " |      >>> df.na.fill(50).show()\n",
            " |      +---+------+-----+----+\n",
            " |      |age|height| name|bool|\n",
            " |      +---+------+-----+----+\n",
            " |      | 10|  80.5|Alice|NULL|\n",
            " |      |  5|  50.0|  Bob|NULL|\n",
            " |      | 50|  50.0|  Tom|NULL|\n",
            " |      | 50|  50.0| NULL|true|\n",
            " |      +---+------+-----+----+\n",
            " |      \n",
            " |      Fill all null values with ``False`` for boolean columns.\n",
            " |      \n",
            " |      >>> df.na.fill(False).show()\n",
            " |      +----+------+-----+-----+\n",
            " |      | age|height| name| bool|\n",
            " |      +----+------+-----+-----+\n",
            " |      |  10|  80.5|Alice|false|\n",
            " |      |   5|  NULL|  Bob|false|\n",
            " |      |NULL|  NULL|  Tom|false|\n",
            " |      |NULL|  NULL| NULL| true|\n",
            " |      +----+------+-----+-----+\n",
            " |      \n",
            " |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
            " |      \n",
            " |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
            " |      +---+------+-------+----+\n",
            " |      |age|height|   name|bool|\n",
            " |      +---+------+-------+----+\n",
            " |      | 10|  80.5|  Alice|NULL|\n",
            " |      |  5|  NULL|    Bob|NULL|\n",
            " |      | 50|  NULL|    Tom|NULL|\n",
            " |      | 50|  NULL|unknown|true|\n",
            " |      +---+------+-------+----+\n",
            " |  \n",
            " |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Filters rows using the given condition.\n",
            " |      \n",
            " |      :func:`where` is an alias for :func:`filter`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      condition : :class:`Column` or str\n",
            " |          a :class:`Column` of :class:`types.BooleanType`\n",
            " |          or a string of SQL expressions.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Filtered DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Filter by :class:`Column` instances.\n",
            " |      \n",
            " |      >>> df.filter(df.age > 3).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df.where(df.age == 2).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Filter by SQL expression in a string.\n",
            " |      \n",
            " |      >>> df.filter(\"age > 3\").show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df.where(\"age = 2\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  first(self) -> Optional[pyspark.sql.types.Row]\n",
            " |      Returns the first row as a :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Row`\n",
            " |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.first()\n",
            " |      Row(age=2, name='Alice')\n",
            " |  \n",
            " |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
            " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
            " |      \n",
            " |      This is a shorthand for ``df.rdd.foreach()``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |          A function that accepts one parameter which will\n",
            " |          receive each row to process.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> def func(person):\n",
            " |      ...     print(person.name)\n",
            " |      ...\n",
            " |      >>> df.foreach(func)\n",
            " |  \n",
            " |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
            " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
            " |      \n",
            " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |          A function that accepts one parameter which will receive\n",
            " |          each partition to process.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> def func(itr):\n",
            " |      ...     for person in itr:\n",
            " |      ...         print(person.name)\n",
            " |      ...\n",
            " |      >>> df.foreachPartition(func)\n",
            " |  \n",
            " |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
            " |      Finding frequent items for columns, possibly with false positives. Using the\n",
            " |      frequent element count algorithm described in\n",
            " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
            " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list or tuple\n",
            " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
            " |          strings.\n",
            " |      support : float, optional\n",
            " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
            " |          The support must be greater than 1e-4.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with frequent items.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
            " |      +------------+------------+\n",
            " |      |c1_freqItems|c2_freqItems|\n",
            " |      +------------+------------+\n",
            " |      |   [4, 1, 3]| [8, 11, 10]|\n",
            " |      +------------+------------+\n",
            " |  \n",
            " |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Groups the :class:`DataFrame` using the specified columns,\n",
            " |      so we can run aggregation on them. See :class:`GroupedData`\n",
            " |      for all the available aggregate functions.\n",
            " |      \n",
            " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          columns to group by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Grouped data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Empty grouping columns triggers a global aggregation.\n",
            " |      \n",
            " |      >>> df.groupBy().avg().show()\n",
            " |      +--------+\n",
            " |      |avg(age)|\n",
            " |      +--------+\n",
            " |      |    2.75|\n",
            " |      +--------+\n",
            " |      \n",
            " |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
            " |      \n",
            " |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
            " |      +-----+--------+\n",
            " |      | name|sum(age)|\n",
            " |      +-----+--------+\n",
            " |      |Alice|       2|\n",
            " |      |  Bob|       9|\n",
            " |      +-----+--------+\n",
            " |      \n",
            " |      Group-by 'name', and calculate maximum values.\n",
            " |      \n",
            " |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
            " |      +-----+--------+\n",
            " |      | name|max(age)|\n",
            " |      +-----+--------+\n",
            " |      |Alice|       2|\n",
            " |      |  Bob|       5|\n",
            " |      +-----+--------+\n",
            " |      \n",
            " |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
            " |      \n",
            " |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
            " |      +-----+---+-----+\n",
            " |      | name|age|count|\n",
            " |      +-----+---+-----+\n",
            " |      |Alice|  2|    1|\n",
            " |      |  Bob|  2|    2|\n",
            " |      |  Bob|  5|    1|\n",
            " |      +-----+---+-----+\n",
            " |  \n",
            " |  groupby = groupBy(self, *cols)\n",
            " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4\n",
            " |  \n",
            " |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
            " |      Returns the first ``n`` rows.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      n : int, optional\n",
            " |          default 1. Number of rows to return.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      If n is greater than 1, return a list of :class:`Row`.\n",
            " |      If n is 1, return a single Row.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.head()\n",
            " |      Row(age=2, name='Alice')\n",
            " |      >>> df.head(1)\n",
            " |      [Row(age=2, name='Alice')]\n",
            " |  \n",
            " |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
            " |      Specifies some hint on the current :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          A name of the hint.\n",
            " |      parameters : str, list, float or int\n",
            " |          Optional parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Hinted DataFrame\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      ... +- SortMergeJoin ...\n",
            " |      ...\n",
            " |      \n",
            " |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
            " |      \n",
            " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      ... +- BroadcastHashJoin ...\n",
            " |      ...\n",
            " |  \n",
            " |  inputFiles(self) -> List[str]\n",
            " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
            " |      This method simply asks each constituent BaseRelation for its respective files and\n",
            " |      takes the union of all results. Depending on the source relations, this may not find\n",
            " |      all input files. Duplicates are removed.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of file paths.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a single-row DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     df = spark.read.format(\"json\").load(d)\n",
            " |      ...\n",
            " |      ...     # Returns the number of input files.\n",
            " |      ...     len(df.inputFiles())\n",
            " |      1\n",
            " |  \n",
            " |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows only in\n",
            " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
            " |      Note that any duplicates are removed. To preserve duplicates\n",
            " |      use :func:`intersectAll`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Combined DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is equivalent to `INTERSECT` in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  b|  3|\n",
            " |      |  a|  1|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
            " |      and another :class:`DataFrame` while preserving duplicates.\n",
            " |      \n",
            " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
            " |      resolves columns by position (not by name).\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Combined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  a|  1|\n",
            " |      |  a|  1|\n",
            " |      |  b|  3|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  isEmpty(self) -> bool\n",
            " |      Checks if the :class:`DataFrame` is empty and returns a boolean value.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Returns ``True`` if the DataFrame is empty, ``False`` otherwise.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.count : Counts the number of rows in DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      - Unlike `count()`, this method does not trigger any computation.\n",
            " |      - An empty DataFrame has no rows. It may have columns, but no data.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Checking if an empty DataFrame is empty\n",
            " |      \n",
            " |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
            " |      >>> df_empty.isEmpty()\n",
            " |      True\n",
            " |      \n",
            " |      Example 2: Checking if a non-empty DataFrame is empty\n",
            " |      \n",
            " |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
            " |      >>> df_non_empty.isEmpty()\n",
            " |      False\n",
            " |      \n",
            " |      Example 3: Checking if a DataFrame with null values is empty\n",
            " |      \n",
            " |      >>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n",
            " |      >>> df_nulls.isEmpty()\n",
            " |      False\n",
            " |      \n",
            " |      Example 4: Checking if a DataFrame with no rows but with columns is empty\n",
            " |      \n",
            " |      >>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n",
            " |      >>> df_no_rows.isEmpty()\n",
            " |      True\n",
            " |  \n",
            " |  isLocal(self) -> bool\n",
            " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
            " |      (without any Spark executors).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.sql(\"SHOW TABLES\")\n",
            " |      >>> df.isLocal()\n",
            " |      True\n",
            " |  \n",
            " |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
            " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Right side of the join\n",
            " |      on : str, list or :class:`Column`, optional\n",
            " |          a string for the join column name, a list of column names,\n",
            " |          a join expression (Column), or a list of Columns.\n",
            " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
            " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
            " |      how : str, optional\n",
            " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
            " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
            " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
            " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Joined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
            " |      \n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import desc\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
            " |      >>> df4 = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      \n",
            " |      Inner join on columns (default)\n",
            " |      \n",
            " |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
            " |      +----+------+\n",
            " |      |name|height|\n",
            " |      +----+------+\n",
            " |      | Bob|    85|\n",
            " |      +----+------+\n",
            " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
            " |      +----+---+\n",
            " |      |name|age|\n",
            " |      +----+---+\n",
            " |      | Bob|  5|\n",
            " |      +----+---+\n",
            " |      \n",
            " |      Outer join for both DataFrames on the 'name' column.\n",
            " |      \n",
            " |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
            " |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
            " |      +-----+------+\n",
            " |      | name|height|\n",
            " |      +-----+------+\n",
            " |      |  Bob|    85|\n",
            " |      |Alice|  NULL|\n",
            " |      | NULL|    80|\n",
            " |      +-----+------+\n",
            " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
            " |      +-----+------+\n",
            " |      | name|height|\n",
            " |      +-----+------+\n",
            " |      |  Tom|    80|\n",
            " |      |  Bob|    85|\n",
            " |      |Alice|  NULL|\n",
            " |      +-----+------+\n",
            " |      \n",
            " |      Outer join for both DataFrams with multiple columns.\n",
            " |      \n",
            " |      >>> df.join(\n",
            " |      ...     df3,\n",
            " |      ...     [df.name == df3.name, df.age == df3.age],\n",
            " |      ...     'outer'\n",
            " |      ... ).select(df.name, df3.age).show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice|  2|\n",
            " |      |  Bob|  5|\n",
            " |      +-----+---+\n",
            " |  \n",
            " |  limit(self, num: int) -> 'DataFrame'\n",
            " |      Limits the result count to the number specified.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subset of the records\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.limit(1).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      | 14| Tom|\n",
            " |      +---+----+\n",
            " |      >>> df.limit(0).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      +---+----+\n",
            " |  \n",
            " |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
            " |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
            " |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
            " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eager : bool, optional, default True\n",
            " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Checkpointed DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.localCheckpoint(False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            " |      except for the aggregation, which cannot be reversed.\n",
            " |      \n",
            " |      :func:`melt` is an alias for :func:`unpivot`.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ids : str, Column, tuple, list, optional\n",
            " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            " |          or a list or tuple for multiple columns.\n",
            " |      values : str, Column, tuple, list, optional\n",
            " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            " |          for multiple columns. If not specified or empty, use all columns that\n",
            " |          are not set as `ids`.\n",
            " |      variableColumnName : str\n",
            " |          Name of the variable column.\n",
            " |      valueColumnName : str\n",
            " |          Name of the value column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpivoted DataFrame.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.unpivot\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
            " |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
            " |      DataFrame that returns the same result as the input, with the following guarantees:\n",
            " |      \n",
            " |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
            " |          the Dataset at that point.\n",
            " |      \n",
            " |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
            " |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
            " |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
            " |          the previous completion point.\n",
            " |      \n",
            " |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
            " |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
            " |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
            " |      function.\n",
            " |      \n",
            " |      A user can observe these metrics by adding\n",
            " |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
            " |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
            " |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      observation : :class:`Observation` or str\n",
            " |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
            " |      \n",
            " |          .. versionchanged:: 3.4.0\n",
            " |             Added support for `str` in this parameter.\n",
            " |      exprs : :class:`Column`\n",
            " |          column expressions (:class:`Column`).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          the observed :class:`DataFrame`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
            " |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
            " |      Continuous execution is currently not supported yet.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
            " |      \n",
            " |      >>> from pyspark.sql.functions import col, count, lit, max\n",
            " |      >>> from pyspark.sql import Observation\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> observation = Observation(\"my metrics\")\n",
            " |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
            " |      >>> observed_df.count()\n",
            " |      2\n",
            " |      >>> observation.get\n",
            " |      {'count': 2, 'max(age)': 5}\n",
            " |      \n",
            " |      When ``observation`` is a string, streaming queries also work as below.\n",
            " |      \n",
            " |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
            " |      >>> class MyErrorListener(StreamingQueryListener):\n",
            " |      ...    def onQueryStarted(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      ...    def onQueryProgress(self, event):\n",
            " |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
            " |      ...        # Trigger if the number of errors exceeds 5 percent\n",
            " |      ...        num_rows = row.rc\n",
            " |      ...        num_error_rows = row.erc\n",
            " |      ...        ratio = num_error_rows / num_rows\n",
            " |      ...        if ratio > 0.05:\n",
            " |      ...            # Trigger alert\n",
            " |      ...            pass\n",
            " |      ...\n",
            " |      ...    def onQueryIdle(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      ...    def onQueryTerminated(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      >>> spark.streams.addListener(MyErrorListener())\n",
            " |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
            " |      ... observed_ds = df.observe(\n",
            " |      ...     \"my_event\",\n",
            " |      ...     count(lit(1)).alias(\"rc\"),\n",
            " |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
            " |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
            " |  \n",
            " |  offset(self, num: int) -> 'DataFrame'\n",
            " |      Returns a new :class: `DataFrame` by skipping the first `n` rows.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports vanilla PySpark.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to skip.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subset of the records\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.offset(1).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |      >>> df.offset(10).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      +---+----+\n",
            " |  \n",
            " |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |  \n",
            " |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 3.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
            " |      to pandas-on-Spark, it will lose the index information and the original index\n",
            " |      will be turned into a normal column.\n",
            " |      \n",
            " |      This is only available if Pandas is installed and available.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      index_col: str or list of str, optional, default: None\n",
            " |          Index column of table in Spark.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`PandasOnSparkDataFrame`\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.pandas.frame.DataFrame.to_spark\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      >>> df.pandas_api()  # doctest: +SKIP\n",
            " |         age   name\n",
            " |      0   14    Tom\n",
            " |      1   23  Alice\n",
            " |      2   16    Bob\n",
            " |      \n",
            " |      We can specify the index columns.\n",
            " |      \n",
            " |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
            " |            name\n",
            " |      age\n",
            " |      14     Tom\n",
            " |      23   Alice\n",
            " |      16     Bob\n",
            " |  \n",
            " |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
            " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
            " |      operations after the first time it is computed. This can only be used to assign\n",
            " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
            " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      storageLevel : :class:`StorageLevel`\n",
            " |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Persisted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.persist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      \n",
            " |      >>> df.explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      AdaptiveSparkPlan isFinalPlan=false\n",
            " |      +- InMemoryTableScan ...\n",
            " |      \n",
            " |      Persists the data in the disk by specifying the storage level.\n",
            " |      \n",
            " |      >>> from pyspark.storagelevel import StorageLevel\n",
            " |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
            " |      DataFrame[id: bigint]\n",
            " |  \n",
            " |  printSchema(self, level: Optional[int] = None) -> None\n",
            " |      Prints out the schema in the tree format.\n",
            " |      Optionally allows to specify how many levels to print if schema is nested.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      level : int, optional, default None\n",
            " |          How many levels to print for nested schemas.\n",
            " |      \n",
            " |          .. versionchanged:: 3.5.0\n",
            " |              Added Level parameter.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.printSchema()\n",
            " |      root\n",
            " |       |-- age: long (nullable = true)\n",
            " |       |-- name: string (nullable = true)\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
            " |      >>> df.printSchema(1)\n",
            " |      root\n",
            " |       |-- a: long (nullable = true)\n",
            " |       |-- b: struct (nullable = true)\n",
            " |      \n",
            " |      >>> df.printSchema(2)\n",
            " |      root\n",
            " |       |-- a: long (nullable = true)\n",
            " |       |-- b: struct (nullable = true)\n",
            " |       |    |-- _1: long (nullable = true)\n",
            " |       |    |-- _2: long (nullable = true)\n",
            " |  \n",
            " |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
            " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      weights : list\n",
            " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
            " |          Weights will be normalized if they don't sum up to 1.0.\n",
            " |      seed : int, optional\n",
            " |          The seed for sampling.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of DataFrames.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      \n",
            " |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
            " |      >>> splits[0].count()\n",
            " |      2\n",
            " |      >>> splits[1].count()\n",
            " |      2\n",
            " |  \n",
            " |  registerTempTable(self, name: str) -> None\n",
            " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      .. deprecated:: 2.0.0\n",
            " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the temporary table to register.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.registerTempTable(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            " |      resulting :class:`DataFrame` is hash partitioned.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          can be an int to specify the target number of partitions or a Column.\n",
            " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            " |          the default number of partitions is used.\n",
            " |      cols : str or :class:`Column`\n",
            " |          partitioning columns.\n",
            " |      \n",
            " |          .. versionchanged:: 1.6.0\n",
            " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
            " |             optional if partitioning columns are specified.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Repartitioned DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Repartition the data into 10 partitions.\n",
            " |      \n",
            " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
            " |      10\n",
            " |      \n",
            " |      Repartition the data into 7 partitions by 'age' column.\n",
            " |      \n",
            " |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
            " |      7\n",
            " |      \n",
            " |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
            " |      \n",
            " |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
            " |      3\n",
            " |  \n",
            " |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            " |      resulting :class:`DataFrame` is range partitioned.\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          can be an int to specify the target number of partitions or a Column.\n",
            " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            " |          the default number of partitions is used.\n",
            " |      cols : str or :class:`Column`\n",
            " |          partitioning columns.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Repartitioned DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      At least one partition-by expression must be specified.\n",
            " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
            " |      \n",
            " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
            " |      Hence, the output may not be consistent, since sampling can return different values.\n",
            " |      The sample size can be controlled by the config\n",
            " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Repartition the data into 2 partitions by range in 'age' column.\n",
            " |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
            " |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
            " |      \n",
            " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
            " |      2\n",
            " |  \n",
            " |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
            " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
            " |      aliases of each other.\n",
            " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
            " |      or strings. Value can have None. When replacing, the new value will be cast\n",
            " |      to the type of the existing column.\n",
            " |      For numeric replacements all values to be replaced should have unique\n",
            " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
            " |      and arbitrary replacement will be used.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      to_replace : bool, int, float, string, list or dict\n",
            " |          Value to be replaced.\n",
            " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
            " |          must be a mapping between a value and a replacement.\n",
            " |      value : bool, int, float, string or None, optional\n",
            " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
            " |          list, `value` should be of the same length and type as `to_replace`.\n",
            " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
            " |          used as a replacement for each item in `to_replace`.\n",
            " |      subset : list, optional\n",
            " |          optional list of column names to consider.\n",
            " |          Columns specified in subset that do not have matching data types are ignored.\n",
            " |          For example, if `value` is a string, and subset contains a non-string column,\n",
            " |          then the non-string column is simply ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with replaced values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (10, 80, \"Alice\"),\n",
            " |      ...     (5, None, \"Bob\"),\n",
            " |      ...     (None, 10, \"Tom\"),\n",
            " |      ...     (None, None, None)],\n",
            " |      ...     schema=[\"age\", \"height\", \"name\"])\n",
            " |      \n",
            " |      Replace 10 to 20 in all columns.\n",
            " |      \n",
            " |      >>> df.na.replace(10, 20).show()\n",
            " |      +----+------+-----+\n",
            " |      | age|height| name|\n",
            " |      +----+------+-----+\n",
            " |      |  20|    80|Alice|\n",
            " |      |   5|  NULL|  Bob|\n",
            " |      |NULL|    20|  Tom|\n",
            " |      |NULL|  NULL| NULL|\n",
            " |      +----+------+-----+\n",
            " |      \n",
            " |      Replace 'Alice' to null in all columns.\n",
            " |      \n",
            " |      >>> df.na.replace('Alice', None).show()\n",
            " |      +----+------+----+\n",
            " |      | age|height|name|\n",
            " |      +----+------+----+\n",
            " |      |  10|    80|NULL|\n",
            " |      |   5|  NULL| Bob|\n",
            " |      |NULL|    10| Tom|\n",
            " |      |NULL|  NULL|NULL|\n",
            " |      +----+------+----+\n",
            " |      \n",
            " |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
            " |      \n",
            " |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
            " |      +----+------+----+\n",
            " |      | age|height|name|\n",
            " |      +----+------+----+\n",
            " |      |  10|    80|   A|\n",
            " |      |   5|  NULL|   B|\n",
            " |      |NULL|    10| Tom|\n",
            " |      |NULL|  NULL|NULL|\n",
            " |      +----+------+----+\n",
            " |  \n",
            " |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
            " |      the specified columns, so we can run aggregation on them.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          Columns to roll-up by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Rolled-up data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            " |      +-----+----+-----+\n",
            " |      | name| age|count|\n",
            " |      +-----+----+-----+\n",
            " |      | NULL|NULL|    2|\n",
            " |      |Alice|NULL|    1|\n",
            " |      |Alice|   2|    1|\n",
            " |      |  Bob|NULL|    1|\n",
            " |      |  Bob|   5|    1|\n",
            " |      +-----+----+-----+\n",
            " |  \n",
            " |  sameSemantics(self, other: 'DataFrame') -> bool\n",
            " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
            " |      therefore return the same results.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
            " |      such as attribute names.\n",
            " |      \n",
            " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
            " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
            " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
            " |      \n",
            " |      This API is a developer API.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          The other DataFrame to compare against.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Whether these two DataFrames are similar.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.range(10)\n",
            " |      >>> df2 = spark.range(10)\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
            " |      True\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
            " |      False\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
            " |      True\n",
            " |  \n",
            " |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
            " |      Returns a sampled subset of this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      withReplacement : bool, optional\n",
            " |          Sample with replacement or not (default ``False``).\n",
            " |      fraction : float, optional\n",
            " |          Fraction of rows to generate, range [0.0, 1.0].\n",
            " |      seed : int, optional\n",
            " |          Seed for sampling (default a random seed).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Sampled rows from given DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
            " |      count of the given :class:`DataFrame`.\n",
            " |      \n",
            " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.sample(0.5, 3).count() # doctest: +SKIP\n",
            " |      7\n",
            " |      >>> df.sample(fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            " |      7\n",
            " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            " |      1\n",
            " |      >>> df.sample(1.0).count()\n",
            " |      10\n",
            " |      >>> df.sample(fraction=1.0).count()\n",
            " |      10\n",
            " |      >>> df.sample(False, fraction=1.0).count()\n",
            " |      10\n",
            " |  \n",
            " |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
            " |      Returns a stratified sample without replacement based on the\n",
            " |      fraction given on each stratum.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col : :class:`Column` or str\n",
            " |          column that defines strata\n",
            " |      \n",
            " |          .. versionchanged:: 3.0.0\n",
            " |             Added sampling by a column of :class:`Column`\n",
            " |      fractions : dict\n",
            " |          sampling fraction for each stratum. If a stratum is not\n",
            " |          specified, we treat its fraction as zero.\n",
            " |      seed : int, optional\n",
            " |          random seed\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      a new :class:`DataFrame` that represents the stratified sample\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col\n",
            " |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
            " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
            " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
            " |      +---+-----+\n",
            " |      |key|count|\n",
            " |      +---+-----+\n",
            " |      |  0|    3|\n",
            " |      |  1|    6|\n",
            " |      +---+-----+\n",
            " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
            " |      33\n",
            " |  \n",
            " |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, :class:`Column`, or list\n",
            " |          column names (string) or expressions (:class:`Column`).\n",
            " |          If one of the column names is '*', that column is expanded to include all columns\n",
            " |          in the current :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A DataFrame with subset (or all) of columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Select all columns in the DataFrame.\n",
            " |      \n",
            " |      >>> df.select('*').show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      |  5|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Select a column with other expressions in the DataFrame.\n",
            " |      \n",
            " |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice| 12|\n",
            " |      |  Bob| 15|\n",
            " |      +-----+---+\n",
            " |  \n",
            " |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
            " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
            " |      \n",
            " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A DataFrame with new/old columns transformed by expressions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
            " |      +---------+--------+\n",
            " |      |(age * 2)|abs(age)|\n",
            " |      +---------+--------+\n",
            " |      |        4|       2|\n",
            " |      |       10|       5|\n",
            " |      +---------+--------+\n",
            " |  \n",
            " |  semanticHash(self) -> int\n",
            " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
            " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
            " |      \n",
            " |      This API is a developer API.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Hash value.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
            " |      1855039936\n",
            " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
            " |      1855039936\n",
            " |  \n",
            " |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
            " |      Prints the first ``n`` rows to the console.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      n : int, optional\n",
            " |          Number of rows to show.\n",
            " |      truncate : bool or int, optional\n",
            " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
            " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
            " |          and align cells right.\n",
            " |      vertical : bool, optional\n",
            " |          If set to ``True``, print output rows vertically (one line\n",
            " |          per column value).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Show only top 2 rows.\n",
            " |      \n",
            " |      >>> df.show(2)\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      +---+-----+\n",
            " |      only showing top 2 rows\n",
            " |      \n",
            " |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
            " |      \n",
            " |      >>> df.show(truncate=3)\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      | 14| Tom|\n",
            " |      | 23| Ali|\n",
            " |      | 16| Bob|\n",
            " |      +---+----+\n",
            " |      \n",
            " |      Show :class:`DataFrame` vertically.\n",
            " |      \n",
            " |      >>> df.show(vertical=True)\n",
            " |      -RECORD 0-----\n",
            " |      age  | 14\n",
            " |      name | Tom\n",
            " |      -RECORD 1-----\n",
            " |      age  | 23\n",
            " |      name | Alice\n",
            " |      -RECORD 2-----\n",
            " |      age  | 16\n",
            " |      name | Bob\n",
            " |  \n",
            " |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list, or :class:`Column`, optional\n",
            " |           list of :class:`Column` or column names to sort by.\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      ascending : bool or list, optional, default True\n",
            " |          boolean or list of boolean.\n",
            " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Sorted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import desc, asc\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Sort the DataFrame in ascending order.\n",
            " |      \n",
            " |      >>> df.sort(asc(\"age\")).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      |  5|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Sort the DataFrame in descending order.\n",
            " |      \n",
            " |      >>> df.sort(df.age.desc()).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      >>> df.orderBy(df.age.desc()).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      >>> df.sort(\"age\", ascending=False).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Specify multiple columns\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      |  2|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Specify multiple columns for sorting order at `ascending`.\n",
            " |      \n",
            " |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list or :class:`Column`, optional\n",
            " |          list of :class:`Column` or column names to sort by.\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      ascending : bool or list, optional, default True\n",
            " |          boolean or list of boolean.\n",
            " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame sorted by partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
            " |      but not in another :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be subtracted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subtracted DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.subtract(df2).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  c|  4|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  summary(self, *statistics: str) -> 'DataFrame'\n",
            " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
            " |      - count\n",
            " |      - mean\n",
            " |      - stddev\n",
            " |      - min\n",
            " |      - max\n",
            " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
            " |      \n",
            " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
            " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      statistics : str, optional\n",
            " |           Column names to calculate statistics by (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new DataFrame that provides statistics for the given DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            " |      ... )\n",
            " |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |summary| age|            weight|           height|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |  count|   3|                 3|                3|\n",
            " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            " |      |    min|  11|              37.8|            142.2|\n",
            " |      |    25%|  11|              37.8|            142.2|\n",
            " |      |    50%|  12|              40.3|            142.3|\n",
            " |      |    75%|  13|              44.1|            150.5|\n",
            " |      |    max|  13|              44.1|            150.5|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      \n",
            " |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
            " |      +-------+---+------+------+\n",
            " |      |summary|age|weight|height|\n",
            " |      +-------+---+------+------+\n",
            " |      |  count|  3|     3|     3|\n",
            " |      |    min| 11|  37.8| 142.2|\n",
            " |      |    25%| 11|  37.8| 142.2|\n",
            " |      |    75%| 13|  44.1| 150.5|\n",
            " |      |    max| 13|  44.1| 150.5|\n",
            " |      +-------+---+------+------+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.display\n",
            " |  \n",
            " |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
            " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
            " |      \n",
            " |      Running tail requires moving data into the application's driver process, and doing so with\n",
            " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      >>> df.tail(2)\n",
            " |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
            " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records..\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the first 2 rows of the :class:`DataFrame`.\n",
            " |      \n",
            " |      >>> df.take(2)\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
            " |  \n",
            " |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
            " |      schema.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      schema : :class:`StructType`\n",
            " |          Specified schema.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Reconciled DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
            " |      \n",
            " |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
            " |          Missing columns and/or inner fields (present in the specified schema but not input\n",
            " |          DataFrame) lead to failures.\n",
            " |      \n",
            " |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
            " |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
            " |          not string to int.\n",
            " |      \n",
            " |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
            " |          still keep their own metadata if not overwritten by the specified schema.\n",
            " |      \n",
            " |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
            " |          is nullable but the specified schema requires them to be not nullable.\n",
            " |      \n",
            " |      Supports Spark Connect.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StructField, StringType\n",
            " |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
            " |      >>> df.schema\n",
            " |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
            " |      \n",
            " |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
            " |      >>> df2 = df.to(schema)\n",
            " |      >>> df2.schema\n",
            " |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
            " |      >>> df2.show()\n",
            " |      +---+---+\n",
            " |      |  j|  i|\n",
            " |      +---+---+\n",
            " |      |  1|  a|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  toDF(self, *cols: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` that with new specified column names\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      *cols : tuple\n",
            " |          a tuple of string new column name. The length of the\n",
            " |          list needs to be the same as the number of columns in the initial\n",
            " |          :class:`DataFrame`\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new column names.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
            " |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.toDF('f1', 'f2').show()\n",
            " |      +---+-----+\n",
            " |      | f1|   f2|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
            " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
            " |      \n",
            " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      use_unicode : bool, optional, default True\n",
            " |          Whether to convert to unicode or not.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.toJSON().first()\n",
            " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
            " |  \n",
            " |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
            " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
            " |      The iterator will consume as much memory as the largest partition in this\n",
            " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
            " |      partitions.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      prefetchPartitions : bool, optional\n",
            " |          If Spark should pre-fetch the next partition before it is needed.\n",
            " |      \n",
            " |          .. versionchanged:: 3.4.0\n",
            " |              This argument does not take effect for Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      Iterator\n",
            " |          Iterator of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> list(df.toLocalIterator())\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      # Keep to_koalas for backward compatibility for now.\n",
            " |  \n",
            " |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
            " |  \n",
            " |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a function that takes and returns a :class:`DataFrame`.\n",
            " |      *args\n",
            " |          Positional arguments to pass to func.\n",
            " |      \n",
            " |          .. versionadded:: 3.3.0\n",
            " |      **kwargs\n",
            " |          Keyword arguments to pass to func.\n",
            " |      \n",
            " |          .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Transformed DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col\n",
            " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
            " |      >>> def cast_all_to_int(input_df):\n",
            " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
            " |      ...\n",
            " |      >>> def sort_columns_asc(input_df):\n",
            " |      ...     return input_df.select(*sorted(input_df.columns))\n",
            " |      ...\n",
            " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
            " |      +-----+---+\n",
            " |      |float|int|\n",
            " |      +-----+---+\n",
            " |      |    1|  1|\n",
            " |      |    2|  2|\n",
            " |      +-----+---+\n",
            " |      \n",
            " |      >>> def add_n(input_df, n):\n",
            " |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
            " |      ...                             for col_name in input_df.columns])\n",
            " |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
            " |      +---+-----+\n",
            " |      |int|float|\n",
            " |      +---+-----+\n",
            " |      | 12| 12.0|\n",
            " |      | 13| 13.0|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be unioned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing the combined rows with corresponding columns.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.unionAll\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method performs a SQL-style set union of the rows from both `DataFrame` objects,\n",
            " |      with no automatic deduplication of elements.\n",
            " |      \n",
            " |      Use the `distinct()` method to perform deduplication of rows.\n",
            " |      \n",
            " |      The method resolves columns by position (not by name), following the standard behavior\n",
            " |      in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Combining two DataFrames with the same schema\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            " |      >>> df3 = df1.union(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+\n",
            " |      | id|value|\n",
            " |      +---+-----+\n",
            " |      |  1|    A|\n",
            " |      |  2|    B|\n",
            " |      |  3|    C|\n",
            " |      |  4|    D|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Example 2: Combining two DataFrames with different schemas\n",
            " |      \n",
            " |      >>> from pyspark.sql.functions import lit\n",
            " |      >>> df1 = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
            " |      >>> df2 = spark.createDataFrame([(3, \"Charlie\"), (4, \"Dave\")], [\"id\", \"name\"])\n",
            " |      >>> df1 = df1.withColumn(\"age\", lit(30))\n",
            " |      >>> df2 = df2.withColumn(\"age\", lit(40))\n",
            " |      >>> df3 = df1.union(df2)  # doctest: +SKIP\n",
            " |      >>> df3.show()  # doctest: +SKIP\n",
            " |      +-----+-------+---+\n",
            " |      | name|     id|age|\n",
            " |      +-----+-------+---+\n",
            " |      |Alice|      1| 30|\n",
            " |      |  Bob|      2| 30|\n",
            " |      |    3|Charlie| 40|\n",
            " |      |    4|   Dave| 40|\n",
            " |      +-----+-------+---+\n",
            " |      \n",
            " |      Example 3: Combining two DataFrames with mismatched columns\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n",
            " |      >>> df3 = df1.union(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+---+\n",
            " |      |  A|  B|\n",
            " |      +---+---+\n",
            " |      |  1|  2|\n",
            " |      |  3|  4|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Example 4: Combining duplicate rows from two different DataFrames\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            " |      >>> df3 = df1.union(df2).distinct().sort(\"id\")\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+\n",
            " |      | id|value|\n",
            " |      +---+-----+\n",
            " |      |  1|    A|\n",
            " |      |  2|    B|\n",
            " |      |  3|    C|\n",
            " |      |  4|    D|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing combined rows from both dataframes.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method combines all rows from both `DataFrame` objects with no automatic\n",
            " |      deduplication of elements.\n",
            " |      \n",
            " |      Use the `distinct()` method to perform deduplication of rows.\n",
            " |      \n",
            " |      :func:`unionAll` is an alias to :func:`union`\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.union\n",
            " |  \n",
            " |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      This method performs a union operation on both input DataFrames, resolving columns by\n",
            " |      name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
            " |      be filled with null.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      allowMissingColumns : bool, optional, default False\n",
            " |         Specify whether to allow missing columns.\n",
            " |      \n",
            " |         .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing the combined rows with corresponding\n",
            " |          columns of the two given DataFrames.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Union of two DataFrames with same columns in different order.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
            " |      >>> df1.unionByName(df2).show()\n",
            " |      +----+----+----+\n",
            " |      |col0|col1|col2|\n",
            " |      +----+----+----+\n",
            " |      |   1|   2|   3|\n",
            " |      |   6|   4|   5|\n",
            " |      +----+----+----+\n",
            " |      \n",
            " |      Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+\n",
            " |      |col0|col1|col2|col3|\n",
            " |      +----+----+----+----+\n",
            " |      |   1|   2|   3|NULL|\n",
            " |      |NULL|   4|   5|   6|\n",
            " |      +----+----+----+----+\n",
            " |      \n",
            " |      Example 3: Union of two DataFrames with few common columns.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+----+\n",
            " |      |col0|col1|col2|col3|col4|\n",
            " |      +----+----+----+----+----+\n",
            " |      |   1|   2|   3|NULL|NULL|\n",
            " |      |NULL|   4|   5|   6|   7|\n",
            " |      +----+----+----+----+----+\n",
            " |      \n",
            " |      Example 4: Union of two DataFrames with completely different columns.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+----+----+\n",
            " |      |col0|col1|col2|col3|col4|col5|\n",
            " |      +----+----+----+----+----+----+\n",
            " |      |   0|   1|   2|NULL|NULL|NULL|\n",
            " |      |NULL|NULL|NULL|   3|   4|   5|\n",
            " |      +----+----+----+----+----+----+\n",
            " |  \n",
            " |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
            " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
            " |      memory and disk.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      blocking : bool\n",
            " |          Whether to block until all blocks are deleted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpersisted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.persist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      >>> df.unpersist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.unpersist(True)\n",
            " |      DataFrame[id: bigint]\n",
            " |  \n",
            " |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            " |      except for the aggregation, which cannot be reversed.\n",
            " |      \n",
            " |      This function is useful to massage a DataFrame into a format where some\n",
            " |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
            " |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
            " |      by `variableColumnName` and `valueColumnName`.\n",
            " |      \n",
            " |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
            " |      \"variable\" and \"value\" columns.\n",
            " |      \n",
            " |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
            " |      When `values` is `None`, all non-id columns will be unpivoted.\n",
            " |      \n",
            " |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
            " |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
            " |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
            " |      do not have a common data type and `unpivot` fails.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ids : str, Column, tuple, list\n",
            " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            " |          or a list or tuple for multiple columns.\n",
            " |      values : str, Column, tuple, list, optional\n",
            " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            " |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
            " |          columns that are not set as `ids`.\n",
            " |      variableColumnName : str\n",
            " |          Name of the variable column.\n",
            " |      valueColumnName : str\n",
            " |          Name of the value column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpivoted DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Supports Spark Connect.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
            " |      ...     [\"id\", \"int\", \"double\"],\n",
            " |      ... )\n",
            " |      >>> df.show()\n",
            " |      +---+---+------+\n",
            " |      | id|int|double|\n",
            " |      +---+---+------+\n",
            " |      |  1| 11|   1.1|\n",
            " |      |  2| 12|   1.2|\n",
            " |      +---+---+------+\n",
            " |      \n",
            " |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
            " |      +---+------+----+\n",
            " |      | id|   var| val|\n",
            " |      +---+------+----+\n",
            " |      |  1|   int|11.0|\n",
            " |      |  1|double| 1.1|\n",
            " |      |  2|   int|12.0|\n",
            " |      |  2|double| 1.2|\n",
            " |      +---+------+----+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.melt\n",
            " |  \n",
            " |  where = filter(self, condition)\n",
            " |      :func:`where` is an alias for :func:`filter`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |  \n",
            " |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
            " |      existing column that has the same name.\n",
            " |      \n",
            " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
            " |      a column from some other :class:`DataFrame` will raise an error.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colName : str\n",
            " |          string, name of the new column.\n",
            " |      col : :class:`Column`\n",
            " |          a :class:`Column` expression for the new column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new or replaced column.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method introduces a projection internally. Therefore, calling it multiple\n",
            " |      times, for instance, via loops in order to add multiple columns can generate big\n",
            " |      plans which can cause performance issues and even `StackOverflowException`.\n",
            " |      To avoid this, use :func:`select` with multiple columns at once.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumn('age2', df.age + 2).show()\n",
            " |      +---+-----+----+\n",
            " |      |age| name|age2|\n",
            " |      +---+-----+----+\n",
            " |      |  2|Alice|   4|\n",
            " |      |  5|  Bob|   7|\n",
            " |      +---+-----+----+\n",
            " |  \n",
            " |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
            " |      This is a no-op if the schema doesn't contain the given column name.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      existing : str\n",
            " |          string, name of the existing column to rename.\n",
            " |      new : str\n",
            " |          string, new name of the column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with renamed column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumnRenamed('age', 'age2').show()\n",
            " |      +----+-----+\n",
            " |      |age2| name|\n",
            " |      +----+-----+\n",
            " |      |   2|Alice|\n",
            " |      |   5|  Bob|\n",
            " |      +----+-----+\n",
            " |  \n",
            " |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
            " |      existing columns that have the same names.\n",
            " |      \n",
            " |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
            " |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |         Added support for multiple columns adding\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colsMap : dict\n",
            " |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new or replaced columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
            " |      +---+-----+----+----+\n",
            " |      |age| name|age2|age3|\n",
            " |      +---+-----+----+----+\n",
            " |      |  2|Alice|   4|   5|\n",
            " |      |  5|  Bob|   7|   8|\n",
            " |      +---+-----+----+----+\n",
            " |  \n",
            " |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
            " |      This is a no-op if the schema doesn't contain the given column names.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |         Added support for multiple columns renaming\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colsMap : dict\n",
            " |          a dict of existing column names and corresponding desired column names.\n",
            " |          Currently, only a single map is supported.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with renamed columns.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`withColumnRenamed`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Support Spark Connect\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
            " |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
            " |      +---+-----+----+----+\n",
            " |      |age| name|age4|age5|\n",
            " |      +---+-----+----+----+\n",
            " |      |  2|Alice|   4|   5|\n",
            " |      |  5|  Bob|   7|   8|\n",
            " |      +---+-----+----+----+\n",
            " |  \n",
            " |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      columnName : str\n",
            " |          string, name of the existing column to update the metadata.\n",
            " |      metadata : dict\n",
            " |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with updated metadata column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
            " |      >>> df_meta.schema['age'].metadata\n",
            " |      {'foo': 'bar'}\n",
            " |  \n",
            " |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
            " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
            " |      in time before which we assume no more late data is going to arrive.\n",
            " |      \n",
            " |      Spark will use this watermark for several purposes:\n",
            " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
            " |          when using output modes that do not allow updates.\n",
            " |      \n",
            " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
            " |      \n",
            " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
            " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
            " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
            " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
            " |      process records that arrive more than `delayThreshold` late.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eventTime : str\n",
            " |          the name of the column that contains the event time of the row.\n",
            " |      delayThreshold : str\n",
            " |          the minimum delay to wait to data to arrive late, relative to the\n",
            " |          latest record that has been processed in the form of an interval\n",
            " |          (e.g. \"1 minute\" or \"5 hours\").\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Watermarked DataFrame\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is a feature only for Structured Streaming.\n",
            " |      \n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
            " |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            " |      ...     \"value % 5 AS value\", \"timestamp\")\n",
            " |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            " |      DataFrame[value: bigint, time: timestamp]\n",
            " |      \n",
            " |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
            " |      \n",
            " |      >>> import time\n",
            " |      >>> from pyspark.sql.functions import window\n",
            " |      >>> query = (df\n",
            " |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
            " |      ...     .groupBy(\n",
            " |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
            " |      ...         df.value)\n",
            " |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
            " |      >>> time.sleep(3)\n",
            " |      >>> query.stop()\n",
            " |  \n",
            " |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
            " |      Create a write configuration builder for v2 sources.\n",
            " |      \n",
            " |      This builder is used to configure and execute write operations.\n",
            " |      \n",
            " |      For example, to append or create or replace existing tables.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      table : str\n",
            " |          Target table name to write to.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameWriterV2`\n",
            " |          DataFrameWriterV2 to use further to specify how to save the data\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
            " |      >>> df.writeTo(                              # doctest: +SKIP\n",
            " |      ...     \"catalog.db.table\"\n",
            " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  columns\n",
            " |      Retrieves the names of all columns in the :class:`DataFrame` as a list.\n",
            " |      \n",
            " |      The order of the column names in the list reflects their order in the DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of column names in the DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Retrieve column names of a DataFrame\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
            " |      ...     [\"age\", \"name\", \"state\"]\n",
            " |      ... )\n",
            " |      >>> df.columns\n",
            " |      ['age', 'name', 'state']\n",
            " |      \n",
            " |      Example 2: Using column names to project specific columns\n",
            " |      \n",
            " |      >>> selected_cols = [col for col in df.columns if col != \"age\"]\n",
            " |      >>> df.select(selected_cols).show()\n",
            " |      +-----+-----+\n",
            " |      | name|state|\n",
            " |      +-----+-----+\n",
            " |      |  Tom|   CA|\n",
            " |      |Alice|   NY|\n",
            " |      |  Bob|   TX|\n",
            " |      +-----+-----+\n",
            " |      \n",
            " |      Example 3: Checking if a specific column exists in a DataFrame\n",
            " |      \n",
            " |      >>> \"state\" in df.columns\n",
            " |      True\n",
            " |      >>> \"salary\" in df.columns\n",
            " |      False\n",
            " |      \n",
            " |      Example 4: Iterating over columns to apply a transformation\n",
            " |      \n",
            " |      >>> import pyspark.sql.functions as f\n",
            " |      >>> for col_name in df.columns:\n",
            " |      ...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n",
            " |      >>> df.show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|state|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  TOM|   CA|\n",
            " |      | 23|ALICE|   NY|\n",
            " |      | 16|  BOB|   TX|\n",
            " |      +---+-----+-----+\n",
            " |      \n",
            " |      Example 5: Renaming columns and checking the updated column names\n",
            " |      \n",
            " |      >>> df = df.withColumnRenamed(\"name\", \"first_name\")\n",
            " |      >>> df.columns\n",
            " |      ['age', 'first_name', 'state']\n",
            " |      \n",
            " |      Example 6: Using the `columns` property to ensure two DataFrames have the\n",
            " |      same columns before a union\n",
            " |      \n",
            " |      >>> df2 = spark.createDataFrame(\n",
            " |      ...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n",
            " |      >>> df.columns == df2.columns\n",
            " |      False\n",
            " |  \n",
            " |  dtypes\n",
            " |      Returns all column names and their data types as a list.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of columns as tuple pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.dtypes\n",
            " |      [('age', 'bigint'), ('name', 'string')]\n",
            " |  \n",
            " |  isStreaming\n",
            " |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
            " |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
            " |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
            " |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
            " |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
            " |      is a streaming source present.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Whether it's streaming DataFrame or not.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.readStream.format(\"rate\").load()\n",
            " |      >>> df.isStreaming\n",
            " |      True\n",
            " |  \n",
            " |  na\n",
            " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameNaFunctions`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
            " |      >>> type(df.na)\n",
            " |      <class '...dataframe.DataFrameNaFunctions'>\n",
            " |      \n",
            " |      Replace the missing values as 2.\n",
            " |      \n",
            " |      >>> df.na.fill(2).show()\n",
            " |      +---+---+\n",
            " |      | c1| c2|\n",
            " |      +---+---+\n",
            " |      |  1|  2|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  rdd\n",
            " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> type(df.rdd)\n",
            " |      <class 'pyspark.rdd.RDD'>\n",
            " |  \n",
            " |  schema\n",
            " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve the schema of the current DataFrame.\n",
            " |      \n",
            " |      >>> df.schema\n",
            " |      StructType([StructField('age', LongType(), True),\n",
            " |                  StructField('name', StringType(), True)])\n",
            " |  \n",
            " |  sparkSession\n",
            " |      Returns Spark session that created this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`SparkSession`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> type(df.sparkSession)\n",
            " |      <class '...session.SparkSession'>\n",
            " |  \n",
            " |  sql_ctx\n",
            " |  \n",
            " |  stat\n",
            " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameStatFunctions`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import pyspark.sql.functions as f\n",
            " |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
            " |      >>> type(df.stat)\n",
            " |      <class '...dataframe.DataFrameStatFunctions'>\n",
            " |      >>> df.stat.corr(\"id\", \"c\")\n",
            " |      1.0\n",
            " |  \n",
            " |  storageLevel\n",
            " |      Get the :class:`DataFrame`'s current storage level.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StorageLevel`\n",
            " |          Currently defined storage level.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.range(10)\n",
            " |      >>> df1.storageLevel\n",
            " |      StorageLevel(False, False, False, False, 1)\n",
            " |      >>> df1.cache().storageLevel\n",
            " |      StorageLevel(True, True, False, True, 1)\n",
            " |      \n",
            " |      >>> df2 = spark.range(5)\n",
            " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
            " |      StorageLevel(True, False, False, False, 2)\n",
            " |  \n",
            " |  write\n",
            " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
            " |      storage.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameWriter`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> type(df.write)\n",
            " |      <class '...readwriter.DataFrameWriter'>\n",
            " |      \n",
            " |      Write the DataFrame as a table.\n",
            " |      \n",
            " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
            " |      >>> df.write.saveAsTable(\"tab2\")\n",
            " |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
            " |  \n",
            " |  writeStream\n",
            " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
            " |      storage.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataStreamWriter`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import time\n",
            " |      >>> import tempfile\n",
            " |      >>> df = spark.readStream.format(\"rate\").load()\n",
            " |      >>> type(df.writeStream)\n",
            " |      <class '...streaming.readwriter.DataStreamWriter'>\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Create a table with Rate source.\n",
            " |      ...     query = df.writeStream.toTable(\n",
            " |      ...         \"my_table\", checkpointLocation=d)\n",
            " |      ...     time.sleep(3)\n",
            " |      ...     query.stop()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            " |  \n",
            " |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            " |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
            " |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
            " |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
            " |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
            " |      Each `pyarrow.RecordBatch` size can be controlled by\n",
            " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            " |      output can be different.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
            " |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
            " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            " |          the return type of the `func` in PySpark. The value can be either a\n",
            " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            " |      barrier : bool, optional, default False\n",
            " |          Use barrier mode execution.\n",
            " |      \n",
            " |          .. versionadded: 3.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import pyarrow  # doctest: +SKIP\n",
            " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            " |      >>> def filter_func(iterator):\n",
            " |      ...     for batch in iterator:\n",
            " |      ...         pdf = batch.to_pandas()\n",
            " |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
            " |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Set ``barrier`` to ``True`` to force the ``mapInArrow`` stage running in the\n",
            " |      barrier mode, it ensures all Python workers in the stage will be\n",
            " |      launched concurrently.\n",
            " |      \n",
            " |      >>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is unstable, and for developers.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.sql.functions.pandas_udf\n",
            " |      pyspark.sql.DataFrame.mapInPandas\n",
            " |  \n",
            " |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
            " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
            " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
            " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
            " |      Each `pandas.DataFrame` size can be controlled by\n",
            " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            " |      output can be different.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
            " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
            " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            " |          the return type of the `func` in PySpark. The value can be either a\n",
            " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            " |      barrier : bool, optional, default False\n",
            " |          Use barrier mode execution.\n",
            " |      \n",
            " |          .. versionadded: 3.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import pandas_udf\n",
            " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            " |      >>> def filter_func(iterator):\n",
            " |      ...     for pdf in iterator:\n",
            " |      ...         yield pdf[pdf.id == 1]\n",
            " |      ...\n",
            " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Set ``barrier`` to ``True`` to force the ``mapInPandas`` stage running in the\n",
            " |      barrier mode, it ensures all Python workers in the stage will be\n",
            " |      launched concurrently.\n",
            " |      \n",
            " |      >>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.sql.functions.pandas_udf\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
            " |  \n",
            " |  toPandas(self) -> 'PandasDataFrameLike'\n",
            " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
            " |      \n",
            " |      This is only available if Pandas is installed and available.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
            " |      expected to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df.toPandas()  # doctest: +SKIP\n",
            " |         age   name\n",
            " |      0    2  Alice\n",
            " |      1    5    Bob\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(df.dropDuplicates())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD2vyR3N7OXV",
        "outputId": "0818623f-be54-4999-e60e-6e84444cb509"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6eLqgRF7qXT"
      },
      "source": [
        "# `orderBy()` and `sort()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvN8l84_7pR6",
        "outputId": "caeaedc7-6e6e-4362-b307-7c31ab3e1c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dd= sns.load_dataset('iris')\n",
        "df= spark.createDataFrame(dd)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYFcU_bH7YEf",
        "outputId": "52788fb0-728f-48ca-a3ae-2a88f3afad27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort('species').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erM6oyzg8VQF",
        "outputId": "0931c064-11e5-4fce-9669-8e56e9a5abfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "|         5.0|        2.0|         3.5|        1.0|versicolor|\n",
            "|         6.0|        2.2|         5.0|        1.5| virginica|\n",
            "|         6.0|        2.2|         4.0|        1.0|versicolor|\n",
            "|         6.2|        2.2|         4.5|        1.5|versicolor|\n",
            "|         6.3|        2.3|         4.4|        1.3|versicolor|\n",
            "|         5.5|        2.3|         4.0|        1.3|versicolor|\n",
            "|         5.0|        2.3|         3.3|        1.0|versicolor|\n",
            "|         4.5|        2.3|         1.3|        0.3|    setosa|\n",
            "|         5.5|        2.4|         3.8|        1.1|versicolor|\n",
            "|         4.9|        2.4|         3.3|        1.0|versicolor|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort('sepal_width').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssUTnrM_8b_C",
        "outputId": "a1c81cd7-85cf-406e-deb1-fb09d4df8940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "|         5.0|        2.0|         3.5|        1.0|versicolor|\n",
            "|         6.0|        2.2|         5.0|        1.5| virginica|\n",
            "|         6.0|        2.2|         4.0|        1.0|versicolor|\n",
            "|         6.2|        2.2|         4.5|        1.5|versicolor|\n",
            "|         6.3|        2.3|         4.4|        1.3|versicolor|\n",
            "|         5.5|        2.3|         4.0|        1.3|versicolor|\n",
            "|         5.0|        2.3|         3.3|        1.0|versicolor|\n",
            "|         4.5|        2.3|         1.3|        0.3|    setosa|\n",
            "|         5.5|        2.4|         3.8|        1.1|versicolor|\n",
            "|         4.9|        2.4|         3.3|        1.0|versicolor|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.orderBy(df['sepal_width']).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bImpdrbU8qJp",
        "outputId": "7d756c6b-2ef5-44a9-949a-9a6b3ca71c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "|         5.0|        2.0|         3.5|        1.0|versicolor|\n",
            "|         6.0|        2.2|         4.0|        1.0|versicolor|\n",
            "|         6.2|        2.2|         4.5|        1.5|versicolor|\n",
            "|         6.0|        2.2|         5.0|        1.5| virginica|\n",
            "|         4.5|        2.3|         1.3|        0.3|    setosa|\n",
            "|         5.0|        2.3|         3.3|        1.0|versicolor|\n",
            "|         5.5|        2.3|         4.0|        1.3|versicolor|\n",
            "|         6.3|        2.3|         4.4|        1.3|versicolor|\n",
            "|         4.9|        2.4|         3.3|        1.0|versicolor|\n",
            "|         5.5|        2.4|         3.7|        1.0|versicolor|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.orderBy(df['sepal_width'], df['petal_length']).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWnA5iYr8ypg",
        "outputId": "11ac71f9-fe34-4ea5-af9f-f9e9dbba090a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+---------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|  species|\n",
            "+------------+-----------+------------+-----------+---------+\n",
            "|         5.7|        4.4|         1.5|        0.4|   setosa|\n",
            "|         5.5|        4.2|         1.4|        0.2|   setosa|\n",
            "|         5.2|        4.1|         1.5|        0.1|   setosa|\n",
            "|         5.8|        4.0|         1.2|        0.2|   setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|   setosa|\n",
            "|         5.4|        3.9|         1.3|        0.4|   setosa|\n",
            "|         7.7|        3.8|         6.7|        2.2|virginica|\n",
            "|         7.9|        3.8|         6.4|        2.0|virginica|\n",
            "|         5.1|        3.8|         1.9|        0.4|   setosa|\n",
            "|         5.7|        3.8|         1.7|        0.3|   setosa|\n",
            "+------------+-----------+------------+-----------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.orderBy(df['sepal_width'].desc(), df['petal_length'].desc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGbR6Uh99KUr"
      },
      "source": [
        "# `union()` and `unionAll()`\n",
        "* `union()` and `unionAll()` transformations are used to merge two or more DataFrame's of the same schema or structure.\n",
        "* `union()` and `unionAll()` method merges two DataFrames regardless of duplicate data.\n",
        "* To remove duplicates use `distinct()` function.\n",
        "* Similar as Union All in SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxyNaPk18-B3",
        "outputId": "96221754-e320-4f40-c967-1e713ee110a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dd= sns.load_dataset('iris')\n",
        "\n",
        "df1= spark.createDataFrame(dd)\n",
        "df1.show(5)\n",
        "\n",
        "df2= spark.createDataFrame(dd)\n",
        "df2.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJfeV_R89-fC",
        "outputId": "1845ebae-dd73-44bb-f57e-9b23878a6b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 150\n",
            "300\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df1.count(), df2.count())\n",
        "print(df1.union(df2).count())\n",
        "df1.union(df2).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIs4sg2J-E7O",
        "outputId": "98296494-5980-4a8f-c290-f67000e0e49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 150\n",
            "300\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df1.count(), df2.count())\n",
        "print(df1.unionAll(df2).count())\n",
        "df1.unionAll(df2).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5HxGiYx-Z74",
        "outputId": "df513e41-b8d1-4e35-ad82-e7cf883204e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "+---+------+------+------+\n",
            "\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data1= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000)]\n",
        "schema1= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "d1= spark.createDataFrame(data1, schema1)\n",
        "d1.show()\n",
        "\n",
        "data2= [(2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', 'M', 4000)]\n",
        "schema2= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "d2= spark.createDataFrame(data2, schema2)\n",
        "d2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1Kqv6vp_BKj",
        "outputId": "718ee91b-7b96-402c-b204-066504b330a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 3\n",
            "5\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(d1.count(), d2.count())\n",
        "print(d1.union(d2).count())\n",
        "d1.union(d2).show(10)\n",
        "\n",
        "# Not remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PqWkXjm_JDg",
        "outputId": "83a836ab-aa16-4b66-b5d2-e1d4cbc6b063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 3\n",
            "5\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(d1.count(), d2.count())\n",
        "print(d1.unionAll(d2).count())\n",
        "d1.unionAll(d2).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwjyOa4V_NOL",
        "outputId": "c8af2efe-b053-49cf-82d2-9567bd9b1fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 3\n",
            "5\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(d1.count(), d2.count())\n",
        "print(d1.unionAll(d2).count())\n",
        "d1.unionAll(d2).distinct().show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ2ZWIN4AG2S"
      },
      "source": [
        "# `groupBy()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GARgWNWI_ue5",
        "outputId": "9ecfa933-a6cb-4d12-d893-096aed58e209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dd= sns.load_dataset('iris')\n",
        "\n",
        "df= spark.createDataFrame(dd)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHtEHfQYAOyy",
        "outputId": "4289e0a8-db0d-4839-a713-bac5989a1f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|   species|count|\n",
            "+----------+-----+\n",
            "|versicolor|   50|\n",
            "|    setosa|   50|\n",
            "| virginica|   50|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('species').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7CVmvQAVC1",
        "outputId": "8fbb3f75-d4d6-4b0f-a3f7-00f39fc2cd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----+\n",
            "|   species|petal_width|count|\n",
            "+----------+-----------+-----+\n",
            "|versicolor|        1.4|    7|\n",
            "|    setosa|        0.4|    7|\n",
            "|versicolor|        1.1|    3|\n",
            "|    setosa|        0.2|   29|\n",
            "|versicolor|        1.8|    1|\n",
            "|    setosa|        0.6|    1|\n",
            "|    setosa|        0.3|    7|\n",
            "|versicolor|        1.0|    7|\n",
            "|    setosa|        0.1|    5|\n",
            "|versicolor|        1.5|   10|\n",
            "|versicolor|        1.3|   13|\n",
            "|versicolor|        1.2|    5|\n",
            "|versicolor|        1.6|    3|\n",
            "|    setosa|        0.5|    1|\n",
            "| virginica|        2.2|    3|\n",
            "| virginica|        1.7|    1|\n",
            "| virginica|        2.5|    3|\n",
            "| virginica|        2.4|    3|\n",
            "| virginica|        1.6|    1|\n",
            "| virginica|        1.9|    5|\n",
            "+----------+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('species', 'petal_width').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJYwa9CrAdv5",
        "outputId": "24f78555-af8a-4ae7-ed82-6bf54c59fc48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+\n",
            "|   species|min(petal_width)|\n",
            "+----------+----------------+\n",
            "|versicolor|             1.0|\n",
            "|    setosa|             0.1|\n",
            "| virginica|             1.4|\n",
            "+----------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('species').min(\"petal_width\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCaU1eGRAsKO",
        "outputId": "2a0cf438-9a66-48f8-c48b-5e20ce1e7bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+\n",
            "|   species|max(petal_width)|\n",
            "+----------+----------------+\n",
            "|versicolor|             1.8|\n",
            "|    setosa|             0.6|\n",
            "| virginica|             2.5|\n",
            "+----------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('species').max(\"petal_width\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3j-zqk6Aude",
        "outputId": "e7d6ead3-445d-493e-d562-4fb40418aedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|   species|  avg(petal_width)|\n",
            "+----------+------------------+\n",
            "|versicolor|1.3260000000000003|\n",
            "|    setosa|0.2459999999999999|\n",
            "| virginica|             2.026|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('species').avg(\"petal_width\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZFES2VAzAS",
        "outputId": "0ad8ec43-869e-4fb8-cde6-e90d2f24f878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GroupedData[grouping expressions: [species], value: [sepal_length: double, sepal_width: double ... 3 more fields], type: GroupBy]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupBy('species')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX5FT42zBhmE"
      },
      "source": [
        "# `groupBy().agg()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk0sLg-8BLoO",
        "outputId": "f2a5ed3f-3823-46e3-a8b6-9895c66761a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------+-------------+-------------+------------------+\n",
            "|   species|CountOfSpecies|MinPitalWidth|MaxPitalWidth|    MeanPitalWidth|\n",
            "+----------+--------------+-------------+-------------+------------------+\n",
            "|versicolor|            50|          1.0|          1.8|1.3260000000000003|\n",
            "|    setosa|            50|          0.1|          0.6|0.2459999999999999|\n",
            "| virginica|            50|          1.4|          2.5|             2.026|\n",
            "+----------+--------------+-------------+-------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count, min, max, avg\n",
        "\n",
        "df.groupBy('species').agg(count('*').alias(\"CountOfSpecies\"),\n",
        "                          min('petal_width').alias('MinPitalWidth'),\n",
        "                          max('petal_width').alias('MaxPitalWidth'),\n",
        "                          avg('petal_width').alias('MeanPitalWidth')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy2eWHxID3w7"
      },
      "source": [
        "# `UnionByName()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEB3pUckDQkH",
        "outputId": "95a46134-f16f-42ae-da64-747c8ce87c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "| id|  name|gender|\n",
            "+---+------+------+\n",
            "|  1| Wasiq|     M|\n",
            "|  2|Zainul|     M|\n",
            "+---+------+------+\n",
            "\n",
            "+---+----+------+\n",
            "| id|name|others|\n",
            "+---+----+------+\n",
            "|  3| abc|  4000|\n",
            "+---+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data1= [(1, 'Wasiq', 'M'), (2, 'Zainul', 'M')]\n",
        "schema1= ['id', 'name', 'gender']\n",
        "\n",
        "d1= spark.createDataFrame(data1, schema1)\n",
        "d1.show()\n",
        "\n",
        "data2= [(3, 'abc', 4000)]\n",
        "schema2= ['id', 'name', 'others']\n",
        "\n",
        "d2= spark.createDataFrame(data2, schema2)\n",
        "d2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jOIf-rlEJ6q",
        "outputId": "5fce7728-4de9-4072-8ca5-2e49c4393992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  NULL|\n",
            "|  2|Zainul|     M|  NULL|\n",
            "|  3|   abc|  NULL|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "d1.unionByName(allowMissingColumns=True, other= d2).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O5tchtoFkiF"
      },
      "source": [
        "# `select()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWz6vKT3ET2p",
        "outputId": "fef2a735-89b8-4cec-cfd4-3c3bf54a408a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dd= sns.load_dataset('iris')\n",
        "\n",
        "df= spark.createDataFrame(dd)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRU9sZLpFnds",
        "outputId": "bc1dd399-de3d-4250-e6de-224e128a2b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+\n",
            "|sepal_length|sepal_width|\n",
            "+------------+-----------+\n",
            "|         5.1|        3.5|\n",
            "|         4.9|        3.0|\n",
            "|         4.7|        3.2|\n",
            "|         4.6|        3.1|\n",
            "|         5.0|        3.6|\n",
            "+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('sepal_length', 'sepal_width').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTkoCkmlFz7V",
        "outputId": "daf76f79-87e1-4590-b65a-dcfea21285ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+\n",
            "|sepal_length|sepal_width|\n",
            "+------------+-----------+\n",
            "|         5.1|        3.5|\n",
            "|         4.9|        3.0|\n",
            "|         4.7|        3.2|\n",
            "|         4.6|        3.1|\n",
            "|         5.0|        3.6|\n",
            "+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----------+\n",
            "|sepal_length|sepal_width|\n",
            "+------------+-----------+\n",
            "|         5.1|        3.5|\n",
            "|         4.9|        3.0|\n",
            "|         4.7|        3.2|\n",
            "|         4.6|        3.1|\n",
            "|         5.0|        3.6|\n",
            "+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df.sepal_length, df.sepal_width).show(5)\n",
        "\n",
        "df.select(df['sepal_length'], df['sepal_width']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96b9yXriF43v",
        "outputId": "7d521be9-2722-498f-d5d7-3e6008a8e42f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+\n",
            "|sepal_length|sepal_width|\n",
            "+------------+-----------+\n",
            "|         5.1|        3.5|\n",
            "|         4.9|        3.0|\n",
            "|         4.7|        3.2|\n",
            "|         4.6|        3.1|\n",
            "|         5.0|        3.6|\n",
            "+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----------+\n",
            "|sepal_length|sepal_width|\n",
            "+------------+-----------+\n",
            "|         5.1|        3.5|\n",
            "|         4.9|        3.0|\n",
            "|         4.7|        3.2|\n",
            "|         4.6|        3.1|\n",
            "|         5.0|        3.6|\n",
            "+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# using col() function\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(col('sepal_length'), col('sepal_width')).show(5)\n",
        "df.select(['sepal_length', 'sepal_width']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilb76J-AGMZI",
        "outputId": "f126798a-81c0-40ef-9ed5-421be406a0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('*').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rxTe25UGmhf"
      },
      "source": [
        "# `join()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CatAKOJnGU0B",
        "outputId": "0d735444-ed70-4bc7-db8d-1ccfbdce5d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|\n",
            "|  2|Zainul|     M|  3000|\n",
            "+---+------+------+------+\n",
            "\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "|  3|   abc|     F|  4000|\n",
            "|  4|   xyz|     M|  4000|\n",
            "+---+------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data1= [(1, 'Wasiq', 'M', 2000), (2, 'Zainul', 'M', 3000)]\n",
        "schema1= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "d1= spark.createDataFrame(data1, schema1)\n",
        "d1.show()\n",
        "\n",
        "data2= [(2, 'Zainul', 'M', 3000), (3, 'abc', 'F', 4000), (4, 'xyz', 'M', 4000)]\n",
        "schema2= ['id', 'name', 'gender', 'others']\n",
        "\n",
        "d2= spark.createDataFrame(data2, schema2)\n",
        "d2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGc3xvIYGtja",
        "outputId": "c9c809ad-abbe-4756-f44c-80f1b53b37d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INNER JOIN\n",
            "+---+------+------+------+---+------+------+------+\n",
            "| id|  name|gender|others| id|  name|gender|others|\n",
            "+---+------+------+------+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|  2|Zainul|     M|  3000|\n",
            "+---+------+------+------+---+------+------+------+\n",
            "\n",
            "==================================================\n",
            "LEFT JOIN\n",
            "+---+------+------+------+----+------+------+------+\n",
            "| id|  name|gender|others|  id|  name|gender|others|\n",
            "+---+------+------+------+----+------+------+------+\n",
            "|  1| Wasiq|     M|  2000|NULL|  NULL|  NULL|  NULL|\n",
            "|  2|Zainul|     M|  3000|   2|Zainul|     M|  3000|\n",
            "+---+------+------+------+----+------+------+------+\n",
            "\n",
            "==================================================\n",
            "RIGHT JOIN\n",
            "+----+------+------+------+---+------+------+------+\n",
            "|  id|  name|gender|others| id|  name|gender|others|\n",
            "+----+------+------+------+---+------+------+------+\n",
            "|   2|Zainul|     M|  3000|  2|Zainul|     M|  3000|\n",
            "|NULL|  NULL|  NULL|  NULL|  3|   abc|     F|  4000|\n",
            "|NULL|  NULL|  NULL|  NULL|  4|   xyz|     M|  4000|\n",
            "+----+------+------+------+---+------+------+------+\n",
            "\n",
            "==================================================\n",
            "FULL JOIN\n",
            "+----+------+------+------+----+------+------+------+\n",
            "|  id|  name|gender|others|  id|  name|gender|others|\n",
            "+----+------+------+------+----+------+------+------+\n",
            "|   1| Wasiq|     M|  2000|NULL|  NULL|  NULL|  NULL|\n",
            "|   2|Zainul|     M|  3000|   2|Zainul|     M|  3000|\n",
            "|NULL|  NULL|  NULL|  NULL|   3|   abc|     F|  4000|\n",
            "|NULL|  NULL|  NULL|  NULL|   4|   xyz|     M|  4000|\n",
            "+----+------+------+------+----+------+------+------+\n",
            "\n",
            "==================================================\n",
            "LEFT SEMI JOIN\n",
            "+---+------+------+------+\n",
            "| id|  name|gender|others|\n",
            "+---+------+------+------+\n",
            "|  2|Zainul|     M|  3000|\n",
            "+---+------+------+------+\n",
            "\n",
            "==================================================\n",
            "LEFT ANTI JOIN\n",
            "+---+-----+------+------+\n",
            "| id| name|gender|others|\n",
            "+---+-----+------+------+\n",
            "|  1|Wasiq|     M|  2000|\n",
            "+---+-----+------+------+\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"INNER JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'inner').show()\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"LEFT JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'left').show()\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"RIGHT JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'right').show()\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"FULL JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'full').show()\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"LEFT SEMI JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'leftsemi').show()\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"LEFT ANTI JOIN\")\n",
        "d1.join(d2, d1.id == d2.id, 'leftanti').show()\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY8CvBjgHVkQ",
        "outputId": "d444b0d2-c11a-43a4-a2b7-9da76c5efb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "| id|  name|  name|\n",
            "+---+------+------+\n",
            "|  1| Wasiq| Wasiq|\n",
            "|  2|Zainul|Zainul|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "d1.alias('test1').join(d1.alias('test2'), col('test1.id') == col('test2.id'), 'left')\\\n",
        ".select([col('test1.id'), col('test1.name'), col('test2.name')]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn8EGu4tLjmb"
      },
      "source": [
        "# `pivot()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pytcBIe4IrdA",
        "outputId": "21a7f250-edce-4626-9d85-3995887aee0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ddt= sns.load_dataset('titanic')\n",
        "\n",
        "df_titanic= spark.createDataFrame(ddt)\n",
        "df_titanic.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUKi-zhELv18",
        "outputId": "77a5613b-4972-4a00-c881-66797d969f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+----+\n",
            "|embark_town|female|male|\n",
            "+-----------+------+----+\n",
            "| Queenstown|    36|  41|\n",
            "|Southampton|   203| 441|\n",
            "|        NaN|     2|NULL|\n",
            "|  Cherbourg|    73|  95|\n",
            "+-----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.groupBy('embark_town').pivot('sex').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZHV81ICL7I1",
        "outputId": "51abc90c-247f-417e-c99d-ccd90a1f54f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+----+\n",
            "|embark_town|survived|female|male|\n",
            "+-----------+--------+------+----+\n",
            "| Queenstown|       1|    27|   3|\n",
            "|  Cherbourg|       0|     9|  66|\n",
            "|  Cherbourg|       1|    64|  29|\n",
            "|Southampton|       0|    63| 364|\n",
            "| Queenstown|       0|     9|  38|\n",
            "|Southampton|       1|   140|  77|\n",
            "|        NaN|       1|     2|NULL|\n",
            "+-----------+--------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.groupBy(['embark_town', 'survived']).pivot('sex').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFNm_Sx5Nrrq"
      },
      "source": [
        "# Unpivot Dataframe\n",
        "\n",
        "Unpivot is rotating columns into rows. PySpark SQL doesn't have unpivot function hence will use the **`stack()`** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRWdNIhQNPrn",
        "outputId": "d4cceb19-72c5-4232-a31a-416af32f5182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+----+\n",
            "|embark_town|female|male|\n",
            "+-----------+------+----+\n",
            "| Queenstown|    36|  41|\n",
            "|Southampton|   203| 441|\n",
            "|        NaN|     2|NULL|\n",
            "|  Cherbourg|    73|  95|\n",
            "+-----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "un_pivot= df_titanic.groupBy('embark_town').pivot('sex').count()\n",
        "un_pivot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g6vlVnZOTrC",
        "outputId": "d0017148-fc56-40ef-b44b-c8681a9b9cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+-----+\n",
            "|embark_town|gender|count|\n",
            "+-----------+------+-----+\n",
            "| Queenstown|  Male|   41|\n",
            "| Queenstown|Female|   36|\n",
            "|Southampton|  Male|  441|\n",
            "|Southampton|Female|  203|\n",
            "|        NaN|  Male| NULL|\n",
            "|        NaN|Female|    2|\n",
            "|  Cherbourg|  Male|   95|\n",
            "|  Cherbourg|Female|   73|\n",
            "+-----------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "un_pivot.select('embark_town', expr(\"stack(2, 'Male', male, 'Female', female) as (gender, count)\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Y195cQOrEs",
        "outputId": "10f3fc51-c76c-449f-82c8-f20765fc428c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+-----+\n",
            "|embark_town|survived|gender|count|\n",
            "+-----------+--------+------+-----+\n",
            "| Queenstown|       1|  Male|    3|\n",
            "| Queenstown|       1|Female|   27|\n",
            "|  Cherbourg|       0|  Male|   66|\n",
            "|  Cherbourg|       0|Female|    9|\n",
            "|  Cherbourg|       1|  Male|   29|\n",
            "|  Cherbourg|       1|Female|   64|\n",
            "|Southampton|       0|  Male|  364|\n",
            "|Southampton|       0|Female|   63|\n",
            "| Queenstown|       0|  Male|   38|\n",
            "| Queenstown|       0|Female|    9|\n",
            "|Southampton|       1|  Male|   77|\n",
            "|Southampton|       1|Female|  140|\n",
            "|        NaN|       1|  Male| NULL|\n",
            "|        NaN|       1|Female|    2|\n",
            "+-----------+--------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.groupBy(['embark_town', 'survived']).pivot('sex').count()\\\n",
        ".select('embark_town', 'survived', expr(\"stack(2, 'Male', male, 'Female', female) as (gender, count)\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8HQjMpaECP"
      },
      "source": [
        "# `fillna()` and `fillna()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRnJahjJPuep",
        "outputId": "e69bcd47-75be-41df-d047-460dd8166585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ddt= sns.load_dataset('titanic')\n",
        "\n",
        "df_titanic= spark.createDataFrame(ddt)\n",
        "df_titanic.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huVb0NwBaJ3o",
        "outputId": "9c2d363f-ce4e-40e3-da87-4de3a4424ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S| Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C| First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S| Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S| First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S| Third|  man|      true| NaN|Southampton|   no| true|\n",
            "|       0|     3|  male| 0.0|    0|    0| 8.4583|       Q| Third|  man|      true| NaN| Queenstown|   no| true|\n",
            "|       0|     1|  male|54.0|    0|    0|51.8625|       S| First|  man|      true|   E|Southampton|   no| true|\n",
            "|       0|     3|  male| 2.0|    3|    1| 21.075|       S| Third|child|     false| NaN|Southampton|   no|false|\n",
            "|       1|     3|female|27.0|    0|    2|11.1333|       S| Third|woman|     false| NaN|Southampton|  yes|false|\n",
            "|       1|     2|female|14.0|    1|    0|30.0708|       C|Second|child|     false| NaN|  Cherbourg|  yes|false|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.na.fill(0, ['age']).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leTU-lwtaTGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee8eaaa-e9c9-401f-a1ff-cbe9e7c4dff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S| Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C| First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S| Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S| First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S| Third|  man|      true| NaN|Southampton|   no| true|\n",
            "|       0|     3|  male| NaN|    0|    0| 8.4583|       Q| Third|  man|      true| NaN| Queenstown|   no| true|\n",
            "|       0|     1|  male|54.0|    0|    0|51.8625|       S| First|  man|      true|   E|Southampton|   no| true|\n",
            "|       0|     3|  male| 2.0|    3|    1| 21.075|       S| Third|child|     false| NaN|Southampton|   no|false|\n",
            "|       1|     3|female|27.0|    0|    2|11.1333|       S| Third|woman|     false| NaN|Southampton|  yes|false|\n",
            "|       1|     2|female|14.0|    1|    0|30.0708|       C|Second|child|     false| NaN|  Cherbourg|  yes|false|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.fillna('unknown', ['deck']).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_titanic.fillna({'deck': 'unknown', 'age': 0}).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k3SHiJmdSr-",
        "outputId": "83f693ee-8702-4fb9-9134-8c65b6e22e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S| Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C| First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S| Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S| First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S| Third|  man|      true| NaN|Southampton|   no| true|\n",
            "|       0|     3|  male| 0.0|    0|    0| 8.4583|       Q| Third|  man|      true| NaN| Queenstown|   no| true|\n",
            "|       0|     1|  male|54.0|    0|    0|51.8625|       S| First|  man|      true|   E|Southampton|   no| true|\n",
            "|       0|     3|  male| 2.0|    3|    1| 21.075|       S| Third|child|     false| NaN|Southampton|   no|false|\n",
            "|       1|     3|female|27.0|    0|    2|11.1333|       S| Third|woman|     false| NaN|Southampton|  yes|false|\n",
            "|       1|     2|female|14.0|    1|    0|30.0708|       C|Second|child|     false| NaN|  Cherbourg|  yes|false|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `sample()`"
      ],
      "metadata": {
        "id": "7-YTndSbd6L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ddt= sns.load_dataset('titanic')\n",
        "\n",
        "df_titanic= spark.createDataFrame(ddt)\n",
        "df_titanic.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrNOgHaTdfPV",
        "outputId": "6196f8ae-948d-42c8-c770-c05f743fe7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_titanic_sample1= df_titanic.sample(fraction= 0.1, seed= 123)\n",
        "df_titanic_sample2= df_titanic.sample(fraction= 0.1, seed= 10)\n",
        "\n",
        "df_titanic_sample1.show(5)\n",
        "df_titanic_sample2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjP2gAVhd88J",
        "outputId": "80c73e91-669a-4276-ab1a-d035ddc0033a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       1|     3|  male| NaN|    0|    0| 7.2292|       C| Third|  man|      true| NaN|  Cherbourg|  yes| true|\n",
            "|       0|     3|  male|21.0|    0|    0|   8.05|       S| Third|  man|      true| NaN|Southampton|   no| true|\n",
            "|       0|     2|female|27.0|    1|    0|   21.0|       S|Second|woman|     false| NaN|Southampton|   no|false|\n",
            "|       1|     2|female| 3.0|    1|    2|41.5792|       C|Second|child|     false| NaN|  Cherbourg|  yes|false|\n",
            "|       1|     2|female|21.0|    0|    0|   10.5|       S|Second|woman|     false| NaN|Southampton|  yes| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     2|  male|35.0|    0|    0|   26.0|       S|Second|  man|      true| NaN|Southampton|   no| true|\n",
            "|       1|     3|female|15.0|    0|    0| 8.0292|       Q| Third|child|     false| NaN| Queenstown|  yes| true|\n",
            "|       1|     3|female|38.0|    1|    5|31.3875|       S| Third|woman|     false| NaN|Southampton|  yes|false|\n",
            "|       0|     3|  male| NaN|    0|    0|  7.225|       C| Third|  man|      true| NaN|  Cherbourg|   no| true|\n",
            "|       0|     3|  male| NaN|    0|    0| 7.8958|       C| Third|  man|      true| NaN|  Cherbourg|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `collect()`\n",
        "\n",
        "* `collect()` retrieves all elements in a DataFrame as an Array of Row type to the driver node.\n",
        "* `collect()` is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further.\n",
        "* `collect()` use it with small DataFrames. With big DataFrames it may result in out of memory error as its return entier data to single node(driver)"
      ],
      "metadata": {
        "id": "ez9qwCKTfcZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataRows= df_titanic.collect()\n",
        "\n",
        "print(type(dataRows))\n",
        "print(dataRows[0])\n",
        "print(dataRows[10])\n",
        "print(dataRows[10][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8hT3AAReXcB",
        "outputId": "2fbd46e4-ac46-4a29-bfde-22313785cf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "Row(survived=0, pclass=3, sex='male', age=22.0, sibsp=1, parch=0, fare=7.25, embarked='S', class='Third', who='man', adult_male=True, deck='NaN', embark_town='Southampton', alive='no', alone=False)\n",
            "Row(survived=1, pclass=3, sex='female', age=4.0, sibsp=1, parch=1, fare=16.7, embarked='S', class='Third', who='child', adult_male=False, deck='G', embark_town='Southampton', alive='yes', alone=False)\n",
            "female\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(1, \"Wasiq\", 24), (2, \"Zainul\", 17)]\n",
        "schema= ['id', 'name', 'age']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgZRjRdFgLmq",
        "outputId": "7ca6256d-db0c-4d4e-a212-1fc9ec538fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "\n",
        "def convertToUpper(df):\n",
        "  return df.withColumn('name', upper(df.name))\n",
        "\n",
        "def doubleTheAge(df):\n",
        "  return df.withColumn('age', df.age*2)\n",
        "\n",
        "df.transform(convertToUpper).transform(doubleTheAge).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNOE-xrohH6t",
        "outputId": "138ff239-c2e2-43b7-8e1d-e45f490609d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| WASIQ| 48|\n",
            "|  2|ZAINUL| 34|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper, transform\n",
        "data=[(1, 'maheer' ,['azure', 'dotnet']), (2, 'wafa', ['aws', 'java'])]\n",
        "\n",
        "\n",
        "schema = ['id', 'name', 'skills']\n",
        "df= spark.createDataFrame (data , schema)\n",
        "df.show()\n",
        "\n",
        "df.select('id', 'name', transform('skills', lambda x: upper(x)).alias('Skill')).show()\n",
        "\n",
        "def convertToUpper1(x):\n",
        "  return upper(x)\n",
        "\n",
        "df.select(transform('skills', convertToUpper1).alias('SKILLS')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbcmBtv7hpfa",
        "outputId": "c36a8e9f-27d9-42ce-cc8c-24d4464d2d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------------+\n",
            "| id|  name|         skills|\n",
            "+---+------+---------------+\n",
            "|  1|maheer|[azure, dotnet]|\n",
            "|  2|  wafa|    [aws, java]|\n",
            "+---+------+---------------+\n",
            "\n",
            "+---+------+---------------+\n",
            "| id|  name|          Skill|\n",
            "+---+------+---------------+\n",
            "|  1|maheer|[AZURE, DOTNET]|\n",
            "|  2|  wafa|    [AWS, JAVA]|\n",
            "+---+------+---------------+\n",
            "\n",
            "+---------------+\n",
            "|         SKILLS|\n",
            "+---------------+\n",
            "|[AZURE, DOTNET]|\n",
            "|    [AWS, JAVA]|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `createOrReplaceTempView()`\n",
        "\n",
        "* Advantage of Spark, you can work with SQL along with DataFrames. That means, if you are comfortable withSQL, you can create temporary view on Dataframe by using createOrReplaceTempView() and use SQL to select and manipulate data.\n",
        "* Temp Views are session scoped and cannot be shared between the sessions."
      ],
      "metadata": {
        "id": "ZwxxHBanl002"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(1, \"Wasiq\", 24), (2, \"Zainul\", 17)]\n",
        "schema= ['id', 'name', 'age']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhaibcWQid8j",
        "outputId": "935c15a1-52a9-4861-e26c-7bcb4f2201d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('test')\n",
        "spark.sql('select * from test').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeBH3HnYl-Dh",
        "outputId": "fb8f23aa-24a9-406f-b0b4-063380d9556b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql= spark.sql(\"SELECT id, UPPER(name) as NAME FROM test\")\n",
        "df_sql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBkk33YzmLGH",
        "outputId": "2462e3f4-1677-43d9-a4cc-22800278af2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  NAME|\n",
            "+---+------+\n",
            "|  1| WASIQ|\n",
            "|  2|ZAINUL|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `createOrReplaceGlobalTempView()`"
      ],
      "metadata": {
        "id": "WGdxxdIanVyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(1, \"Wasiq\", 24), (2, \"Zainul\", 17)]\n",
        "schema= ['id', 'name', 'age']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.createOrReplaceGlobalTempView('testGlobal')"
      ],
      "metadata": {
        "id": "FBqh86Kkm4ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.currentDatabase()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IgS5oY9Gnr2g",
        "outputId": "774d1f9b-23fb-42e2-b89a-a0ef8a7b93e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'default'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables('default')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ2ywnFWoEde",
        "outputId": "57243d21-9b14-4176-9487-8bff54b4369b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='test', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDF (User Defined Function)"
      ],
      "metadata": {
        "id": "GusB2-VQoxYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(1, \"Wasiq\", 24), (2, \"Zainul\", 17)]\n",
        "schema= ['id', 'name', 'age']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOQ3VElhoi4h",
        "outputId": "9c59f29c-7f7e-43ff-dfe5-6042feace0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def age1(a):\n",
        "  return a+1\n",
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "AgeAfterOneYear= udf(lambda x: age1(x), IntegerType())\n",
        "\n",
        "df.select('*', AgeAfterOneYear(col('age')).alias('Age After One Year')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j43eEcdHo8ju",
        "outputId": "c14444c9-f475-4d75-cdd1-d3e265a34a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+------------------+\n",
            "| id|  name|age|Age After One Year|\n",
            "+---+------+---+------------------+\n",
            "|  1| Wasiq| 24|                25|\n",
            "|  2|Zainul| 17|                18|\n",
            "+---+------+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD (Resilent Distributed Dataset)\n",
        "\n",
        "* Its collection of objects similar to list in Python. Its immutable and in memory processing.\n",
        "* By using `parallelize()` function of SparkContext you can create an RDD."
      ],
      "metadata": {
        "id": "4jCbFzChqKgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(1, \"Wasiq\", 24), (2, \"Zainul\", 17)]\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "print(rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGnErLDNpq0l",
        "outputId": "e010e9ac-fab9-4c89-bfec-bb133c045018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'Wasiq', 24), (2, 'Zainul', 17)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(rdd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6ITbi1wrZm-",
        "outputId": "139839c7-dc57-4d9d-d2b8-1efc85ea5466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_to_df= rdd.toDF()\n",
        "rdd_to_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udjuLYeprg4L",
        "outputId": "84894fcf-b035-4b62-92ed-48993a9b798a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| _1|    _2| _3|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_to_df= rdd.toDF(schema= ['id', 'name', 'age'])\n",
        "rdd_to_df.show()"
      ],
      "metadata": {
        "id": "CpX--9gfrqAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ff54f5-9586-4850-f827-c343be13abd8"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.createDataFrame(rdd, schema= ['id', 'name', 'age']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YXHJbWxsFXb",
        "outputId": "1511dce1-b399-4eef-f84a-3d73526f42c9"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1| Wasiq| 24|\n",
            "|  2|Zainul| 17|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `map()` Transformation"
      ],
      "metadata": {
        "id": "or3jUfjTuIBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [('Mohammad', 'Wasiq'), ('Zainul', 'Pasha')]\n",
        "\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "print(rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdPGBbHCsPvw",
        "outputId": "10e84efd-e9de-43eb-f7ed-98e86c098a16"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Mohammad', 'Wasiq'), ('Zainul', 'Pasha')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1= rdd.map(lambda x: (x[0].upper(), x[1].upper()))\n",
        "print(rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htFKYlppuclE",
        "outputId": "90974b4a-e9af-422e-827d-53c42f871bfb"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('MOHAMMAD', 'WASIQ'), ('ZAINUL', 'PASHA')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= [('Mohammad', 'Wasiq'), ('Zainul', 'Pasha')]\n",
        "\n",
        "df= spark.createDataFrame(data, schema= ['first_name', 'last_name'])\n",
        "df.show()\n",
        "\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd1= df.rdd.map(lambda x: x + (x[0]+ x[1], ))\n",
        "df1= rdd1.toDF(['first_name', 'last_name', 'full_name'])\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV1__EMOuh55",
        "outputId": "eff21eb1-77cc-4cd7-8545-3d966cb64a5e"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|first_name|last_name|\n",
            "+----------+---------+\n",
            "|  Mohammad|    Wasiq|\n",
            "|    Zainul|    Pasha|\n",
            "+----------+---------+\n",
            "\n",
            "+----------+---------+-------------+\n",
            "|first_name|last_name|    full_name|\n",
            "+----------+---------+-------------+\n",
            "|  Mohammad|    Wasiq|MohammadWasiq|\n",
            "|    Zainul|    Pasha|  ZainulPasha|\n",
            "+----------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fullname(x):\n",
        "  x= x + (x[0]+ ' ' + x[1], )\n",
        "  return x\n",
        "\n",
        "df= spark.createDataFrame(data, schema= ['first_name', 'last_name'])\n",
        "df.show()\n",
        "\n",
        "data= [('Mohammad', 'Wasiq'), ('Zainul', 'Pasha')]\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd1= df.rdd.map(fullname)\n",
        "df1= rdd1.toDF(['first_name', 'last_name', 'full_name'])\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz4FSPFdvG5F",
        "outputId": "8d843bda-110c-4c37-c527-410d5e330fcf"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|first_name|last_name|\n",
            "+----------+---------+\n",
            "|  Mohammad|    Wasiq|\n",
            "|    Zainul|    Pasha|\n",
            "+----------+---------+\n",
            "\n",
            "+----------+---------+--------------+\n",
            "|first_name|last_name|     full_name|\n",
            "+----------+---------+--------------+\n",
            "|  Mohammad|    Wasiq|Mohammad Wasiq|\n",
            "|    Zainul|    Pasha|  Zainul Pasha|\n",
            "+----------+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# flatMap()"
      ],
      "metadata": {
        "id": "derIs1A-v0Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [('Mohammad Wasiq', 'Zainul Pasha')]\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "\n",
        "for item in rdd.collect():\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rnve2cavsIA",
        "outputId": "12f6ff51-c01c-4780-e2d6-932cf295dd98"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Mohammad Wasiq', 'Zainul Pasha')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Mohammad Wasiq\", \"Zainul Abedeen\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "rdd1 = rdd.flatMap(lambda x: x.split(' '))\n",
        "\n",
        "for item in rdd1.collect():\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTHHkE-KwIwk",
        "outputId": "ba00fc71-14fc-44fd-bdbf-4eb39fd9be3c"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mohammad\n",
            "Wasiq\n",
            "Zainul\n",
            "Abedeen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `partitionBy()`"
      ],
      "metadata": {
        "id": "NlSuiaBHxnTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dd= sns.load_dataset('titanic')\n",
        "\n",
        "df= spark.createDataFrame(dd)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4huS6S4hxPkD",
        "outputId": "f6a089fb-a556-41ce-bb6e-8f432be9918e"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(path= \"/content/Data/embark_town\", mode='overwrite', partitionBy= 'embark_town')\n",
        "\n",
        "df.write.parquet(path= \"/content/Data/embark_town_and_class\", mode='overwrite', partitionBy= ['embark_town', 'class'])"
      ],
      "metadata": {
        "id": "6bGcBOHpx6Ww"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `from_json()`"
      ],
      "metadata": {
        "id": "C9Ln0Yuty4y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [(\"Wasiq\", \"{'hair':'black', 'eye':'black'}\")]\n",
        "schema= ['name', 'props']\n",
        "\n",
        "df= spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTkehJFeyn3g",
        "outputId": "e291f44c-b754-40bc-a96e-3a7807614d95"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------+\n",
            "|name |props                          |\n",
            "+-----+-------------------------------+\n",
            "|Wasiq|{'hair':'black', 'eye':'black'}|\n",
            "+-----+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "# propsMap column with MapType get generates from json string\n",
        "# df1= df.withColumn('propsMap', from_json(df.props, MapType(StringType(), StringType())))\n",
        "\n",
        "df1= df.withColumn('propsMap', from_json(col('props'), MapType(StringType(), StringType())))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n",
        "display(df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "dQ6oRWSKzbj_",
        "outputId": "484e441b-84ad-4e65-f0f4-97913b722509"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------+-----------------------------+\n",
            "|name |props                          |propsMap                     |\n",
            "+-----+-------------------------------+-----------------------------+\n",
            "|Wasiq|{'hair':'black', 'eye':'black'}|{hair -> black, eye -> black}|\n",
            "+-----+-------------------------------+-----------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            " |-- propsMap: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, props: string, propsMap: map<string,string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing 'eye' key from MapType column 'propsMap'\n",
        "df2= df1.withColumn('eye', col('propsMap')['eye'])\n",
        "\n",
        "# df2= df1.withColumn('eye', df.propsMap.eye)\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lybVwy5z_YZ",
        "outputId": "605b783b-a449-4be8-f417-e75b1389660b"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------+-----------------------------+-----+\n",
            "|name |props                          |propsMap                     |eye  |\n",
            "+-----+-------------------------------+-----------------------------+-----+\n",
            "|Wasiq|{'hair':'black', 'eye':'black'}|{hair -> black, eye -> black}|black|\n",
            "+-----+-------------------------------+-----------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import MapType, StringType, StructType, StructField\n",
        "\n",
        "structTypeSchema= StructType([\n",
        "    StructField('hair', StringType(), True),\n",
        "    StructField('eye', StringType(), True)\n",
        "])\n",
        "\n",
        "df3= df.withColumn('propsStruct', from_json(df.props, structTypeSchema))\n",
        "df3.show(truncate=False)\n",
        "df3.printSchema()\n",
        "display(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "X0N6Z2S_0TR7",
        "outputId": "994c3e11-3bfd-415e-e677-59c45a20afe5"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------+--------------+\n",
            "|name |props                          |propsStruct   |\n",
            "+-----+-------------------------------+--------------+\n",
            "|Wasiq|{'hair':'black', 'eye':'black'}|{black, black}|\n",
            "+-----+-------------------------------+--------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            " |-- propsStruct: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, props: string, propsStruct: struct<hair:string,eye:string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df3.withColumn('eye', col('propsStruct')['eye']).show(truncate=False)\n",
        "\n",
        "df3.withColumn('hair', df3.propsStruct.hair).withColumn('eye', df3.propsStruct.eye).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdouGubA1W4K",
        "outputId": "e7c836a2-3212-49ed-e318-4ddfb3619027"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------+--------------+-----+-----+\n",
            "|name |props                          |propsStruct   |hair |eye  |\n",
            "+-----+-------------------------------+--------------+-----+-----+\n",
            "|Wasiq|{'hair':'black', 'eye':'black'}|{black, black}|black|black|\n",
            "+-----+-------------------------------+--------------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date Functions\n",
        "\n",
        "* DateType default fomart is `yyyy-MM-dd`\n",
        "* `current_date()` get the current system date. By default, the data will be returned in `yyyy-dd-mm` format.\n",
        "* `date_format()` to parses the date and converts from yyyy-MM-dd to specified format.\n",
        "* `to_date()` converts date string in to datetype. We need to specify format of date in the string in the function."
      ],
      "metadata": {
        "id": "neNqnga33Pkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date, date_format, to_date, lit\n",
        "\n",
        "df= spark.range(1)\n",
        "\n",
        "df.withColumn('TodayDate', current_date()).show()\n",
        "\n",
        "df.withColumn('NewFormat', date_format(lit('2024-09-16'), 'MM.dd.yyyy')).show()\n",
        "\n",
        "df.withColumn('NewDateFormat', date_format(lit('2024-09-16'), 'dd-MM-yyyy')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKyQEnTV2ypm",
        "outputId": "90b114b5-6c1f-478c-ea1e-b78ac9f4683c"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id| TodayDate|\n",
            "+---+----------+\n",
            "|  0|2024-09-16|\n",
            "+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| id| NewFormat|\n",
            "+---+----------+\n",
            "|  0|09.16.2024|\n",
            "+---+----------+\n",
            "\n",
            "+---+-------------+\n",
            "| id|NewDateFormat|\n",
            "+---+-------------+\n",
            "|  0|   16-09-2024|\n",
            "+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import datediff, months_between, add_months, date_add, year, month\n",
        "\n",
        "df= spark.createDataFrame([('2024-09-16', '2000-08-27')], ['d1', 'd2'])\n",
        "\n",
        "df.withColumn('diff', datediff(df.d2, df.d1)).show()\n",
        "\n",
        "df.withColumn('monthsBetween', months_between(df.d2, df.d1)).show()\n",
        "\n",
        "df.withColumn('addmonth', add_months(df.d2, 4)).show()\n",
        "df.withColumn('submonth', add_months(df.d2, -4)).show()\n",
        "\n",
        "df.withColumn('addDate', date_add(df.d2, 4)).show()\n",
        "df.withColumn('subDate', date_add(df.d2, -4)).show()\n",
        "\n",
        "df.withColumn('year', year(df.d2)).show()\n",
        "df.withColumn('month', month(df.d2)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5OieQHJ4B1-",
        "outputId": "657f6ad5-6c7c-4e8b-9c4b-a613e7559f4d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+\n",
            "|        d1|        d2| diff|\n",
            "+----------+----------+-----+\n",
            "|2024-09-16|2000-08-27|-8786|\n",
            "+----------+----------+-----+\n",
            "\n",
            "+----------+----------+-------------+\n",
            "|        d1|        d2|monthsBetween|\n",
            "+----------+----------+-------------+\n",
            "|2024-09-16|2000-08-27|-288.64516129|\n",
            "+----------+----------+-------------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|  addmonth|\n",
            "+----------+----------+----------+\n",
            "|2024-09-16|2000-08-27|2000-12-27|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|  submonth|\n",
            "+----------+----------+----------+\n",
            "|2024-09-16|2000-08-27|2000-04-27|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|   addDate|\n",
            "+----------+----------+----------+\n",
            "|2024-09-16|2000-08-27|2000-08-31|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|   subDate|\n",
            "+----------+----------+----------+\n",
            "|2024-09-16|2000-08-27|2000-08-23|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----+\n",
            "|        d1|        d2|year|\n",
            "+----------+----------+----+\n",
            "|2024-09-16|2000-08-27|2000|\n",
            "+----------+----------+----+\n",
            "\n",
            "+----------+----------+-----+\n",
            "|        d1|        d2|month|\n",
            "+----------+----------+-----+\n",
            "|2024-09-16|2000-08-27|    8|\n",
            "+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp, to_timestamp, lit, hour, minute, second\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.range(1)\n",
        "df.show()\n",
        "\n",
        "# Add a timestamp column\n",
        "df1 = df.withColumn('timestamp', current_timestamp())\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n",
        "\n",
        "# Add a column with a specific timestamp format\n",
        "df2 = df1.withColumn('toTimestamp', to_timestamp(lit('16.09.2024 11:07:00'), 'dd.MM.yyyy HH:mm:ss'))\n",
        "df2.show(truncate=False)\n",
        "df2.printSchema()\n",
        "\n",
        "# Select id and extract hour, minute, and second from the current timestamp\n",
        "df2.select(\n",
        "    'id',\n",
        "    hour(current_timestamp()).alias('hour'),\n",
        "    minute(current_timestamp()).alias('minute'),\n",
        "    second(current_timestamp()).alias('second')\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMbkf6SF5gMi",
        "outputId": "a0c8ad4d-6a82-4d48-d9ec-7baf1efa7dde"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "+---+\n",
            "\n",
            "+---+--------------------------+\n",
            "|id |timestamp                 |\n",
            "+---+--------------------------+\n",
            "|0  |2024-09-16 17:37:40.532002|\n",
            "+---+--------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- timestamp: timestamp (nullable = false)\n",
            "\n",
            "+---+--------------------------+-------------------+\n",
            "|id |timestamp                 |toTimestamp        |\n",
            "+---+--------------------------+-------------------+\n",
            "|0  |2024-09-16 17:37:40.850625|2024-09-16 11:07:00|\n",
            "+---+--------------------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- timestamp: timestamp (nullable = false)\n",
            " |-- toTimestamp: timestamp (nullable = true)\n",
            "\n",
            "+---+----+------+------+\n",
            "| id|hour|minute|second|\n",
            "+---+----+------+------+\n",
            "|  0|  17|    37|    41|\n",
            "+---+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `approx_count_distinct()`"
      ],
      "metadata": {
        "id": "7xZE_Ncc6qaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, collect_list, collect_set, countDistinct, count\n",
        "\n",
        "simpleData = [\n",
        "    (\"Maheer\", \"IT\", 5000),\n",
        "    (\"Wafa\", \"IT\", 6000),\n",
        "    (\"Asif\", \"HR\", 7000)\n",
        "]\n",
        "\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=simpleData, schema=schema)\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "print(\"Select distinct values\")\n",
        "df.select(\"department\").distinct().show()\n",
        "\n",
        "print(\"Calculate average salary\")\n",
        "df.select(avg(\"salary\")).show()\n",
        "\n",
        "print(\"Collect list of salaries\")\n",
        "df.select(collect_list(\"salary\")).show()\n",
        "\n",
        "print(\"Collect set of salaries\")\n",
        "df.select(collect_set(\"salary\")).show()\n",
        "\n",
        "print(\"Count distinct salaries\")\n",
        "df.select(countDistinct(\"salary\")).show()\n",
        "\n",
        "print(\"Count total number of rows\")\n",
        "df.select(count(\"salary\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8SUibEm6Va_",
        "outputId": "d88f7495-bbba-444e-be13-8c406473dbf8"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|       Maheer|        IT|  5000|\n",
            "|         Wafa|        IT|  6000|\n",
            "|         Asif|        HR|  7000|\n",
            "+-------------+----------+------+\n",
            "\n",
            "Select distinct values\n",
            "+----------+\n",
            "|department|\n",
            "+----------+\n",
            "|        IT|\n",
            "|        HR|\n",
            "+----------+\n",
            "\n",
            "Calculate average salary\n",
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|     6000.0|\n",
            "+-----------+\n",
            "\n",
            "Collect list of salaries\n",
            "+--------------------+\n",
            "|collect_list(salary)|\n",
            "+--------------------+\n",
            "|  [5000, 6000, 7000]|\n",
            "+--------------------+\n",
            "\n",
            "Collect set of salaries\n",
            "+-------------------+\n",
            "|collect_set(salary)|\n",
            "+-------------------+\n",
            "| [7000, 5000, 6000]|\n",
            "+-------------------+\n",
            "\n",
            "Count distinct salaries\n",
            "+----------------------+\n",
            "|count(DISTINCT salary)|\n",
            "+----------------------+\n",
            "|                     3|\n",
            "+----------------------+\n",
            "\n",
            "Count total number of rows\n",
            "+-------------+\n",
            "|count(salary)|\n",
            "+-------------+\n",
            "|            3|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank Function\n",
        "\n",
        "## `row_number(), rank(), dense_rank()`\n",
        "\n",
        "* we need to partition the data using `Window.partitionBy()` and for row number and rank function we need to additionally orderb on artition data using orderBy clause.\n",
        "* `row_number()` window function is used to give the sequential row\n",
        "number starting from 1 to the result of each window partition\n",
        "* `rank()` window function is used to provide a rank to the\n",
        "window partition. This function leaves gaps in rank when there are ties.\n",
        "* `dense_rank()` window function is used to get the result with rank of\n",
        "rows within a window partition without any gaps. This is similar to\n",
        "`rank()` function difference being rank function leaves gaps in rank when\n",
        "there are ties."
      ],
      "metadata": {
        "id": "iOktyoYK7TrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank\n",
        "\n",
        "data = [\n",
        "    (\"A\", \"HR\", 1000),\n",
        "    (\"B\", \"HR\", 2000),\n",
        "    (\"C\", \"HR\", 2000),\n",
        "    (\"D\", \"IT\", 3000),\n",
        "    (\"E\", \"IT\", 4000)\n",
        "]\n",
        "\n",
        "schema = [\"name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlQwlKF37GpT",
        "outputId": "6bf05978-6b05-4e2b-f126-1096754da091"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|name|department|salary|\n",
            "+----+----------+------+\n",
            "|   A|        HR|  1000|\n",
            "|   B|        HR|  2000|\n",
            "|   C|        HR|  2000|\n",
            "|   D|        IT|  3000|\n",
            "|   E|        IT|  4000|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# row_number()\n",
        "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\", row_number().over(windowSpec)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFYJRQla76pC",
        "outputId": "7e7bec64-3865-41b5-8705-c10f09d97020"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+----------+\n",
            "|name|department|salary|row_number|\n",
            "+----+----------+------+----------+\n",
            "|   A|        HR|  1000|         1|\n",
            "|   B|        HR|  2000|         2|\n",
            "|   C|        HR|  2000|         3|\n",
            "|   D|        IT|  3000|         1|\n",
            "|   E|        IT|  4000|         2|\n",
            "+----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nk()"
      ],
      "metadata": {
        "id": "27nvLYQT8Ib4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank()\n",
        "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jkE-ycn8A9m",
        "outputId": "07008401-939f-40eb-99ea-0f53d81974eb"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+----+\n",
            "|name|department|salary|rank|\n",
            "+----+----------+------+----+\n",
            "|   A|        HR|  1000|   1|\n",
            "|   B|        HR|  2000|   2|\n",
            "|   C|        HR|  2000|   2|\n",
            "|   D|        IT|  3000|   1|\n",
            "|   E|        IT|  4000|   2|\n",
            "+----+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank()\n",
        "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7xsMpz8FNd",
        "outputId": "fb3952fd-91cd-4ba6-ca00-5bd31fabbb81"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+----------+\n",
            "|name|department|salary|dense_rank|\n",
            "+----+----------+------+----------+\n",
            "|   A|        HR|  1000|         1|\n",
            "|   B|        HR|  2000|         2|\n",
            "|   C|        HR|  2000|         2|\n",
            "|   D|        IT|  3000|         1|\n",
            "|   E|        IT|  4000|         2|\n",
            "+----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WGZIAwJy8o0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Special Thanks To\n",
        "\n",
        "* [**WafaStudies YouTube Channel**](https://www.youtube.com/@WafaStudies)\n",
        "\n",
        "* [**PySpark Playlist**](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)\n",
        "\n"
      ],
      "metadata": {
        "id": "mYH0c-wS8pAo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}